{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to figure out how much data is needed for training\n",
    "import mne\n",
    "import scipy.io as sp\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "from mne.decoding import CSP\n",
    "import pymrmr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.signal import kaiser\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC as SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_channels = 64\n",
    "epoch_length = 1123\n",
    "SWITCH_WINDOW_LENGTH = epoch_length\n",
    "CONTINIOUS_WINDOW_LENGTH = 1000\n",
    "sampling_freq = 250\n",
    "number_of_runs = 10\n",
    "number_of_splits = 10\n",
    "number_of_components = 10\n",
    "number_of_selected_features = 10\n",
    "number_of_processes = 15\n",
    "number_of_bands = 9\n",
    "# rf = pd.DataFrame()\n",
    "column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n",
    "column_names_v2 = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block','test_acc_vote']\n",
    "\n",
    "# rf = rf.reindex(columns=column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_csp(x_train, y_train, x_test):\n",
    "    \n",
    "        csp = CSP(number_of_components)\n",
    "        csp_fit = csp.fit(x_train, y_train)\n",
    "        train_feat = csp_fit.transform(x_train)\n",
    "        test_feat = csp_fit.transform(x_test)\n",
    "        return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_extraction(number_of_epochs, class_1, class_2, data, labels):\n",
    "    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n",
    "    Final_labels = np.zeros((size,1)).astype(int)\n",
    "    dataset = np.zeros((size,number_of_channels, epoch_length))\n",
    "    index = 0\n",
    "    for i in range(number_of_epochs):\n",
    "        if labels[i,0] == class_1 or labels[i,0] == class_2:\n",
    "            dataset[index,:,:] = data[i,:,:]\n",
    "            Final_labels[index,0] = labels[i,0]\n",
    "            index = index + 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return dataset, Final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset, labels, number_of_bands, test_data):\n",
    "\n",
    "    low_cutoff = 0\n",
    "    \n",
    "    for b in range(number_of_bands):\n",
    "        logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "        low_cutoff += 4\n",
    "        data = dataset.copy()\n",
    "        data_test = test_data.copy()\n",
    "        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test)\n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(train_features, labels, number_of_selected_features):\n",
    "    X = pd.DataFrame(train_features)\n",
    "    y = pd.DataFrame(labels)\n",
    "    K = number_of_selected_features\n",
    "    \n",
    "    df = pd.concat([y,X], axis = 1)\n",
    "    df.columns = df.columns.astype(str)\n",
    "        \n",
    "    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(path):\n",
    "    data = mne.read_epochs_eeglab(path)\n",
    "    data = data.to_data_frame()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_prev_v2(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        start_index = max(0, i - window_size + 1)\n",
    "        window = prediction_list[start_index:i+1]\n",
    "        counts = Counter(window)\n",
    "        majority = counts.most_common(1)[0][0]\n",
    "        majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(y_true, y_pred):\n",
    "    mismatches = []\n",
    "    total = len(y_true)\n",
    "    mismatch_count = 0\n",
    "    \n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            mismatches.append(i)\n",
    "            mismatch_count += 1\n",
    "            \n",
    "    accuracy = 1 - (mismatch_count / total)\n",
    "    \n",
    "    return accuracy, mismatch_count, mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_average(path,p_num_list,class_list):\n",
    "    PATH = path\n",
    "    vf = pd.DataFrame(columns=column_names_v2) \n",
    "    for p_num in p_num_list:\n",
    "        print(p_num)\n",
    "        rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n",
    "        vf = pd.concat([vf, rf], ignore_index=True)\n",
    "    vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n",
    "\n",
    "    columnNames = ['class','test_acc','vote_acc']\n",
    "    kf = pd.DataFrame(columns=columnNames)\n",
    "    kf.to_csv(PATH+'AverageAcc.csv',index=False)\n",
    "    vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n",
    "    df = vf\n",
    "    blk_list = [1234]\n",
    "    for class_ in class_list:\n",
    "        gf = df[(df['class1'] == class_)]\n",
    "        avg_test = gf['test_acc'].mean()\n",
    "        avg_vote = gf['test_acc_vote'].mean() \n",
    "        new_row = [class_, avg_test,avg_vote] \n",
    "        new_row_df = pd.DataFrame([new_row], columns=columnNames)\n",
    "        rf = pd.read_csv(PATH + 'AverageAcc.csv')\n",
    "        cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "        cf.to_csv(PATH +'AverageAcc.csv',index=False)  \n",
    "    kf = pd.read_csv(PATH +'AverageAcc.csv') \n",
    "    print(kf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label_attacher(data,class_1,class_2,window_type):\n",
    "    extra_points =   SWITCH_WINDOW_LENGTH - CONTINIOUS_WINDOW_LENGTH\n",
    "    SLIDING_POINTS = SWITCH_WINDOW_LENGTH\n",
    "    df = data.copy()\n",
    "    # print(df.head())\n",
    "    df_len = len(data)\n",
    "    number_of_epochs = int(df_len//epoch_length)\n",
    "    print(number_of_epochs)\n",
    "    # print(df.head(11235))\n",
    "    mask = df['condition'] == class_2\n",
    "    first_occurrence_index = mask.idxmax()\n",
    "    # print(first_occurrence_index)\n",
    "    # print(data.iloc[first_occurrence_index-1:].head())\n",
    "\n",
    "\n",
    "    X = df[df.columns[3:]].to_numpy()\n",
    "    X = np.transpose(X)\n",
    "\n",
    "    \n",
    "    dataset = np.zeros((number_of_epochs,number_of_channels,epoch_length))\n",
    "    labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "    k = 0\n",
    "    startIdx = int(k * epoch_length)\n",
    "    endIdx = int((k+1) * epoch_length )\n",
    "    label = 1\n",
    "    for i in range(number_of_epochs):\n",
    "            if (i>=number_of_epochs/2):\n",
    "                 label = 0\n",
    "            slice_X = X[:, startIdx:endIdx]\n",
    "            if window_type.window_name == \"Kaiser\":\n",
    "                kaiser_window = kaiser(epoch_length,window_type.param_value)\n",
    "                slice_X *= kaiser_window\n",
    "\n",
    "            # elif window_type.window_name == \"Hamming\":\n",
    "            #     hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "            #     slice_X *= hamming_window\n",
    "            \n",
    "            # elif window_type.window_name == \"Hanning\":\n",
    "            #     hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "            #     slice_X *= hanning_window\n",
    "            \n",
    "            elif window_type.window_name == \"Rec\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Window type is wrong!\")\n",
    "            dataset[i, :, :] = slice_X\n",
    "            labels[i,0] = label\n",
    "\n",
    "            startIdx+=SLIDING_POINTS\n",
    "            endIdx+=SLIDING_POINTS\n",
    "    modified_dataset = dataset[:, :, extra_points:]\n",
    "    print(modified_dataset.shape,\"dataset shape\")\n",
    "\n",
    "    return modified_dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(new_row, path):\n",
    "    absolute_path = os.path.abspath(path)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(absolute_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    try:\n",
    "        rf = pd.read_csv(absolute_path)\n",
    "    except FileNotFoundError:\n",
    "        rf = pd.DataFrame(columns=column_names_v2)\n",
    "\n",
    "    new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "    cf.to_csv(absolute_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Window:\n",
    "    def __init__(self,param_value,param_name,window_name):\n",
    "        self.param_value = param_value\n",
    "        self.param_name = param_name\n",
    "        self.window_name = window_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P2.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P3.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n",
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Channel names are not unique, found duplicates for: {'FCz'}. Applying running numbers for duplicates.\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "488 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P4.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n",
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Channel names are not unique, found duplicates for: {'FCz'}. Applying running numbers for duplicates.\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "491 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P5.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "489 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P6.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "489 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P7.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P8.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P10.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P11.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P12.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P13.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "488 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P14.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /project/6067835/mahdi146/Cedar/Data/P15.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29826/2716586413.py:2: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  data = mne.read_epochs_eeglab(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "490 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "p_num_list = [2,3,4,5,6,7,8,10,11,12,13,14,15]\n",
    "data_dict = {}\n",
    "for P_num in p_num_list:\n",
    "    data = data_reader('/project/6067835/mahdi146/Cedar/Data/P'+str(P_num)+'.set')\n",
    "    data_dict[P_num] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56 selected_epochs_1\n",
      "56 56 selected_epochs_2\n",
      "28\n",
      "(28, 64, 1000) dataset shape\n",
      "112\n",
      "(112, 64, 1000) dataset shape\n",
      "(28, 90) X_train_features.shape\n",
      "(28, 1) y_train.shape\n",
      "(28, 10) (28,)\n",
      "[40, 38, 25, 16, 51, 80, 50, 30, 22, 12]\n",
      "classifier_name = SVM\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "window_type = Rec\n",
      "window_param_name = No_Params,window_param_value = 0 \n",
      "train_blk_name = 1\n",
      "Participant = 2\n",
      "[1.0] train Hand\n",
      "[0.9464285714285714] test Hand\n",
      "56 56 selected_epochs_1\n",
      "56 56 selected_epochs_2\n",
      "28\n",
      "(28, 64, 1000) dataset shape\n",
      "112\n",
      "(112, 64, 1000) dataset shape\n",
      "(28, 90) X_train_features.shape\n",
      "(28, 1) y_train.shape\n",
      "(28, 10) (28,)\n",
      "[30, 0, 3, 50, 42, 12, 4, 14, 13, 25]\n",
      "classifier_name = SVM\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "window_type = Rec\n",
      "window_param_name = No_Params,window_param_value = 0 \n",
      "train_blk_name = 1\n",
      "Participant = 2\n",
      "[1.0] train Feet\n",
      "[0.8928571428571429] test Feet\n",
      "56 56 selected_epochs_1\n",
      "56 56 selected_epochs_2\n",
      "28\n",
      "(28, 64, 1000) dataset shape\n",
      "112\n",
      "(112, 64, 1000) dataset shape\n",
      "(28, 90) X_train_features.shape\n",
      "(28, 1) y_train.shape\n",
      "(28, 10) (28,)\n",
      "[22, 11, 9, 15, 6, 16, 88, 27, 30, 37]\n",
      "classifier_name = SVM\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "window_type = Rec\n",
      "window_param_name = No_Params,window_param_value = 0 \n",
      "train_blk_name = 1\n",
      "Participant = 2\n",
      "[1.0] train Tongue\n",
      "[0.8303571428571429] test Tongue\n",
      "56 56 selected_epochs_1\n",
      "56 56 selected_epochs_2\n",
      "28\n",
      "(28, 64, 1000) dataset shape\n",
      "112\n",
      "(112, 64, 1000) dataset shape\n",
      "tive minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 9 \t 8 \t 1.000\n",
      "2 \t 15 \t 14 \t 1.000\n",
      "3 \t 22 \t 21 \t 1.000\n",
      "4 \t 20 \t 19 \t 1.000\n",
      "5 \t 19 \t 18 \t 1.000\n",
      "6 \t 32 \t 31 \t 1.000\n",
      "7 \t 10 \t 9 \t 1.000\n",
      "8 \t 42 \t 41 \t 1.000\n",
      "9 \t 8 \t 7 \t 1.000\n",
      "10 \t 4 \t 3 \t 1.000\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 9 \t 8 \t 1.000\n",
      "2 \t 15 \t 14 \t 0.000\n",
      "3 \t 22 \t 21 \t 0.000\n",
      "4 \t 13 \t 12 \t 0.000\n",
      "5 \t 19 \t 18 \t 0.074\n",
      "6 \t 32 \t 31 \t 0.059\n",
      "7 \t 10 \t 9 \t 0.049\n",
      "8 \t 42 \t 41 \t 0.042\n",
      "9 \t 8 \t 7 \t 0.037\n",
      "10 \t 4 \t 3 \t 0.033\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRM(28, 90) X_train_features.shape\n",
      "(28, 1) y_train.shape\n",
      "(28, 10) (28,)\n",
      "[13, 9, 6, 31, 14, 22, 4, 41, 23, 52]\n",
      "R) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 41 \t 40 \t 1.000\n",
      "2 \t 26 \t 25 \t 1.000\n",
      "3 \t 17 \t 16 \t 1.000\n",
      "4 \t 23 \t 22 \t 1.000\n",
      "5 \t 13 \t 12 \t 1.000\n",
      "6 \t 11 \t 10 \t 1.000\n",
      "7 \t 21 \t 20 \t 1.000\n",
      "8 \t 52 \t 51 \t 1.000\n",
      "9 \t 81 \t 80 \t 1.000\n",
      "10 \t 22 \t 21 \t 1.000\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 41 \t 40 \t 1.000\n",
      "2 \t 39 \t 38 \t 0.000\n",
      "3 \t 26 \t 25 \t 0.246\n",
      "4 \t 17 \t 16 \t 0.164\n",
      "5 \t 52 \t 51 \t 0.123\n",
      "6 \t 81 \t 80 \t 0.098\n",
      "7 \t 51 \t 50 \t 0.082\n",
      "8 \t 31 \t 30 \t 0.070\n",
      "9 \t 23 \t 22 \t 0.061\n",
      "10 \t 13 \t 12 \t 0.053\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 31 \t 30 \t 1.000\n",
      "2 \t 4 \t 3 \t 1.000\n",
      "3 \t 51 \t 50 \t 1.000\n",
      "4 \t 43 \t 42 \t 1.000\n",
      "5 \t 13 \t 12 \t 1.000\n",
      "6 \t 12 \t 11 \t 0.818\n",
      "7 \t 14 \t 13 \t 0.814\n",
      "8 \t 36 \t 35 \t 0.811\n",
      "9 \t 15 \t 14 \t 0.811\n",
      "10 \t 10 \t 9 \t 0.811\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 31 \t 30 \t 1.000\n",
      "2 \t 1 \t 0 \t 0.000\n",
      "3 \t 4 \t 3 \t 0.147\n",
      "4 \t 51 \t 50 \t 0.098\n",
      "5 \t 43 \t 42 \t 0.073\n",
      "6 \t 13 \t 12 \t 0.059\n",
      "7 \t 5 \t 4 \t 0.047\n",
      "8 \t 15 \t 14 \t 0.067\n",
      "9 \t 14 \t 13 \t 0.080\n",
      "10 \t 26 \t 25 \t 0.090\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 23 \t 22 \t 1.000\n",
      "2 \t 10 \t 9 \t 1.000\n",
      "3 \t 16 \t 15 \t 0.811\n",
      "4 \t 22 \t 21 \t 0.811\n",
      "5 \t 7 \t 6 \t 0.811\n",
      "6 \t 15 \t 14 \t 0.811\n",
      "7 \t 45 \t 44 \t 0.811\n",
      "8 \t 28 \t 27 \t 0.811\n",
      "9 \t 31 \t 30 \t 0.811\n",
      "10 \t 24 \t 23 \t 0.811\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 23 \t 22 \t 1.000\n",
      "2 \t 12 \t 11 \t 0.000\n",
      "3 \t 10 \t 9 \t 0.152\n",
      "4 \t 16 \t 15 \t 0.098\n",
      "5 \t 7 \t 6 \t 0.120\n",
      "6 \t 17 \t 16 \t 0.130\n",
      "7 \t 89 \t 88 \t 0.141\n",
      "8 \t 28 \t 27 \t 0.174\n",
      "9 \t 31 \t 30 \t 0.175\n",
      "10 \t 38 \t 37 \t 0.161\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-classifier_name = SVM\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "window_type = Rec\n",
      "window_param_name = No_Params,window_param_value = 0 \n",
      "train_blk_name = 1\n",
      "Participant = 2\n",
      "[1.0] train Mis\n",
      "[0.6339285714285714] test Mis\n",
      "56 56 selected_epochs_1\n",
      "56 56 selected_epochs_2\n",
      "28\n",
      "(28, 64, 1000) dataset shape\n",
      "112\n",
      "(112, 64, 1000) dataset shape\n"
     ]
    }
   ],
   "source": [
    "vote_window = 3\n",
    "classifier_dic = {\"SVM\":SVM()}\n",
    "window_type_list = [Window(0,\"No_Params\",\"Rec\")]\n",
    "train_blk_set_dic = {\"1\":[1],\"12\":[1,2],\"123\":[1,2,3],\"1234\":[1,2,3,4]}\n",
    "test_blk_set_dic = {\"2345\":[2,3,4,5],\"345\":[3,4,5],\"45\":[4,5],\"5\":[5]}\n",
    "# number_of_test_blk = 1\n",
    "epoch_slide_length = 14\n",
    "# test_blk_set_dic = {\"7_1\":[7],\"7_2\":[7],\"7_3\":[7],\"7_4\":[7]}\n",
    "# p_num_list = [3]\n",
    "p_num_list = [2,3,4,5,6,7,8,10,11,12,13,14,15]\n",
    "class_1_list = ['Right','Feet','Tongue','Mis']\n",
    "class_2 = 'Rest'\n",
    "PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG-v2/EEG/Results-S-Dataset-TestOnRest'\n",
    "# print(len(data))\n",
    "\n",
    "for classifier, classifier_name in zip(classifier_dic.values(),classifier_dic.keys()):\n",
    "    for window_type in window_type_list:\n",
    "        for train_blk_set,train_blk_name,test_blk_set,test_blk_name in zip(train_blk_set_dic.values(),train_blk_set_dic.keys(),test_blk_set_dic.values(),test_blk_set_dic.keys()):\n",
    "            class_1_list = ['Right','Feet','Tongue','Mis']\n",
    "            for p_num in p_num_list:\n",
    "                #Left handed participants\n",
    "                if p_num in [1,7,9]:\n",
    "                    class_1_list = ['Left','Feet','Tongue','Mis']\n",
    "                data = data_dict[p_num]\n",
    "                for index, class_1 in enumerate(class_1_list):\n",
    "                    import time\n",
    "                    start_time = time.time()\n",
    "                    class_1_data = data.copy()\n",
    "                    class_2_data = data.copy()\n",
    "\n",
    "                    class_1_data = class_1_data[class_1_data['condition']== class_1]\n",
    "                    class_2_data = class_2_data[class_2_data['condition']== class_2]\n",
    "                    df_1 = class_1_data.copy()\n",
    "                    df_2 = class_2_data.copy()\n",
    "\n",
    "                    # Train partitioning\n",
    "                    unique_epochs_ = df_1['epoch'].unique()\n",
    "                    epoch_mapping_ = {old_epoch: new_epoch for new_epoch, old_epoch in enumerate(unique_epochs_)}\n",
    "                    df_1['epoch'] = df_1['epoch'].map(epoch_mapping_)\n",
    "\n",
    "                    unique_epochs = df_2['epoch'].unique()\n",
    "                    epoch_mapping = {old_epoch: new_epoch for new_epoch, old_epoch in enumerate(unique_epochs)}\n",
    "                    df_2['epoch'] = df_2['epoch'].map(epoch_mapping)\n",
    "\n",
    "                    cte_tr = len(train_blk_set)\n",
    "                    starting_epoch_tr_1 = df_1.iloc[0,2]\n",
    "                    starting_epoch_tr_2 = df_2.iloc[0,2]\n",
    "                    end_epoch_tr_2 = end_epoch_tr_1  = cte_tr*epoch_slide_length-1\n",
    "\n",
    "\n",
    "                    data_1_tr = df_1[(df_1['epoch'] >= starting_epoch_tr_1) & (df_1['epoch'] <= end_epoch_tr_1)]\n",
    "                    data_2_tr = df_2[(df_2['epoch'] >= starting_epoch_tr_2) & (df_2['epoch'] <= end_epoch_tr_2)]\n",
    "\n",
    "                    data_tr = pd.concat([data_1_tr, data_2_tr])\n",
    "                    data_tr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "                    #Test partitioning\n",
    "                    df_1_cut = df_1[df_1['epoch']>end_epoch_tr_1]\n",
    "                    df_2_cut = df_2[df_2['epoch']>end_epoch_tr_2]\n",
    "                    unique_epochs_te_1 = df_1_cut['epoch'].unique()\n",
    "                    unique_epochs_te_2 = df_2_cut['epoch'].unique()\n",
    "\n",
    "                    number_of_test_blk = len(test_blk_set)\n",
    "                    np.random.seed(42)\n",
    "                    num_of_test_samples_2 = num_of_test_samples_1 = epoch_slide_length*number_of_test_blk\n",
    "\n",
    "                    if(num_of_test_samples_1>len(unique_epochs_te_1)):\n",
    "                        num_of_test_samples_1 = len(unique_epochs_te_1)\n",
    "                    else:\n",
    "                        num_of_test_samples_1 = epoch_slide_length*number_of_test_blk\n",
    "                    \n",
    "                    if(num_of_test_samples_2>len(unique_epochs_te_2)):\n",
    "                        num_of_test_samples_2 = len(unique_epochs_te_2)\n",
    "                    else:\n",
    "                        num_of_test_samples_2 = epoch_slide_length*number_of_test_blk\n",
    "                    \n",
    "                    print(num_of_test_samples_1,len(unique_epochs_te_1),\"selected_epochs_1\")\n",
    "                    print(num_of_test_samples_2,len(unique_epochs_te_2),\"selected_epochs_2\")\n",
    "\n",
    "                    selected_epochs_1 = np.random.choice(unique_epochs_te_1, size=num_of_test_samples_1, replace=False)\n",
    "                    selected_epochs_2 = np.random.choice(unique_epochs_te_2, size=num_of_test_samples_2, replace=False)\n",
    "\n",
    "                    data_1_te = df_1_cut[df_1_cut['epoch'].isin(selected_epochs_1)]\n",
    "\n",
    "                    data_2_te = df_2_cut[df_2_cut['epoch'].isin(selected_epochs_2)]\n",
    "\n",
    "                    data_te = pd.concat([data_1_te, data_2_te])\n",
    "                    data_te.reset_index(drop=True, inplace=True)\n",
    "                    X_train,y_train = data_label_attacher(data_tr,class_1,class_2,window_type)\n",
    "                    X_test,y_test = data_label_attacher(data_te,class_1,class_2,window_type)\n",
    "                    # print(y_train, y_test)\n",
    "\n",
    "\n",
    "\n",
    "                    [X_train_features, X_test_features] = feature_extraction(X_train, y_train, number_of_bands, X_test)                          \n",
    "                    selected_features = feature_selection(X_train_features, y_train, number_of_selected_features)  \n",
    "                    print(X_train_features.shape,\"X_train_features.shape\")\n",
    "                    print(y_train.shape,\"y_train.shape\")\n",
    "                    # clf = RF(n_jobs=-1,random_state=42)\n",
    "                    # clf = RF(n_jobs=-1)\n",
    "                    clf = classifier\n",
    "                    runs = 1\n",
    "                    train_acc_list = []\n",
    "                    test_acc_list = []\n",
    "                    vote_acc_list = []\n",
    "                    # selected_features = np.arange(0,X_train_features.shape[1],1)\n",
    "                    for r in range(runs):\n",
    "                        if np.isnan(X_train_features[:, selected_features]).any() or np.isinf(X_train_features[:, selected_features]).any():\n",
    "                            raise ValueError(\"Input data contains NaNs or Infs.\")\n",
    "                        print(X_train_features[:, selected_features].shape, y_train[:,0].shape)\n",
    "                        print(selected_features)\n",
    "                        clf.fit(X_train_features[:, selected_features], y_train[:,0])\n",
    "\n",
    "                        y_pr_te = clf.predict(X_test_features[:, selected_features])\n",
    "                        y_pr_tr = clf.predict(X_train_features[:,selected_features])\n",
    "\n",
    "                        accuracy_te = accuracy_score(y_test, y_pr_te)\n",
    "                        test_acc_list.append(accuracy_te)\n",
    "\n",
    "                        accuracy_tr = accuracy_score(y_train,y_pr_tr)\n",
    "                        train_acc_list.append(accuracy_tr)\n",
    "\n",
    "                        y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "                        Y_te_Vote = majority_vote_sliding_with_prev_v2(y_test.reshape(-1),vote_window)\n",
    "                        vote_acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "                        vote_acc_list.append(vote_acc)\n",
    "                    if class_1 == 'Right' or class_1 == 'Left':\n",
    "                        class_1 = 'Hand'\n",
    "                    end_time = time.time()\n",
    "                    running_time = end_time-start_time\n",
    "                    participant = p_num\n",
    "                    class1 = class_1\n",
    "                    class2 = class_2\n",
    "                    running_time = running_time\n",
    "                    test_acc = np.average(test_acc_list)\n",
    "                    train_acc = np.average(train_acc_list)\n",
    "                    vote_acc = np.average(vote_acc_list)\n",
    "                    test_size = X_test.shape\n",
    "                    train_size = X_train.shape\n",
    "                    train_block = train_blk_name\n",
    "                    #TO DO check the test_blk_name  \n",
    "                    test_block = f'{X_test.shape[0]}'\n",
    "                    new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,vote_acc]\n",
    "                    \n",
    "                    path = os.path.join(\n",
    "                    PATH,\n",
    "                    classifier_name,\n",
    "                    f\"{number_of_components}_CSP_Components\",\n",
    "                    f\"{number_of_selected_features}-Selected_Features\",\n",
    "                    f\"{window_type.window_name}_Window\",\n",
    "                    f\"{window_type.param_name}\",\n",
    "                    f\"{window_type.param_value}_{window_type.param_name}\",\n",
    "                    f\"{train_blk_name}_Train/\"\n",
    "                    )\n",
    "                    if index == 0:\n",
    "                        # clean_csv(path + f\"P{p_num}.csv\")\n",
    "                        save_csv(new_row, path + f\"P{p_num}.csv\")\n",
    "                    else:                                \n",
    "                        save_csv(new_row, path + f\"P{p_num}.csv\")\n",
    "                    \n",
    "                    print(\n",
    "                    f\"classifier_name = {classifier_name}\\n\"\n",
    "                    f\"number_of_components = {number_of_components}\\n\"\n",
    "                    f\"number_of_selected_features = {number_of_selected_features}\\n\"\n",
    "                    f\"window_type = {window_type.window_name}\\n\"\n",
    "                    f\"window_param_name = {window_type.param_name},window_param_value = {window_type.param_value} \\n\"\n",
    "                    f\"train_blk_name = {train_blk_name}\\n\"\n",
    "                    f\"Participant = {p_num}\"\n",
    "                    )\n",
    "\n",
    "                    print(train_acc_list,\"train\",class_1)\n",
    "                    print(test_acc_list,\"test\",class_1)\n",
    "                    \n",
    "            class_1_list = ['Hand','Feet','Tongue','Mis']\n",
    "            get_results_average(path,p_num_list,class_1_list)\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel2",
   "language": "python",
   "name": "kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
