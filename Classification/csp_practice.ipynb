{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_csp(x_train, y_train, x_test,number_of_components):\n",
    "    # csp = CSP(n_components=number_of_components, reg='ledoit_wolf', log=True)\n",
    "    csp = CSP(number_of_components)\n",
    "\n",
    "\n",
    "    csp_fit = csp.fit(x_train, y_train)\n",
    "    train_feat = csp_fit.transform(x_train)\n",
    "    test_feat = csp_fit.transform(x_test)\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_data_modifier(data,label):\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_v3(dataset, labels, number_of_bands, test_data_,number_of_components):\n",
    "\n",
    "    low_cutoff = 0\n",
    "    number_of_bands = 3\n",
    "    slide = 4\n",
    "    sampling_freq = 250\n",
    "    train_data = dataset.copy()\n",
    "    test_data = test_data_.copy()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    for b in range(number_of_bands):\n",
    "        logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "        low_cutoff += slide\n",
    "        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + slide, verbose = False, n_jobs = 4)\n",
    "        filtered_data_test = mne.filter.filter_data(test_data_, sampling_freq, low_cutoff, low_cutoff + slide, verbose = False, n_jobs = 4)\n",
    "    \n",
    "\n",
    "        if low_cutoff == 12:\n",
    "            train_data_set1 = train_data[:, :, :250]\n",
    "            test_data_set1 = test_data[:, :, :250]\n",
    "            train_feats,test_feats = temp_func(train_data_set1, labels, test_data_set1,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "            train_data_set2 = train_data[:, :, 250:500]\n",
    "            test_data_set2 = test_data[:, :, 250:500]\n",
    "            train_feats,test_feats = temp_func(train_data_set2, labels, test_data_set2,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "            train_data_set3 = train_data[:, :, 500:750]\n",
    "            test_data_set3 = test_data[:, :, 500:750]\n",
    "            train_feats,test_feats = temp_func(train_data_set3, labels, test_data_set3,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "            train_data_set4 = train_data[:, :, 750:1000]\n",
    "            test_data_set4 = test_data[:, :, 750:1000]\n",
    "            train_feats,test_feats = temp_func(train_data_set4, labels, test_data_set4,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "\n",
    "        elif low_cutoff == 8:\n",
    "            train_data_set1 = train_data[:, :, :500]\n",
    "            test_data_set1 = test_data[:, :, :500]\n",
    "            train_features,test_features = temp_func(train_data_set1, labels, test_data_set1,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "            train_data_set2 = train_data[:, :, 500:1000]\n",
    "            test_data_set2 = test_data[:, :, 500:1000]\n",
    "            train_feats,test_feats = temp_func(train_data_set2, labels, test_data_set2,10)\n",
    "\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "\n",
    "            print(train_features.shape,\"shape shape\")\n",
    "\n",
    "        \n",
    "        elif low_cutoff == 4:\n",
    "            train_data_set1 = train_data[:, :, :1000]\n",
    "            test_data_set1 = test_data[:, :, :1000]\n",
    "            train_feats,test_feats = temp_func(train_data_set1, labels, test_data_set1,10)\n",
    "\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_func(dataset, labels, test_data,number_of_components):\n",
    "    \n",
    "    data = dataset.copy()\n",
    "    data_test = test_data.copy() \n",
    "    # print(\"Frequency range: \",low_cutoff)\n",
    "    # filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + slide, verbose = False, n_jobs = 4)\n",
    "    # filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + slide, verbose = False, n_jobs = 4)\n",
    "    # print(filtered_data.shape,\"filtered_data\")\n",
    "    # print(filtered_data_test.shape,\"filtered_data_test\")\n",
    "    filtered_data = data\n",
    "    filtered_data_test = data_test\n",
    "\n",
    "    \n",
    "    [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test,number_of_components)\n",
    "    return train_feats, test_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 30) shape shape\n",
      "(8, 70) train_features\n",
      "(2, 70) train_features\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate random train data and labels\n",
    "train_data = np.random.rand(8, 64, 1000)\n",
    "train_labels = np.random.rand(8, 1)\n",
    "\n",
    "# Generate random test data and labels\n",
    "test_data = np.random.rand(2, 64, 1000)\n",
    "test_labels = np.random.rand(2, 1)\n",
    "\n",
    "# train_data_set1 = train_data[:, :, :250]\n",
    "# train_data_set2 = train_data[:, :, 250:500]\n",
    "# train_data_set3 = train_data[:, :, 500:750]\n",
    "# train_data_set4 = train_data[:, :, 750:1000]\n",
    "# combined_train_data = np.concatenate([train_data_set1, train_data_set2,train_data_set3,train_data_set4], axis=0)\n",
    "\n",
    "# print(combined_train_data.shape,\"combined_train_data\")\n",
    "\n",
    "train_features,test_features =  feature_extractor_v3(train_data,train_labels,1,test_data,10)\n",
    "print(train_features.shape,\"train_features\")\n",
    "print(test_features.shape,\"train_features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
