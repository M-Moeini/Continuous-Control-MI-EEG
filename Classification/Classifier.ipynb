{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696289517013,
     "user": {
      "displayName": "Mahdi Moeini",
      "userId": "03671813669356560168"
     },
     "user_tz": -210
    },
    "id": "za0kvkt7u2Z5",
    "outputId": "78ba6e7e-7a2c-4096-e595-beca84ab98ba"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import mne\n",
    "import scipy.io as sp\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "from mne.decoding import CSP\n",
    "import pymrmr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import hamming\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import blackman\n",
    "from scipy.signal import kaiser\n",
    "from scipy.signal import gaussian\n",
    "from sklearn.decomposition import FastICA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pyriemann.estimation import Covariances\n",
    "# from pyriemann.tangentspace import TangentSpace\n",
    "# from pyriemann.classification import MDM\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import medfilt\n",
    "import os\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "# Set display options for NumPy\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_TIME_LENGTH = 4\n",
    "SAMPLING_RATE = 250\n",
    "NUMBER_OF_CHANNELS = 64\n",
    "beta = 1.5\n",
    "\n",
    "epoch_length = 1000\n",
    "sampling_freq = 250\n",
    "number_of_runs = 10\n",
    "number_of_components = 10\n",
    "number_of_selected_features = 10\n",
    "number_of_processes = 10\n",
    "number_of_bands = 9\n",
    "column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n",
    "column_names_v2 = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block','test_acc_vote']\n",
    "\n",
    "\n",
    "trial_order=[['Tongue','Feet','Mis','Hand'],\n",
    "            ['Feet','Mis','Hand','Tongue'],\n",
    "            ['Hand','Feet','Tongue','Mis'],\n",
    "            ['Tongue','Mis','Hand','Feet'],\n",
    "            ['Mis','Feet','Hand','Tongue'],\n",
    "            ['Feet','Hand','Tongue','Mis'],\n",
    "            ['Hand','Tongue','Mis','Feet'],\n",
    "            ['Tongue','Feet','Mis','Hand'],\n",
    "            ['Mis','Tongue','Hand','Feet']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_rest_times(b_num):\n",
    "    if b_num == 0:\n",
    "        task_time = [[12, 16, 20, 8],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "        rest_time = [[20, 8, 16, 12],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [12, 20, 16, 8],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "    elif b_num == 1:\n",
    "        task_time = [[12, 8, 20, 16],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [8, 20, 16, 12],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "        rest_time = [[16, 12, 8, 20],\n",
    "                    [8, 20, 12, 16],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [12, 16, 20, 8]]\n",
    "        \n",
    "    elif b_num == 2:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [8, 12, 16, 20]]\n",
    "        \n",
    "        rest_time = [[8, 20, 16, 12],\n",
    "                    [12, 8, 20, 16],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "    elif b_num == 3:\n",
    "        task_time = [[12, 16, 20, 8],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "        rest_time = [[20, 8, 16, 12],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [12, 20, 16, 8],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "    elif b_num == 4:\n",
    "        task_time = [[16, 8, 20, 12],\n",
    "                    [12, 16, 8, 20],\n",
    "                    [20, 8, 12, 16],\n",
    "                    [8, 20, 12, 16]]\n",
    "        \n",
    "        rest_time = [[8, 12, 16, 20],\n",
    "                    [16, 20, 12, 8],\n",
    "                    [12, 16, 8, 20],\n",
    "                    [20, 8, 12, 16]]\n",
    "        \n",
    "    elif b_num == 5:\n",
    "        task_time = [[16, 12, 8, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [8, 16, 20, 12],\n",
    "                    [12, 8, 16, 20]]\n",
    "\n",
    "        rest_time = [[12, 8, 16, 20],\n",
    "                    [16, 8, 20, 12],\n",
    "                    [20, 12, 16, 8],\n",
    "                    [8, 16, 12, 20]]\n",
    "        \n",
    "    elif b_num == 6:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 8, 16, 12],\n",
    "                    [8, 16, 12, 20],\n",
    "                    [16, 20, 12, 8]]\n",
    "\n",
    "        rest_time = [[16, 8, 12, 20],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [8, 16, 20, 12]]     \n",
    "    elif b_num ==7:\n",
    "        task_time = [[12, 8, 20, 16],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [8, 20, 16, 12],\n",
    "                    [8, 12, 20, 16]]   \n",
    "               \n",
    "        rest_time = [[16, 12, 8, 20],\n",
    "                    [8, 20, 12, 16],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [12, 16, 20, 8]]  \n",
    "    \n",
    "    elif b_num == 8:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [8, 12, 16, 20]]\n",
    "        \n",
    "        rest_time = [[8, 20, 16, 12],\n",
    "                    [12, 8, 20, 16],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "    else:\n",
    "        raise(\"Error in block number\")\n",
    "    \n",
    "\n",
    "    return task_time,rest_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_times_genertor(task_times,rest_times):\n",
    "    block_times = [item for pair in zip(task_times, rest_times) for item in pair]\n",
    "    return block_times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_zeros_with_average(matrix):\n",
    "    # Iterate through the matrix\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            for k in range(matrix.shape[2]):\n",
    "                if matrix[i, j, k] == 0:\n",
    "                    # Find the neighboring non-zero elements\n",
    "                    neighbors = []\n",
    "                    if i > 0 and matrix[i - 1, j, k] != 0:\n",
    "                        neighbors.append(matrix[i - 1, j, k])\n",
    "                    if i < matrix.shape[0] - 1 and matrix[i + 1, j, k] != 0:\n",
    "                        neighbors.append(matrix[i + 1, j, k])\n",
    "                    if j > 0 and matrix[i, j - 1, k] != 0:\n",
    "                        neighbors.append(matrix[i, j - 1, k])\n",
    "                    if j < matrix.shape[1] - 1 and matrix[i, j + 1, k] != 0:\n",
    "                        neighbors.append(matrix[i, j + 1, k])\n",
    "                    if k > 0 and matrix[i, j, k - 1] != 0:\n",
    "                        neighbors.append(matrix[i, j, k - 1])\n",
    "                    if k < matrix.shape[2] - 1 and matrix[i, j, k + 1] != 0:\n",
    "                        neighbors.append(matrix[i, j, k + 1])\n",
    "\n",
    "                    # Fill the zero with the average of neighboring non-zero values\n",
    "                    if neighbors:\n",
    "                        matrix[i, j, k] = sum(neighbors) / len(neighbors)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_csp(x_train, y_train, x_test,number_of_components):\n",
    "    # csp = CSP(n_components=number_of_components, reg='ledoit_wolf', log=True)\n",
    "    csp = CSP(number_of_components)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # x_train = fill_zeros_with_average(x_train)\n",
    "    # x_train = np.add(x_train, 0.000001)\n",
    "\n",
    "\n",
    "\n",
    "    # nan_count = np.isnan(x_train).sum()\n",
    "    # print(\"Number of NaN values:\", nan_count)\n",
    "\n",
    "    # empty_field_count = np.count_nonzero(x_train == 0)\n",
    "    # print(\"Number of empty fields:\", empty_field_count)\n",
    "\n",
    "    # zeros_locations_3d = np.where(x_train == 0)\n",
    "    # print(\"Locations of zeros:\", zeros_locations)\n",
    "    \n",
    "# Printing indices and corresponding values\n",
    "    # for depth_idx, row_idx, col_idx in zip(zeros_locations_3d[0], zeros_locations_3d[1], zeros_locations_3d[2]):\n",
    "    #     value_at_zero_location = x_train[depth_idx, row_idx, col_idx]\n",
    "    #     print(f\"Zero found at position ({depth_idx}, {row_idx}, {col_idx}) with value {value_at_zero_location}\")\n",
    "\n",
    "\n",
    "    csp_fit = csp.fit(x_train, y_train)\n",
    "    train_feat = csp_fit.transform(x_train)\n",
    "    test_feat = csp_fit.transform(x_test)\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_extractor(number_of_epochs, class_1, class_2, data, labels):\n",
    "    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n",
    "    Final_labels = np.zeros((size,1)).astype(int)\n",
    "    dataset = np.zeros((size,num_channels, epoch_length))\n",
    "    index = 0\n",
    "    for i in range(number_of_epochs):\n",
    "        if labels[i,0] == class_1 or labels[i,0] == class_2:\n",
    "            dataset[index,:,:] = data[i,:,:]\n",
    "            Final_labels[index,0] = labels[i,0]\n",
    "            index = index + 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return dataset, Final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(dataset, labels, number_of_bands, test_data,number_of_components):\n",
    "\n",
    "    low_cutoff = 0\n",
    "    \n",
    "    for b in range(number_of_bands):\n",
    "        logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "        low_cutoff += 4\n",
    "        data = dataset.copy()\n",
    "        data_test = test_data.copy() \n",
    "        print(\"Frequency range: \",low_cutoff)\n",
    "        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "\n",
    "        #PCA\n",
    "        # from mne.decoding import UnsupervisedSpatialFilter\n",
    "        # from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "        # pca = UnsupervisedSpatialFilter(PCA(64), average=False)\n",
    "        # pca_fit = pca.fit(filtered_data)\n",
    "        # filtered_data = pca_fit.transform(filtered_data)\n",
    "        # filtered_data_test = pca_fit.transform(filtered_data_test)\n",
    "        # train_feats = filtered_data\n",
    "        # test_feats = filtered_data_test\n",
    "\n",
    "        # filtered_data = data\n",
    "        # filtered_data_test = data_test\n",
    "        \n",
    "        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test,number_of_components)\n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(train_features, labels, number_of_selected_features):\n",
    "    X = pd.DataFrame(train_features)\n",
    "    y = pd.DataFrame(labels)\n",
    "    K = number_of_selected_features\n",
    "    \n",
    "    df = pd.concat([y,X], axis = 1)\n",
    "    df.columns = df.columns.astype(str)\n",
    "        \n",
    "    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(path,p_num,block_list):\n",
    "    data_dict = {}\n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "        df = pd.DataFrame(mat['Data'])\n",
    "        data_dict[b_num] = df\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_start_indices(dataframe):\n",
    "    group_indices = []\n",
    "    current_label = None\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        if row.iloc[-1] != current_label:\n",
    "            group_indices.append(idx)\n",
    "            current_label = row.iloc[-1]\n",
    "\n",
    "    return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_samples_counter(df,class_1,class_2):\n",
    "    x=0\n",
    "    i=0\n",
    "    sampleList = []\n",
    "    while i<len(df):\n",
    "        if (df.iloc[i,-1]==class_1):\n",
    "            x+=1\n",
    "        else:\n",
    "            i-=1\n",
    "            sampleList.append(x)\n",
    "            x=0\n",
    "            class_1,class_2 = class_2,class_1\n",
    "        i+=1\n",
    "    sampleList.append(x)\n",
    "    print(sampleList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_samples_block_counter(df,trial_order,b_num):\n",
    "\n",
    "    df.drop(df[df.iloc[:,-1].isin(['Begin', 'End'])].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)    \n",
    "    df['group'] = (df.iloc[:,-1] != df.iloc[:,-1].shift(1)).cumsum()\n",
    "\n",
    "    \n",
    "    group_counts_Rest = df[df.iloc[:,-1] == 'Rest'].groupby('group').size()\n",
    "    with open('sampleList.txt', 'a') as file:\n",
    "        file.write(f'block {b_num+1} '+'\\n')\n",
    "        for j in range (len(trial_order)):\n",
    "            print(trial_order[j])\n",
    "            trial_num = j\n",
    "            task_times,rest_times = get_task_rest_times(b_num)\n",
    "            trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n",
    "            trial_samples = [item*SAMPLING_RATE for item in trial_times]\n",
    "            group_counts_task = df[df.iloc[:,-1] == trial_order[j]].groupby('group').size()\n",
    "            sampleList = []\n",
    "            for i in range(4):\n",
    "                task = group_counts_task.iloc[i]\n",
    "                rest = group_counts_Rest.iloc[4*j+i]\n",
    "                sampleList.append(task)\n",
    "                sampleList.append(rest)\n",
    "            # extra_samples = [x-y for x,y in zip(sampleList,trial_samples)]\n",
    "            file.write(', '.join(map(str, sampleList)) + f' trial={trial_order[j]} '+'\\n')\n",
    "            print(sampleList)\n",
    "        file.write('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(df,class_1,class_2,tasks_time):\n",
    "    # sys.exit() \n",
    "    class_x = class_1\n",
    "    class_y = class_2\n",
    "    new_df = pd.DataFrame()\n",
    "    trial_df = df.copy() \n",
    "    # print(tasks_time)\n",
    "    for i in range(len(tasks_time)):\n",
    "        sample_point = tasks_time[i]*SAMPLING_RATE\n",
    "        if(trial_df.iloc[sample_point+1,-1] == class_x ):\n",
    "            if(i==len(tasks_time)-1):\n",
    "                temp_df = trial_df.iloc[:sample_point,:]\n",
    "                new_df = pd.concat([new_df, temp_df], axis=0)\n",
    "                new_df.reset_index(drop=True, inplace=True)\n",
    "            else:    \n",
    "                temp_df = trial_df.iloc[:sample_point,:]\n",
    "                next_task_idx = trial_df[trial_df.iloc[:, -1] == class_y].index\n",
    "                trial_df.drop(trial_df.index[0:next_task_idx[0]], inplace=True)\n",
    "                trial_df.reset_index(drop=True, inplace=True)\n",
    "                new_df = pd.concat([new_df, temp_df], axis=0)\n",
    "                new_df.reset_index(drop=True, inplace=True)\n",
    "                class_x,class_y = class_y,class_x\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_seperator(cleaned_df,class_1,class_2):\n",
    "    df = cleaned_df\n",
    "    sorting_order = {class_1: 0, class_2: 1}\n",
    "\n",
    "    df['sorting_order'] = df.iloc[:, -1].map(sorting_order)\n",
    "    df.sort_values(by=['sorting_order', df.columns[-1]], inplace=True)\n",
    "    df.drop('sorting_order', axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(dataset,labels):\n",
    "    print(dataset.shape)\n",
    "    print(labels.shape)\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    shuffled_dataset = dataset[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    return shuffled_dataset,shuffled_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_epoch(df_len,sliding_len,window_len):\n",
    "    # print(window_len,sliding_len,df_len)\n",
    "    number_of_epochs = int((int(df_len-window_len)/sliding_len)) +1\n",
    "    return number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label_attacher(cleaned_df,class_1,class_2,random_flag,class_seperator_flag,sliding_time,window_time_length,window_type,number_of_channels):\n",
    "    SLIDING_POINTS = int(sliding_time*SAMPLING_RATE)\n",
    "    window_time = window_time_length\n",
    "    WINDOW_SAMPLE_LENGTH = window_time*SAMPLING_RATE\n",
    "    new_df_ = cleaned_df.copy()\n",
    "    new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    X = new_df_.to_numpy()\n",
    "    X = np.transpose(X)\n",
    "    number_of_epochs = cal_epoch(int(int(len(cleaned_df)/SAMPLING_RATE)),sliding_time,window_time)\n",
    "    # print(number_of_epochs)\n",
    "    dataset = np.zeros((number_of_epochs,number_of_channels,WINDOW_SAMPLE_LENGTH))\n",
    "    labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "    index = get_group_start_indices(cleaned_df)\n",
    "    index.append(len(cleaned_df))\n",
    "    k = 0  \n",
    "    startIdx = int(k * WINDOW_SAMPLE_LENGTH)\n",
    "    endIdx = int((k+1) * WINDOW_SAMPLE_LENGTH )\n",
    "    l = 0\n",
    "    label = 1\n",
    "    for i in range(number_of_epochs):\n",
    "        \n",
    "        if(startIdx>=index[l] and endIdx<=index[l+1]):\n",
    "            # print(startIdx,endIdx,index[l],index[l+1],\"start, end, index[l], index[l+1] in if\")\n",
    "            slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "            if window_type == \"Kaiser\":\n",
    "                kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "                slice_X *= kaiser_window\n",
    "\n",
    "            elif window_type == \"Hamming\":\n",
    "                hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hamming_window\n",
    "            \n",
    "            elif window_type == \"Hanning\":\n",
    "                hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hanning_window\n",
    "            \n",
    "            elif window_type == \"Rec\":\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Window type is wrong!\")\n",
    "\n",
    "            dataset[i, :, :] = slice_X\n",
    "            labels[i,0] = label\n",
    "            # print(\"i is: \",i)\n",
    "            # print(\"label is: \",label)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            temp = endIdx-index[l+1]\n",
    "            # print(temp,endIdx,index[l+1],\"temp,end,index l+1\")\n",
    "            slice_X = X[:, startIdx:endIdx]\n",
    "            if window_type == \"Kaiser\":\n",
    "                kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "                slice_X *= kaiser_window\n",
    "\n",
    "            elif window_type == \"Hamming\":\n",
    "                hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hamming_window\n",
    "            \n",
    "            elif window_type == \"Hanning\":\n",
    "                hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hanning_window\n",
    "            \n",
    "            elif window_type == \"Rec\":\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Window type is wrong!\")\n",
    "            dataset[i, :, :] = slice_X\n",
    "\n",
    "            if(temp<WINDOW_SAMPLE_LENGTH/2):\n",
    "                # print(\"i is: \",i)\n",
    "                # print(\"label is: \",label)\n",
    "                labels[i,0] = label\n",
    "            else:\n",
    "                labels[i,0] = int(not(label))\n",
    "                # print(\"i is: \",i)\n",
    "                # print(\"label is: \",int(not(label)))\n",
    "\n",
    "            if(startIdx>=index[l+1]):\n",
    "                l+=1\n",
    "                # print(f\"label changed in i = {i}\")\n",
    "                label = int(not(label))\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        startIdx+=SLIDING_POINTS\n",
    "        endIdx+=SLIDING_POINTS\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # a = df_len - wdinow_len\n",
    "        # a/sliding_len\n",
    "        # b = a%sliding_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "    # new_df_ = cleaned_df.copy()\n",
    "    # new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    # X = new_df_.to_numpy()\n",
    "    # X = np.transpose(X)\n",
    "    # number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n",
    "    # number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/SLIDING_POINTS) +1\n",
    "\n",
    "    \n",
    "    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n",
    "    # labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # #Initialization\n",
    "    # if class_seperator_flag:\n",
    "    #     seperated_class_df = class_seperator(cleaned_df,class_1,class_2)\n",
    "    #     new_df_ = seperated_class_df.copy()\n",
    "    #     new_df_.drop(seperated_class_df.columns[-1], axis=1, inplace=True)\n",
    "    #     X = new_df_.to_numpy()\n",
    "    #     X = np.transpose(X)\n",
    "    #     empty_field_count = np.count_nonzero(X == 0)\n",
    "    #     print(\"Number of empty fields in X:\", empty_field_count)\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"befor filling\",len(zero_indices[0]))\n",
    "    #     # X[zero_indices] += 0.001\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"after filling\",len(zero_indices[0]))\n",
    "    #     number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n",
    "    #     print(number_of_epochs)\n",
    "    # else :  \n",
    "    #     new_df_ = cleaned_df.copy()\n",
    "    #     new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    #     X = new_df_.to_numpy()\n",
    "    #     X = np.transpose(X)\n",
    "    #     empty_field_count = np.count_nonzero(X == 0)\n",
    "    #     print(\"Number of empty fields in X:\", empty_field_count)\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"befor filling\",len(zero_indices[0]))\n",
    "    #     # X[zero_indices] += 0.001\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"after filling\",len(zero_indices[0]))\n",
    "\n",
    "    #     number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n",
    "\n",
    "    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n",
    "    # labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "    # if class_seperator_flag:\n",
    "    #     i = 0  \n",
    "    #     startIdx = i * WINDOW_SAMPLE_LENGTH\n",
    "    #     endIdx = (i+1) * WINDOW_SAMPLE_LENGTH \n",
    "    #     while(endIdx<=int(len(new_df_))/2):\n",
    "    #         slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "\n",
    "    #         dataset[i, :, :] = slice_X\n",
    "    #         labels[i,0] = 0\n",
    "    #         # if (seperated_class_df.iloc[startIdx, 64] == class_1):\n",
    "    #         #     labels[i,0] = 0\n",
    "    #         # elif(seperated_class_df.iloc[startIdx, 64] == class_2):\n",
    "    #         #     labels[i,0] = 1\n",
    "    #         # else:\n",
    "    #         #     labels[i,0] = 2\n",
    "    #         startIdx+=TR_SLIDING_POINTS\n",
    "    #         endIdx+=TR_SLIDING_POINTS\n",
    "    #         i+=1\n",
    "    #     # print(int(len(new_df_))/2,\"len\")    \n",
    "    #     # print(endIdx,\"endIdx\")    \n",
    "    #     # print(seperated_class_df.iloc[endIdx-2:endIdx+2,64])\n",
    "       \n",
    "    #     j = i\n",
    "        \n",
    "    #     startIdx = endIdx-TR_SLIDING_POINTS\n",
    "    #     endIdx = startIdx+WINDOW_SAMPLE_LENGTH\n",
    "    #     print(j, \"j is this\")\n",
    "    #     while(endIdx<=int(len(new_df_))):\n",
    "    #         slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "\n",
    "    #         dataset[j, :, :] = slice_X\n",
    "    #         labels[j,0] = 1\n",
    "    #         # if (cleaned_df.iloc[startIdx, 64] == class_1):\n",
    "    #         #     labels[j,0] = 0\n",
    "    #         # elif(cleaned_df.iloc[startIdx, 64] == class_2):\n",
    "    #         #     labels[j,0] = 1\n",
    "    #         # else:\n",
    "    #         #     labels[j,0] = 2\n",
    "    #         startIdx+=TR_SLIDING_POINTS\n",
    "    #         endIdx+=TR_SLIDING_POINTS\n",
    "    #         j+=1\n",
    "    #     print(j, \"j is this\")\n",
    "    #     # dataset,labels = shuffler(dataset,labels)\n",
    "\n",
    "    # else:\n",
    "    #     i = 0  \n",
    "    #     start_idx = i * WINDOW_SAMPLE_LENGTH\n",
    "    #     end_idx = (i+1) * WINDOW_SAMPLE_LENGTH \n",
    "    #     while (end_idx<=int(len(new_df_))):\n",
    "    #         slice_X = X[:, start_idx:end_idx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "            \n",
    "    #         dataset[i, :, :] = slice_X\n",
    "    #         if (cleaned_df.iloc[start_idx, 64] == class_1):\n",
    "    #             labels[i,0] = 0\n",
    "    #         elif(cleaned_df.iloc[start_idx, 64] == class_2):\n",
    "    #             labels[i,0] = 1\n",
    "    #         else:\n",
    "    #             labels[i,0] = 2\n",
    "    #         start_idx+=SLIDING_POINTS\n",
    "    #         end_idx+=SLIDING_POINTS\n",
    "    #         i+=1\n",
    "    #     # dataset,labels = shuffler(dataset,labels)\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #For training and test purpose\n",
    "    # if random_flag:\n",
    "    #     randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n",
    "    # else:\n",
    "    #     randomlist = list(range(number_of_epochs))\n",
    "    #Labeling the data\n",
    "\n",
    "\n",
    "\n",
    "    # for i in range(number_of_epochs):\n",
    "    #     start_idx = randomlist[i] * WINDOW_SAMPLE_LENGTH + SLIDING_POINTS\n",
    "    #     end_idx = (randomlist[i] + 1) * WINDOW_SAMPLE_LENGTH\n",
    "    #     slice_X = X[:, start_idx:end_idx]\n",
    "\n",
    "    #     # hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= hamming_window\n",
    "\n",
    "    #     # hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= hanning_window\n",
    "\n",
    "    #     # blackman_window = blackman(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= blackman_window\n",
    "\n",
    "    #     # kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,0.5)\n",
    "    #     # slice_X *= kaiser_window\n",
    "\n",
    "    #     # gaussian_window = gaussian(WINDOW_SAMPLE_LENGTH,0.5)\n",
    "    #     # slice_X *= gaussian_window\n",
    "\n",
    "\n",
    "    #     dataset[i, :, :] = slice_X\n",
    "    #     if (cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_1):\n",
    "    #         labels[i,0] = 0\n",
    "    #     elif(cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_2):\n",
    "    #         labels[i,0] = 1\n",
    "    #     else:\n",
    "    #         labels[i,0] = 2\n",
    "    \n",
    "    # empty_field_count = np.count_nonzero(dataset == 0)\n",
    "    # print(\"Number of empty fields in dataset:\", empty_field_count,\"dataset shape\",dataset.shape)\n",
    "    # print(labels)\n",
    "    return dataset,labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_cutter(data, class_1):\n",
    "    df = data.copy()\n",
    "    Begin_trigger = \"Begin\" + \"_\" + class_1\n",
    "    End_trigger = \"End\" + \"_\" + class_1\n",
    "    Begin_idx = df[df.iloc[:, -1] == Begin_trigger].index\n",
    "    End_idx = df[df.iloc[:, -1] == End_trigger].index\n",
    "    trial_df = df.iloc[Begin_idx[0]+1:End_idx[0],:]\n",
    "    trial_df.reset_index(drop=True, inplace=True)\n",
    "    trial_df.head()\n",
    "    return trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Begin_End_trigger_modifier(data):\n",
    "    df = data.copy()\n",
    "    Begin_indexes = df[df.iloc[:, -1] == 'Begin'].index\n",
    "    End_indexes = df[df.iloc[:, -1] == 'End'].index\n",
    "    if(len(Begin_indexes)==len(End_indexes)):\n",
    "        for i in range(len(Begin_indexes)):\n",
    "            index = Begin_indexes[i]+1\n",
    "            val = df.iloc[index,-1]\n",
    "            df.iloc[Begin_indexes[i],-1] = \"Begin\" + \"_\" + str(val)\n",
    "            df.iloc[End_indexes[i],-1]   =  \"End\" + \"_\" + str(val)\n",
    "    else:\n",
    "        raise ValueError(\"Trigger seinding Exception\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(data_,class_1,class_2,tasks_time,set_type,clean_flag,sliding_time,window_time_length,window_type,number_of_channels):\n",
    "    CLASS_1 = class_1\n",
    "    CLASS_2 = class_2\n",
    "    df = data_.copy()\n",
    "    modified_df = Begin_End_trigger_modifier(df)\n",
    "    trial_df = trial_cutter(modified_df,CLASS_1)\n",
    "    # print(trial_df.shape,\"trial_df\")\n",
    "    indexes = get_group_start_indices(trial_df)\n",
    "    # print(indexes,'tasks index starting point')\n",
    "    if clean_flag:\n",
    "        cleaned_df = data_cleaner(trial_df,CLASS_1,CLASS_2,tasks_time)\n",
    "        final_df = cleaned_df.copy()\n",
    "    else:\n",
    "        final_df = trial_df.copy()\n",
    "    # print(final_df.shape,\"final_df\")\n",
    "\n",
    "    if set_type ==\"TRAIN\":\n",
    "        random_flag = True\n",
    "    elif set_type ==\"TEST\":\n",
    "        random_flag = False\n",
    "    else:\n",
    "        raise(\"Error in set type\")\n",
    "\n",
    "  \n",
    "    final_data, final_labels = data_label_attacher(final_df,CLASS_1,CLASS_2,random_flag,clean_flag,sliding_time,window_time_length,window_type,number_of_channels)\n",
    "      \n",
    "    # print(final_data.shape,\"final_data shape\")\n",
    "    # print(final_labels.shape,\"final_labels shape\")\n",
    "    \n",
    "    return final_data,final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trials_set_builder(data_dict,blocks_set,set_label,class_1,class_2,clean_flag,sliding_time,window_time_length,window_type,channels_to_remove,number_of_channels):\n",
    "                                   \n",
    "    counter = 0\n",
    "\n",
    "    for b_num in blocks_set:\n",
    "        trial_num = trial_order[b_num].index(class_1)\n",
    "        task_times,rest_times = get_task_rest_times(b_num)\n",
    "        # print(task_times[trial_num],rest_times[trial_num])\n",
    "        trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n",
    "        # print(trial_times)\n",
    "        # print(data_dict[b_num].shape,\"shape befor removal\")\n",
    "\n",
    "        # data = remove_outliers_across_channels(data_dict[b_num],10)\n",
    "        # data = remove_outliers(data_dict[b_num])\n",
    "        data = data_dict[b_num]\n",
    "        # data = apply_median_filter(data,9)\n",
    "        if class_1== 'Tongue' or class_1 == 'Mis':\n",
    "            data = channel_remover(data,channels_to_remove)\n",
    "            number_of_channels =  NUMBER_OF_CHANNELS-len(channels_to_remove)\n",
    "        else:\n",
    "            number_of_channels = NUMBER_OF_CHANNELS\n",
    "        # print(number_of_channels,\"number_of_channels\")\n",
    "        # print(data.shape,\"shape after removal\")\n",
    "        # sys.exit()\n",
    "        df = data.copy()\n",
    "        # last_column = df.pop(df.columns[-1])\n",
    "        # df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "        # eeg_data = df.to_numpy().T  # Transpose to have channels in columns\n",
    "\n",
    "        # channel_names = [f'Ch{i+1}' for i in range(63)]\n",
    "\n",
    "        # # Create MNE-Python RawArray object\n",
    "        # info = mne.create_info(ch_names=channel_names, sfreq=sampling_freq, ch_types='eeg')\n",
    "        # raw = mne.io.RawArray(eeg_data, info)\n",
    "\n",
    "        # # Apply ICA\n",
    "        # ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "        # ica.fit(raw)\n",
    "        # ica_components = ica.get_components()\n",
    "\n",
    "        # # Convert the ICA components to a DataFrame\n",
    "        # df2 = pd.DataFrame(data=ica_components.T, columns=channel_names)\n",
    "        # df2 = df2.assign(LastColumn=last_column)\n",
    "        # # df = data.copy(deep=False)\n",
    "        dataset,labels = preprocessor(df,class_1,class_2,trial_times,set_label,clean_flag,sliding_time,window_time_length,window_type,number_of_channels)\n",
    "        # print(dataset.shape)\n",
    "\n",
    "        if counter == 0 :\n",
    "            final_data = dataset\n",
    "            final_labels = labels\n",
    "            # print(\"Before concatenation - final_data shape:\", final_data.shape, \"dataset shape:\", dataset.shape)\n",
    "        else:\n",
    "            final_data = np.vstack((final_data, dataset))\n",
    "            final_labels = np.vstack((final_labels, labels))\n",
    "            print(\"After concatenation - final_data shape:\", final_data.shape, \"final_labels shape:\", final_labels.shape)\n",
    "\n",
    "        counter+=1 \n",
    "    # empty_field_count = np.count_nonzero(final_data == 0)\n",
    "    # print(\"Number of empty fields in final_data:\", empty_field_count,\"final_data shape\",final_data.shape)\n",
    "    return final_data,final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(data_list):\n",
    "    counted_values = Counter(data_list)\n",
    "    duplicate_values = {value: count for value, count in counted_values.items() if count > 1}\n",
    "    return duplicate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Statistical_analysor(p_num_list,data_dicts_list):\n",
    "\n",
    "    with open('/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Classification/Statistics.txt', 'w') as file:\n",
    "        for p in range(len(p_num_list)):\n",
    "            file.write(f'Particpant: {p+3} '+'\\n')\n",
    "            for b in range(7):\n",
    "                file.write(f'Block: {b+1} '+'\\n')\n",
    "                data_pd = data_dicts_list[p][b]\n",
    "                data = data_pd.iloc[:, :-1]\n",
    "                data_np = data.values\n",
    "                eeg_data = data_np\n",
    "                print(\"Data type:\", type(eeg_data))\n",
    "                print(\"Shape:\", eeg_data.shape)\n",
    "                eeg_data = np.array(eeg_data)\n",
    "                mean_values = np.mean(eeg_data, axis=0)\n",
    "                variance_values = np.var(eeg_data, axis=0)\n",
    "                std_deviation_values = []\n",
    "                \n",
    "                for i in range(num_channels):\n",
    "                    print(f\"Channel {i + 1}:\")\n",
    "                    print(f\"Mean: {mean_values[i]}\")\n",
    "                    print(f\"Variance: {variance_values[i]}\")\n",
    "                    std_deviation_values.append(np.sqrt(variance_values[i]))\n",
    "                    print(f\"Standard Deviation: {std_deviation_values[i]}\")\n",
    "                    print()\n",
    "                    file.write(f'Channel {i+1}: '+'\\n')\n",
    "                    file.write(f\"Mean: {mean_values[i]}\"+\"\\n\")\n",
    "                    file.write(f\"Variance: {variance_values[i]}\"+\"\\n\")\n",
    "                    file.write(f\"Standard Deviation: {std_deviation_values[i]}\"+\"\\n\\n\")\n",
    "                \n",
    "                lists_to_check = {\n",
    "                'mean_values': mean_values,\n",
    "                'variance_values': variance_values,\n",
    "                'std_deviation_values': std_deviation_values\n",
    "                }\n",
    "                for list_name, data_list in lists_to_check.items():\n",
    "                    duplicate_values = find_duplicates(data_list)\n",
    "                    if duplicate_values:\n",
    "                        print(f\"Duplicate values and their counts for {list_name}:\")\n",
    "                        file.write(f\"Duplicate values and their counts for {list_name}:\"+\"\\n\")\n",
    "                        for value, count in duplicate_values.items():\n",
    "                            print(f\"Value: {value}, Count: {count}\")\n",
    "                            file.write(f\"Value: {value}, Count: {count}\"+\"\\n\")\n",
    "                    else:\n",
    "                        print(f\"No duplicate values found in the {list_name} list.\")\n",
    "                        file.write(f\"No duplicate values found in the {list_name} list.\"+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(y_true, y_pred):\n",
    "    mismatches = []\n",
    "    total = len(y_true)\n",
    "    mismatch_count = 0\n",
    "    \n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            mismatches.append(i)\n",
    "            mismatch_count += 1\n",
    "            \n",
    "    accuracy = 1 - (mismatch_count / total)\n",
    "    \n",
    "    return accuracy, mismatch_count, mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_next(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list) - window_size + 1):\n",
    "        window = prediction_list[i:i+window_size]\n",
    "        window_tuple = tuple(window)\n",
    "        counts = Counter(window_tuple)\n",
    "        majority = counts.most_common(1)[0][0]\n",
    "        majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_prev(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        if i >= window_size - 1:\n",
    "            start_index = i - window_size + 1\n",
    "            window = prediction_list[start_index:i+1]\n",
    "            counts = Counter(window)\n",
    "            majority = counts.most_common(1)[0][0]\n",
    "            majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_prev_v2(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        start_index = max(0, i - window_size + 1)\n",
    "        window = prediction_list[start_index:i+1]\n",
    "        counts = Counter(window)\n",
    "        majority = counts.most_common(1)[0][0]\n",
    "        majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_remover(df, channels):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop(df.columns[channels], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading P3\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Participants/P3/P3B0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Participants/P3/P3B0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p_num \u001b[38;5;129;01min\u001b[39;00m p_num_list:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreading P\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdata_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Participants/P\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     data_dicts_list\u001b[38;5;241m.\u001b[39mappend(data_dict)\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mdata_reader\u001b[0;34m(path, p_num, block_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_num \u001b[38;5;129;01min\u001b[39;00m block_list:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(b_num)\n\u001b[0;32m----> 5\u001b[0m     mat \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp_num\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb_num\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchars_as_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_as_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_compressed_data_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     data_dict[b_num] \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Participants/P3/P3B0.mat'"
     ]
    }
   ],
   "source": [
    "block_list = [0,1,2,3,4,5,6]\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "data_dicts_list = []\n",
    "for p_num in p_num_list:\n",
    "    print(f'reading P{p_num}')\n",
    "    data_dict = data_reader(f'../../Participants/P{p_num}/', p_num, block_list)\n",
    "    data_dicts_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_list = [0,1,2,3,4,5,6]\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "data_dicts_list = []\n",
    "for p_num in p_num_list:\n",
    "    print(f'reading P{p_num}')\n",
    "    data_dict = data_reader(f'/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/P{p_num}/',p_num,block_list)\n",
    "    data_dicts_list.append(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dicts_list[0][6].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(train_data, test_data=None, n_components=50):\n",
    "    # Reshape train data\n",
    "    train_data_reshaped = np.reshape(train_data, (train_data.shape[0], -1))\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train_data = scaler.fit_transform(train_data_reshaped)\n",
    "    # scaled_train_data = train_data_reshaped\n",
    "    \n",
    "    # Apply PCA on train data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(scaled_train_data)\n",
    "    transformed_train_data = pca.transform(scaled_train_data)\n",
    "\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Explained Variance Ratio:\")\n",
    "    print(explained_variance_ratio)\n",
    "    print(\"\\nCumulative Explained Variance:\")\n",
    "    print(cumulative_explained_variance)\n",
    "\n",
    "\n",
    "    test_data_reshaped = np.reshape(test_data, (test_data.shape[0], -1))\n",
    "    scaled_test_data = scaler.transform(test_data_reshaped)\n",
    "    transformed_test_data = pca.transform(scaled_test_data)\n",
    "\n",
    "    return transformed_train_data, transformed_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runs_iterattor(runs,clf,train_features,Y_tr,test_features,Y_te,test_acc_list,train_acc_list,vote_acc_list):\n",
    "\n",
    "#     for r in range(runs):\n",
    "#         clf.fit(train_features, Y_tr)\n",
    "        \n",
    "#         y_pr_te = clf.predict(test_features)\n",
    "#         y_pr_tr = clf.predict(train_features)\n",
    "\n",
    "#         accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#         test_acc_list.append(accuracy_te)\n",
    "\n",
    "#         accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#         train_acc_list.append(accuracy_tr)\n",
    "\n",
    "#         y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "#         Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "#         vote_acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "#         vote_acc_list.append(vote_acc)\n",
    "#     return train_acc_list,test_acc_list,vote_acc_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def class_iterator(class_1_list,data_dicts_list,train_blk_set,test_blocks_set,class_2,clean_flag,sliding_time_tr,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels,p,p_num,train_blk_name,test_blk_name,path):\n",
    "\n",
    "#     for class_1 in class_1_list:\n",
    "#         import time\n",
    "#         start_time = time.time()\n",
    "#         X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blk_set,'TRAIN',class_1,class_2,clean_flag,sliding_time_tr,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "#         X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blocks_set,'TEST',class_1,class_2,clean_flag,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "\n",
    "#         print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "#         print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "#         [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te,number_of_components)\n",
    "#         print(train_features.shape, \"train_features shape\")\n",
    "#         print(test_features.shape, \"test_features shape\")\n",
    "#         selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "\n",
    "#         clf = classifier\n",
    "#         runs = 1\n",
    "#         train_acc_list = []\n",
    "#         test_acc_list = []\n",
    "#         vote_acc_list = []\n",
    "\n",
    "#         train_acc_list,test_acc_list,vote_acc_list = runs_iterattor(runs,clf,train_features[:,selected_features],Y_tr[:,0],test_features[:, selected_features],Y_te,test_acc_list,train_acc_list,vote_acc_list)\n",
    "        \n",
    "\n",
    "#         end_time = time.time()\n",
    "#         running_time = end_time-start_time\n",
    "#         participant = p_num\n",
    "#         class1 = class_1\n",
    "#         class2 = class_2\n",
    "#         running_time = running_time\n",
    "#         test_acc = np.average(test_acc_list)\n",
    "#         train_acc = np.average(train_acc_list)\n",
    "#         vote_acc = np.average(vote_acc_list)\n",
    "#         test_size = X_te.shape\n",
    "#         train_size = X_tr.shape\n",
    "#         train_block = train_blk_name\n",
    "#         test_block = test_blk_name\n",
    "#         new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,vote_acc]\n",
    "\n",
    "#         save_csv(new_row,path)\n",
    "\n",
    "\n",
    "#         print(train_acc_list,\"train\",class_1)\n",
    "#         print(test_acc_list,\"test\",class_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(new_row, path):\n",
    "\n",
    "    absolute_path = os.path.abspath(path)\n",
    "    try:\n",
    "        rf = pd.read_csv(absolute_path)\n",
    "    except FileNotFoundError:\n",
    "        rf = pd.DataFrame(columns=column_names_v2)\n",
    "\n",
    "    new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "    cf.to_csv(absolute_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pca,te_pca = apply_pca(X_tr,X_te,50)\n",
    "print(tr_pca.shape,te_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n",
    "# class_1_list = ['Hand','Feet','Tongue','Mis']\n",
    "# class_2 = 'Rest'\n",
    "# p_num_list = [9]\n",
    "# train_blocks_set = [0,1,2,3,4]\n",
    "# test_blocks_set = [5,6]\n",
    "# window_time_length = 4\n",
    "# sliding_time_tr = 4\n",
    "# sliding_time_te = 4\n",
    "# vote_window = 4\n",
    "# number_of_selected_features = 10\n",
    "# # channels_to_remove = [list(range(1,56))]\n",
    "# channels_to_remove = []\n",
    "# number_of_channels = NUMBER_OF_CHANNELS\n",
    "# params = {\n",
    "#     'max_depth': 5,\n",
    "#     'min_child_weight': 1,\n",
    "#     'gamma': 0,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'learning_rate': 0.1,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# classifier_dic = {\"XGB\":XGBClassifier(),\"LDA\":LDA(),\"RF\":RF()}\n",
    "# number_of_components_list = [10,12,14,8]\n",
    "# number_of_selected_features_list = [10,15,20]\n",
    "# overlapflag = False\n",
    "# window_time_length_list = [4,2,1]\n",
    "# window_type_list = [\"Rec\",\"Kaiser\",\"Hanning\",\"Hamming\"]\n",
    "# train_blk_set_dic = {\"01234\":[0,1,2,3,4],\"0123\":[0,1,2,3],,\"012\":[0,1,2],,\"01\":[0,1],\"0\":[0]}\n",
    "\n",
    "classifier_dic = {\"XGB\":XGBClassifier()}\n",
    "number_of_components_list = [10]\n",
    "number_of_selected_features_list = [10]\n",
    "channels_to_remove = []\n",
    "number_of_channels = NUMBER_OF_CHANNELS\n",
    "overlapflag = \"Without_Overlap\"\n",
    "window_time_length_list = [2,1]\n",
    "sliding_time_tr = 4\n",
    "sliding_time_te = 4\n",
    "vote_window = 4\n",
    "window_type_list = [\"Rec\",\"Kaiser\",\"Hamming\",\"Hanning\"]\n",
    "train_blk_set_dic = {\"12345\":[0,1,2,3,4],\"1234\":[0,1,2,3],\"123\":[0,1,2],\"12\":[0,1],\"1\":[0]}\n",
    "test_blk_set_dic = {\"67\":[5,6],\"567\":[4,5,6],\"4567\":[3,4,5,6],\"34567\":[2,3,4,5,6],\"234567\":[1,2,3,4,5,6]}\n",
    "# train_blk_set_dic = {\"12345\":[0,1,2,3,4]}\n",
    "# test_blk_set_dic = {\"67\":[5,6]}\n",
    "class_1_list = ['Hand','Feet','Tongue','Mis']\n",
    "class_2 = 'Rest'\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "PATH = \"/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results-v2/\"\n",
    "\n",
    "\n",
    "for classifier, classifier_name in zip(classifier_dic.values(),classifier_dic.keys()):\n",
    "    for number_of_components in number_of_components_list:\n",
    "        for number_of_selected_features in number_of_selected_features_list:\n",
    "                for window_time_length in window_time_length_list:\n",
    "                    if overlapflag == \"With_Overlap\":\n",
    "                        window_type_list = [\"Rec\"]  \n",
    "                        sliding_time_tr = sliding_time_te = window_time_length/2\n",
    "                    else:\n",
    "                        window_type_list = [\"Rec\",\"Kaiser\",\"Hamming\",\"Hanning\"]\n",
    "                        sliding_time_tr = sliding_time_te = window_time_length\n",
    "\n",
    "                    for window_type in window_type_list:\n",
    "                        for train_blk_set,train_blk_name,test_blk_set,test_blk_name in zip(train_blk_set_dic.values(),train_blk_set_dic.keys(),test_blk_set_dic.values(),test_blk_set_dic.keys()):\n",
    "                            p = 0\n",
    "                            for p_num in p_num_list:\n",
    "                                for class_1 in class_1_list:\n",
    "                                    import time\n",
    "                                    start_time = time.time()\n",
    "                                    X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blk_set,'TRAIN',class_1,class_2,True,sliding_time_tr,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "                                    X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blk_set,'TEST',class_1,class_2,True,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "\n",
    "                                    print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "                                    print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "                                    [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te,number_of_components)\n",
    "                                    # [train_features, test_features] =  apply_pca(X_tr,X_te,10)\n",
    "                                    print(train_features.shape, \"train_features shape\")\n",
    "                                    print(test_features.shape, \"test_features shape\")\n",
    "                                    selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "                         \n",
    "                                    clf = classifier\n",
    "                                    runs = 1\n",
    "                                    train_acc_list = []\n",
    "                                    test_acc_list = []\n",
    "                                    vote_acc_list = []\n",
    "\n",
    "                                    for r in range(runs):\n",
    "                                        clf.fit(train_features[:, selected_features], Y_tr[:,0])\n",
    "                                        \n",
    "                                        y_pr_te = clf.predict(test_features[:, selected_features])\n",
    "                                        y_pr_tr = clf.predict(train_features[:,selected_features])\n",
    "\n",
    "                                        accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "                                        test_acc_list.append(accuracy_te)\n",
    "\n",
    "\n",
    "                                        accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "                                        train_acc_list.append(accuracy_tr)\n",
    "\n",
    "                       \n",
    "                                        y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "                                        Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "                                        vote_acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "                                        vote_acc_list.append(vote_acc)\n",
    "                                        \n",
    " \n",
    "                                    end_time = time.time()\n",
    "                                    running_time = end_time-start_time\n",
    "                                    participant = p_num\n",
    "                                    class1 = class_1\n",
    "                                    class2 = class_2\n",
    "                                    running_time = running_time\n",
    "                                    test_acc = np.average(test_acc_list)\n",
    "                                    train_acc = np.average(train_acc_list)\n",
    "                                    vote_acc = np.average(vote_acc_list)\n",
    "                                    test_size = X_te.shape\n",
    "                                    train_size = X_tr.shape\n",
    "                                    train_block = train_blk_name\n",
    "                                    test_block = test_blk_name\n",
    "                                    new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,vote_acc]\n",
    "\n",
    "                                    if overlapflag == \"With_Overlap\":\n",
    "                                        path = os.path.join(\n",
    "                                            PATH,\n",
    "                                            classifier_name,\n",
    "                                            f\"{number_of_components}_CSP_Components\",\n",
    "                                            f\"{number_of_selected_features}-Selected_Features\",\n",
    "                                            overlapflag,\n",
    "                                            f\"{window_time_length}_window_time_length\",\n",
    "                                            \"50% Overlap\",\n",
    "                                            f\"{train_blk_name}_Train/\"\n",
    "                                        )\n",
    "                                    else:\n",
    "                                        path = os.path.join(\n",
    "                                            PATH,\n",
    "                                            classifier_name,\n",
    "                                            f\"{number_of_components}_CSP_Components\",\n",
    "                                            f\"{number_of_selected_features}-Selected_Features\",\n",
    "                                            overlapflag,\n",
    "                                            f\"{window_time_length}_window_time_length\",\n",
    "                                            f\"{window_type}_Window\",\n",
    "                                            f\"{train_blk_name}_Train/\"\n",
    "                                        )\n",
    "\n",
    "                                    save_csv(new_row, path + f\"P{p_num}.csv\")\n",
    "                                    print(\n",
    "                                    f\"classifier_name = {classifier_name}\\n\"\n",
    "                                    f\"number_of_components = {number_of_components}\\n\"\n",
    "                                    f\"number_of_selected_features = {number_of_selected_features}\\n\"\n",
    "                                    f\"overlapflag = {overlapflag}\\n\"\n",
    "                                    f\"window_time_length = {window_time_length}\\n\"\n",
    "                                    f\"window_type = {window_type}\\n\"\n",
    "                                    f\"train_blk_name = {train_blk_name}\\n\"\n",
    "                                    f\"Participant = {p_num}\"\n",
    "                                    )\n",
    "\n",
    "                                    print(train_acc_list,\"train\",class_1)\n",
    "                                    print(test_acc_list,\"test\",class_1)\n",
    "                                    \n",
    "                                p+=1\n",
    "                            get_results_average(path,p_num_list,class_1_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "                \n",
    "# p = 0\n",
    "# for p_num in p_num_list:\n",
    "#     for class_1 in class_1_list:\n",
    "#         import time\n",
    "#         start_time = time.time()\n",
    "#         X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blocks_set,'TRAIN',class_1,class_2,True,sliding_time_tr,channels_to_remove,number_of_channels)\n",
    "#         X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blocks_set,'TEST',class_1,class_2,True,sliding_time_te,channels_to_remove,number_of_channels)\n",
    "\n",
    "#         print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "#         print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "#         [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te)\n",
    "#         # [train_features, test_features] =  apply_pca(X_tr,X_te,10)\n",
    "#         print(train_features.shape, \"train_features shape\")\n",
    "#         print(test_features.shape, \"test_features shape\")\n",
    "#         selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "#         train_acc_list = []\n",
    "#         test_acc_list = []\n",
    "\n",
    "#         clf = XGBClassifier()\n",
    "#         for r in range(1):\n",
    "#             clf.fit(train_features[:, selected_features], Y_tr[:,0])\n",
    "            \n",
    "#             y_pr_te = clf.predict(test_features[:, selected_features])\n",
    "#             y_pr_tr = clf.predict(train_features[:,selected_features])\n",
    "\n",
    "\n",
    "#             accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#             test_acc_list.append(accuracy_te)\n",
    "\n",
    "#             accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#             train_acc_list.append(accuracy_tr)\n",
    "\n",
    "#             # clf.fit(train_features[:, :], Y_tr[:,0])\n",
    "\n",
    "#             # y_pr_te = clf.predict(test_features[:, :])\n",
    "#             # y_pr_tr = clf.predict(train_features[:,:])\n",
    "\n",
    "#             # accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#             # test_acc_list.append(accuracy_te)\n",
    "\n",
    "#             # accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#             # train_acc_list.append(accuracy_tr)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # for i in range(len(Y_te)):\n",
    "#             #     print(f\"Test_Real: {Y_te[i][0]}   Test_Predication: {y_pr_te[i]}\")\n",
    "            \n",
    "#             # count_0s = np.count_nonzero(Y_tr == 0)\n",
    "#             # count_1s = np.count_nonzero(Y_tr == 1)\n",
    "\n",
    "#             # print(f\"Number of 0s: {count_0s}\")\n",
    "#             # print(f\"Number of 1s: {count_1s}\")\n",
    "\n",
    "#             # print(\"selected_features: \",selected_features)\n",
    "#             # print(Y_te.shape,y_pr_te.shape,\"shape \")\n",
    "#             y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "#             Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "\n",
    "#             # for i in range(len(Y_te)):\n",
    "#             #     print(f\"Test_Real_Vote: {Y_te_Vote[i]}   Test_Predication_vote: {y_pr_te_Vote[i]}\")\n",
    "\n",
    "\n",
    "#             acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "#             # print(acc,num_of_mismatches,mismatches_list, \"acc, num_of_mismatches ,mismatches_list\",class_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         running_time = end_time-start_time\n",
    "#         participant = p_num\n",
    "#         class1 = class_1\n",
    "#         class2 = class_2\n",
    "#         running_time = running_time\n",
    "#         test_acc = np.average(test_acc_list)\n",
    "#         train_acc = np.average(train_acc_list)\n",
    "#         test_size = X_te.shape\n",
    "#         train_size = X_tr.shape\n",
    "#         train_block = '01234'\n",
    "#         test_block = '56'\n",
    "#         test_acc_vote = acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,test_acc_vote]\n",
    "\n",
    "#         new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "#         rf = pd.read_csv(PATH +'P'+str(p_num)+'.csv')\n",
    "#         cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "#         cf.to_csv(PATH +'P'+str(p_num)+'.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "#         print(train_acc_list,\"train\",class_1)\n",
    "#         print(test_acc_list,\"test\",class_1)\n",
    "        \n",
    "#     p+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Frame Maker\n",
    "PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results-v2/XGB/10_CSP_Components/10-Selected_Features/With_Overlap/Rec_Window/50%_Overlap/12345_Train/'\n",
    "df = pd.read_csv(PATH+'frame.csv')\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "for p_num in p_num_list:\n",
    "    df.to_csv(PATH+'P'+str(p_num)+'.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\n",
    "    PATH,\n",
    "    classifier_name,\n",
    "    f\"{number_of_components}_CSP_Components\",\n",
    "    f\"{number_of_selected_features}-Selected_Features\",\n",
    "    overlapflag,\n",
    "    f\"{window_time_length}_window_time_length\",\n",
    "    f\"{window_type}_Window\",\n",
    "    f\"{train_blk_name}_Train/\",\n",
    ")\n",
    "print(path + f\"P{p_num}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_csv(new_row, classifier, number_of_components, number_of_selected_features, overlapflag,\n",
    "             window_time_length, window_type, train_blk_name, p_num):\n",
    "    # Convert to relative path\n",
    "    relative_path = os.path.join(\n",
    "        classifier,\n",
    "        f\"{number_of_components}_CSP_Components\",\n",
    "        f\"{number_of_selected_features}-Selected_Features\",\n",
    "        overlapflag,\n",
    "        f\"{window_time_length}_window_time_length\",\n",
    "        window_type,\n",
    "        f\"{train_blk_name}_Train\",\n",
    "        f\"P{p_num}.csv\"\n",
    "    )\n",
    "\n",
    "    # Convert relative path to absolute path\n",
    "    absolute_path = os.path.abspath(relative_path)\n",
    "\n",
    "    # Read existing CSV or create an empty DataFrame\n",
    "    try:\n",
    "        rf = pd.read_csv(absolute_path)\n",
    "    except FileNotFoundError:\n",
    "        rf = pd.DataFrame(columns=column_names_v2)\n",
    "\n",
    "    # Create a new row DataFrame\n",
    "    new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "\n",
    "    # Concatenate existing DataFrame and new row DataFrame\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "\n",
    "    # Write the concatenated DataFrame to CSV\n",
    "    cf.to_csv(absolute_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "classifier = \"XGBClassifier\"\n",
    "number_of_components = 10\n",
    "number_of_selected_features = 10\n",
    "overlapflag = \"With_Overlap\"\n",
    "window_time_length = \"4_window_time_length\"\n",
    "window_type = \"50% Overlap\"\n",
    "train_blk_name = \"12345\"\n",
    "p_num = 9\n",
    "\n",
    "new_row = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10]\n",
    "save_csv(new_row, classifier, number_of_components, number_of_selected_features, overlapflag,\n",
    "         window_time_length, window_type, train_blk_name, p_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Specific path and file name\n",
    "# file_path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/frame.csv'  # Replace with your specific path\n",
    "# df = pd.DataFrame(columns=column_names)\n",
    "# # Write the DataFrame to a CSV file at the specified path\n",
    "# df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list = [\"12345\", \"1234\", \"123\", \"12\", \"1\"]\n",
    "\n",
    "second_list = []\n",
    "\n",
    "for item in first_list:\n",
    "    result = \"\"\n",
    "    for i in range(len(item) - 1):\n",
    "        result += str(int(item[i]) + int(item[i + 1]))\n",
    "    second_list.append(result)\n",
    "\n",
    "print(second_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blk_set_dic = {\"01234\":[0,1,2,3,4],\"0123\":[0,1,2,3],\"012\":[0,1,2],\"01\":[0,1],\"0\":[0]}\n",
    "for train_blk_set,train_blk_name in zip(train_blk_set_dic.values(),train_blk_set_dic.keys()):\n",
    "    print(len(train_blk_set))\n",
    "    print(train_blk_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # Specify the threshold for outliers (you can adjust this based on your data)\n",
    "    threshold = 10 ** 5\n",
    "\n",
    "    # Calculate median and MAD for each row\n",
    "    median = df.iloc[:, :-1].median(axis=1)\n",
    "    mad = np.median(np.abs(df.iloc[:, :-1].sub(median, axis=0)), axis=1)\n",
    "    threshold_array = median + threshold * mad\n",
    "\n",
    "    # Identify rows with values exceeding the threshold\n",
    "    outliers = df.iloc[:, :-1].gt(threshold_array[:, None], axis=0).any(axis=1)\n",
    "\n",
    "    # Remove rows identified as outliers\n",
    "    clean_df = df[~outliers]\n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/'\n",
    "def data_reader(path,p_num,block_list):\n",
    "    data_dict = {}\n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        mat = loadmat(path+'P'+ str(p_num) + '/'+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "        df = pd.DataFrame(mat['Data'])\n",
    "        df.to_pickle(f\"{path}Pickels_Participants/P{p_num}/P{p_num}B{b_num}.pkl\")\n",
    "    #     data_dict[b_num] = df\n",
    "    # return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "reading 14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "block_list = [0,1,2,3,4,5,6]\n",
    "p_num_list = [4,5,6,7,10,11,13,14]\n",
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/'\n",
    "for p_num in p_num_list:\n",
    "    print(\"reading\",p_num)\n",
    "    data_reader(path,p_num,block_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/'\n",
    "def pickle_reader(path, p_num, block_list):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        file_path = os.path.join(path, f\"P{p_num}/P{p_num}B{b_num}.pkl\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "\n",
    "                data_dict[b_num] = data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {file_path}: {e}\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 9\n",
      "0\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B0.pkl\n",
      "1\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B1.pkl\n",
      "2\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B2.pkl\n",
      "3\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B3.pkl\n",
      "4\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B4.pkl\n",
      "5\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B5.pkl\n",
      "6\n",
      "File not found: /home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/P9/P9B6.pkl\n",
      "Reading 10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/'\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "block_list = [0,1,2,3,4,5,6]\n",
    "data_dict_list = []\n",
    "for p_num in p_num_list:\n",
    "    print(\"Reading\",p_num)\n",
    "    data_dict = pickle_reader(path,p_num,block_list)\n",
    "    data_dict_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3878301127.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(len()data_dict_list)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(len()data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers_across_channels(df, threshold):\n",
    "    # print(df.shape,\"shape0\")\n",
    "\n",
    "    data_columns = df.columns[:-1]  # Excluding the label column by default\n",
    "\n",
    "    # Separate the label column\n",
    "    labels = df.iloc[:, -1]  # Assuming the label is in the last column\n",
    "    data_without_label = df.iloc[:, :-1]  # DataFrame without the label column\n",
    "\n",
    "    # Calculate Z-scores for each row\n",
    "    # print(data_without_label.shape,\"shape1\")\n",
    "    # print(data_without_label[data_columns].shape,\"shape2\")\n",
    "    # print(data_without_label.head(10))\n",
    "    data_without_label[data_columns] = data_without_label[data_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    z_scores = stats.zscore(data_without_label[data_columns], axis=1)\n",
    "    abs_z_scores = abs(z_scores)\n",
    "\n",
    "    filtered_entries = (abs_z_scores < threshold).all(axis=1)\n",
    "    filtered_data = data_without_label[filtered_entries]\n",
    "\n",
    "    # Remove corresponding labels for removed rows\n",
    "    filtered_labels = labels[filtered_entries]\n",
    "\n",
    "    # If you want to reset index after filtering\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    filtered_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Combine the filtered data and labels\n",
    "    filtered_df = pd.concat([filtered_data, filtered_labels], axis=1)\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = [1, 1, 1, 1, 0, 0, 1, 1, 0, 0]  # Replace with your actual prediction list\n",
    "\n",
    "result = majority_vote_sliding_with_next(prediction_list)\n",
    "result2 = majority_vote_sliding_with_prev_v2(prediction_list)\n",
    "print(\"Majority Votes:\", result)\n",
    "print(\"Majority Votes Previous:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/Win4Shift2/'\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "vf = pd.DataFrame(columns=column_names_v2) \n",
    "for p_num in p_num_list:\n",
    "    print(p_num)\n",
    "    rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n",
    "    vf = pd.concat([vf, rf], ignore_index=True)\n",
    "vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n",
    "# vf.tail()\n",
    "    \n",
    "columnNames = ['class','test_acc','vote_acc']\n",
    "kf = pd.DataFrame(columns=columnNames)\n",
    "kf.to_csv(PATH+'AverageAcc.csv',index=False)\n",
    "\n",
    "vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n",
    "df = vf\n",
    "\n",
    "class_list=['Hand','Feet','Tongue','Mis']\n",
    "blk_list = [1234]\n",
    "for class_ in class_list:\n",
    "    avg_list = []\n",
    "    avg_vote_list = []\n",
    "    for blk in blk_list:\n",
    "        gf = df[(df['train_block'] == blk) & (df['class1'] == class_)]\n",
    "        avg = gf['test_acc'].mean()\n",
    "        avg_vote = gf['test_acc_vote'].mean()\n",
    "        avg_list.append(avg)\n",
    "        avg_vote_list.append(avg_vote)\n",
    "    print(avg_list)    \n",
    "    new_row = [class_, avg_list[0],avg_vote_list[0]] \n",
    "    new_row_df = pd.DataFrame([new_row], columns=columnNames)\n",
    "    rf = pd.read_csv(PATH + 'AverageAccExcP8.csv')\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "    cf.to_csv(PATH +'AverageAccExcP8.csv',index=False)  \n",
    "kf = pd.read_csv(PATH +'AverageAccExcP8.csv') \n",
    "kf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_average(path,p_num_list,class_list):\n",
    "    PATH = path\n",
    "    vf = pd.DataFrame(columns=column_names_v2) \n",
    "    for p_num in p_num_list:\n",
    "        print(p_num)\n",
    "        rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n",
    "        vf = pd.concat([vf, rf], ignore_index=True)\n",
    "    vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n",
    "\n",
    "    columnNames = ['class','test_acc','vote_acc']\n",
    "    kf = pd.DataFrame(columns=columnNames)\n",
    "    kf.to_csv(PATH+'AverageAcc.csv',index=False)\n",
    "    vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n",
    "    df = vf\n",
    "    blk_list = [1234]\n",
    "    for class_ in class_list:\n",
    "        gf = df[(df['class1'] == class_)]\n",
    "        avg_test = gf['test_acc'].mean()\n",
    "        avg_vote = gf['test_acc_vote'].mean() \n",
    "        new_row = [class_, avg_test,avg_vote] \n",
    "        new_row_df = pd.DataFrame([new_row], columns=columnNames)\n",
    "        rf = pd.read_csv(PATH + 'AverageAcc.csv')\n",
    "        cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "        cf.to_csv(PATH +'AverageAcc.csv',index=False)  \n",
    "    kf = pd.read_csv(PATH +'AverageAcc.csv') \n",
    "    print(kf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with the last column named 'label'\n",
    "data = {'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter'],\n",
    "        'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "print(\"his\")\n",
    "# Define a custom sorting order based on the desired grouping\n",
    "sorting_order = {'a': 0, 'b': 1}\n",
    "\n",
    "# Create a new column with the sorting order\n",
    "df['sorting_order'] = df.iloc[:, 2].map(sorting_order)\n",
    "\n",
    "# Sort the DataFrame based on the new column and the original order within each group\n",
    "df.sort_values(by=['sorting_order', df.columns[2]], inplace=True)\n",
    "\n",
    "# Drop the temporary sorting column\n",
    "df.drop('sorting_order', axis=1, inplace=True)\n",
    "\n",
    "# Optional: Reset the index if needed\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','b'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "x=0\n",
    "i=0\n",
    "class_1 = 'a'\n",
    "class_2 = 'b'\n",
    "sampleList = []\n",
    "while i<len(df):\n",
    "    if (df.iloc[i,2]==class_1):\n",
    "        x+=1\n",
    "    else:\n",
    "        i-=1\n",
    "        sampleList.append(x)\n",
    "        x=0\n",
    "        class_1,class_2 = class_2,class_1\n",
    "    i+=1\n",
    "sampleList.append(x)\n",
    "print(sampleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'it', 'does', 'not matter'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'b', 'b', 'b']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(get_group_start_indices(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','c'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify consecutive groups of 'a's by creating a new group ID each time 'label' changes from 'b' to 'a'\n",
    "df['group'] = (df['label'] != df['label'].shift(1)).cumsum()\n",
    "\n",
    "# Count occurrences of 'a' within each group\n",
    "group_counts = df[df['label'] == 'a'].groupby('group').size()\n",
    "\n",
    "group_counts_b = df[df['label'] == 'b'].groupby('group').size()\n",
    "print(group_counts_b)\n",
    "print(group_counts_b.index[0])\n",
    "print(group_counts_b.iloc[0])\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_num = 8\n",
    "b_num = 7\n",
    "path = f'../../Participants/P{p_num}v5/'\n",
    "mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "df = pd.DataFrame(mat['Data'])\n",
    "\n",
    "path = f'../../Participants/P{p_num}v0/'\n",
    "mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "df_1 = pd.DataFrame(mat['Data'])\n",
    "\n",
    "print(df.shape)\n",
    "print(df_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import median\n",
    "\n",
    "def apply_median_filter(df, window_size=10):\n",
    "    channel_data = df.iloc[:, :-1].values\n",
    "    labels = df.iloc[:, -1].values \n",
    "    filtered_channel_data = np.zeros_like(channel_data) \n",
    "\n",
    "    for i in range(channel_data.shape[1]):\n",
    "        print(i,\"channel\")\n",
    "        filtered_channel_data[:, i] = median(channel_data[:, i], selem=np.ones(window_size))\n",
    "\n",
    "    df_filtered = pd.DataFrame(filtered_channel_data, columns=df.columns[:-1])\n",
    "    df_filtered['Label'] = labels\n",
    "    print(df_filtered.shape, \"shape after median filter\")\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_samples_block_counter(df_1,trial_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(7):\n",
    "    extra_samples_block_counter(data_dicts_list[-1][b],trial_order[b],b)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHUhd3A17lB+9o7HFw3Jl4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel2",
   "language": "python",
   "name": "kernel2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
