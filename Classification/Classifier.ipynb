{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696289517013,"user":{"displayName":"Mahdi Moeini","userId":"03671813669356560168"},"user_tz":-210},"id":"za0kvkt7u2Z5","outputId":"78ba6e7e-7a2c-4096-e595-beca84ab98ba"},"outputs":[],"source":["import sys\n","import mne\n","import scipy.io as sp\n","from scipy import interpolate\n","import numpy as np\n","import random\n","import pandas as pd\n","import multiprocessing as mp\n","import concurrent.futures\n","from mne.decoding import CSP\n","import pymrmr\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier as RF\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import logging\n","from scipy.io import loadmat\n","from scipy.signal import hamming\n","from scipy.signal import hann\n","from scipy.signal import blackman\n","from scipy.signal import kaiser\n","from scipy.signal import gaussian\n","from sklearn.decomposition import FastICA\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","# import lightgbm as lgb\n","# from catboost import CatBoostClassifier\n","# from sklearn.impute import KNNImputer\n","# from sklearn.decomposition import PCA\n","# from pyriemann.estimation import Covariances\n","# from pyriemann.tangentspace import TangentSpace\n","# from pyriemann.classification import MDM\n","from collections import Counter\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","\n","\n","# Set display options for NumPy\n","np.set_printoptions(threshold=np.inf)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["WINDOW_TIME_LENGTH = 4\n","SAMPLING_RATE = 250\n","WINDOW_SAMPLE_LENGTH = WINDOW_TIME_LENGTH*SAMPLING_RATE\n","NUMBER_OF_CHANNELS = 64\n","beta = 1.5\n","\n","num_channels = 64\n","epoch_length = 1000\n","sampling_freq = 250\n","number_of_runs = 10\n","number_of_components = 10\n","number_of_selected_features = 10\n","number_of_processes = 10\n","number_of_bands = 9\n","column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n","\n","\n","trial_order=[['Tongue','Feet','Mis','Hand'],\n","            ['Feet','Mis','Hand','Tongue'],\n","            ['Hand','Feet','Tongue','Mis'],\n","            ['Tongue','Mis','Hand','Feet'],\n","            ['Mis','Feet','Hand','Tongue'],\n","            ['Feet','Hand','Tongue','Mis'],\n","            ['Hand','Tongue','Mis','Feet'],\n","            ['Tongue','Feet','Mis','Hand'],\n","            ['Mis','Tongue','Hand','Feet']]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_task_rest_times(b_num):\n","    if b_num == 0:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 1:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]\n","        \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]\n","        \n","    elif b_num == 2:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    elif b_num == 3:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 4:\n","        task_time = [[16, 8, 20, 12],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16],\n","                    [8, 20, 12, 16]]\n","        \n","        rest_time = [[8, 12, 16, 20],\n","                    [16, 20, 12, 8],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16]]\n","        \n","    elif b_num == 5:\n","        task_time = [[16, 12, 8, 20],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12],\n","                    [12, 8, 16, 20]]\n","\n","        rest_time = [[12, 8, 16, 20],\n","                    [16, 8, 20, 12],\n","                    [20, 12, 16, 8],\n","                    [8, 16, 12, 20]]\n","        \n","    elif b_num == 6:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 8, 16, 12],\n","                    [8, 16, 12, 20],\n","                    [16, 20, 12, 8]]\n","\n","        rest_time = [[16, 8, 12, 20],\n","                    [12, 20, 8, 16],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12]]     \n","    elif b_num ==7:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]   \n","               \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]  \n","    \n","    elif b_num == 8:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    else:\n","        print(\"Error in block number\")\n","\n","    return task_time,rest_time\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def trial_times_genertor(task_times,rest_times):\n","    block_times = [item for pair in zip(task_times, rest_times) for item in pair]\n","    return block_times\n","    "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","def fill_zeros_with_average(matrix):\n","    # Iterate through the matrix\n","    for i in range(matrix.shape[0]):\n","        for j in range(matrix.shape[1]):\n","            for k in range(matrix.shape[2]):\n","                if matrix[i, j, k] == 0:\n","                    # Find the neighboring non-zero elements\n","                    neighbors = []\n","                    if i > 0 and matrix[i - 1, j, k] != 0:\n","                        neighbors.append(matrix[i - 1, j, k])\n","                    if i < matrix.shape[0] - 1 and matrix[i + 1, j, k] != 0:\n","                        neighbors.append(matrix[i + 1, j, k])\n","                    if j > 0 and matrix[i, j - 1, k] != 0:\n","                        neighbors.append(matrix[i, j - 1, k])\n","                    if j < matrix.shape[1] - 1 and matrix[i, j + 1, k] != 0:\n","                        neighbors.append(matrix[i, j + 1, k])\n","                    if k > 0 and matrix[i, j, k - 1] != 0:\n","                        neighbors.append(matrix[i, j, k - 1])\n","                    if k < matrix.shape[2] - 1 and matrix[i, j, k + 1] != 0:\n","                        neighbors.append(matrix[i, j, k + 1])\n","\n","                    # Fill the zero with the average of neighboring non-zero values\n","                    if neighbors:\n","                        matrix[i, j, k] = sum(neighbors) / len(neighbors)\n","\n","    return matrix"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def calc_csp(x_train, y_train, x_test):\n","    # csp = CSP(n_components=number_of_components, reg='ledoit_wolf', log=True)\n","    csp = CSP(number_of_components)\n","\n","\n","\n","    \n","    # x_train = fill_zeros_with_average(x_train)\n","    # x_train = np.add(x_train, 0.000001)\n","\n","\n","\n","    nan_count = np.isnan(x_train).sum()\n","    print(\"Number of NaN values:\", nan_count)\n","\n","    empty_field_count = np.count_nonzero(x_train == 0)\n","    print(\"Number of empty fields:\", empty_field_count)\n","\n","    zeros_locations_3d = np.where(x_train == 0)\n","    # print(\"Locations of zeros:\", zeros_locations)\n","    \n","# Printing indices and corresponding values\n","    # for depth_idx, row_idx, col_idx in zip(zeros_locations_3d[0], zeros_locations_3d[1], zeros_locations_3d[2]):\n","    #     value_at_zero_location = x_train[depth_idx, row_idx, col_idx]\n","    #     print(f\"Zero found at position ({depth_idx}, {row_idx}, {col_idx}) with value {value_at_zero_location}\")\n","\n","\n","    csp_fit = csp.fit(x_train, y_train)\n","    train_feat = csp_fit.transform(x_train)\n","    test_feat = csp_fit.transform(x_test)\n","    return train_feat, test_feat"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def class_extractor(number_of_epochs, class_1, class_2, data, labels):\n","    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n","    Final_labels = np.zeros((size,1)).astype(int)\n","    dataset = np.zeros((size,num_channels, epoch_length))\n","    index = 0\n","    for i in range(number_of_epochs):\n","        if labels[i,0] == class_1 or labels[i,0] == class_2:\n","            dataset[index,:,:] = data[i,:,:]\n","            Final_labels[index,0] = labels[i,0]\n","            index = index + 1\n","        else:\n","            continue\n","            \n","    return dataset, Final_labels"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def feature_extractor(dataset, labels, number_of_bands, test_data):\n","\n","    low_cutoff = 0\n","    \n","    for b in range(number_of_bands):\n","        logging.getLogger('mne').setLevel(logging.WARNING)\n","        low_cutoff += 4\n","        data = dataset.copy()\n","        data_test = test_data.copy() \n","\n","        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","\n","        #PCA\n","        # from mne.decoding import UnsupervisedSpatialFilter\n","        # from sklearn.decomposition import PCA, FastICA\n","\n","        # pca = UnsupervisedSpatialFilter(PCA(64), average=False)\n","        # pca_fit = pca.fit(filtered_data)\n","        # filtered_data = pca_fit.transform(filtered_data)\n","        # filtered_data_test = pca_fit.transform(filtered_data_test)\n","        # train_feats = filtered_data\n","        # test_feats = filtered_data_test\n","\n","        # filtered_data = data\n","        # filtered_data_test = data_test\n","        \n","        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test)\n","        if b == 0:\n","            train_features = train_feats\n","            test_features = test_feats\n","        else:\n","            train_features = np.concatenate((train_features, train_feats), axis = 1)\n","            test_features = np.concatenate((test_features, test_feats), axis = 1)\n","    \n","    return train_features, test_features"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def feature_selector(train_features, labels, number_of_selected_features):\n","    X = pd.DataFrame(train_features)\n","    y = pd.DataFrame(labels)\n","    K = number_of_selected_features\n","    \n","    df = pd.concat([y,X], axis = 1)\n","    df.columns = df.columns.astype(str)\n","        \n","    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n","    return selected_features"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def data_reader(path,p_num,block_list):\n","    data_dict = {}\n","    for b_num in block_list:\n","        print(b_num)\n","        mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","        df = pd.DataFrame(mat['Data'])\n","        data_dict[b_num] = df\n","    return data_dict\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def get_group_start_indices(dataframe):\n","    group_indices = []\n","    current_label = None\n","\n","    for idx, row in dataframe.iterrows():\n","        if row.iloc[64] != current_label:\n","            group_indices.append(idx)\n","            current_label = row.iloc[64]\n","\n","    return group_indices"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def extra_samples_counter(df,class_1,class_2):\n","    x=0\n","    i=0\n","    sampleList = []\n","    while i<len(df):\n","        if (df.iloc[i,64]==class_1):\n","            x+=1\n","        else:\n","            i-=1\n","            sampleList.append(x)\n","            x=0\n","            class_1,class_2 = class_2,class_1\n","        i+=1\n","    sampleList.append(x)\n","    print(sampleList)\n","    "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def extra_samples_block_counter(df,trial_order,b_num):\n","\n","    df.drop(df[df.iloc[:,64].isin(['Begin', 'End'])].index, inplace=True)\n","    df.reset_index(drop=True, inplace=True)    \n","    df['group'] = (df.iloc[:,64] != df.iloc[:,64].shift(1)).cumsum()\n","\n","    \n","    group_counts_Rest = df[df.iloc[:,64] == 'Rest'].groupby('group').size()\n","    with open('sampleList.txt', 'a') as file:\n","        file.write(f'block {b_num+1} '+'\\n')\n","        for j in range (len(trial_order)):\n","            print(trial_order[j])\n","            trial_num = j\n","            task_times,rest_times = get_task_rest_times(b_num)\n","            trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","            trial_samples = [item*SAMPLING_RATE for item in trial_times]\n","            group_counts_task = df[df.iloc[:,64] == trial_order[j]].groupby('group').size()\n","            sampleList = []\n","            for i in range(4):\n","                task = group_counts_task.iloc[i]\n","                rest = group_counts_Rest.iloc[4*j+i]\n","                sampleList.append(task)\n","                sampleList.append(rest)\n","            # extra_samples = [x-y for x,y in zip(sampleList,trial_samples)]\n","            file.write(', '.join(map(str, sampleList)) + f' trial={trial_order[j]} '+'\\n')\n","            print(sampleList)\n","        file.write('\\n\\n')\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def data_cleaner(df,class_1,class_2,tasks_time):\n","    # sys.exit() \n","    class_x = class_1\n","    class_y = class_2\n","    new_df = pd.DataFrame()\n","    trial_df = df.copy() \n","    print(tasks_time)\n","    for i in range(len(tasks_time)):\n","        sample_point = tasks_time[i]*SAMPLING_RATE\n","        if(trial_df.iloc[sample_point+1,64] == class_x ):\n","            if(i==len(tasks_time)-1):\n","                temp_df = trial_df.iloc[:sample_point,:]\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","            else:    \n","                temp_df = trial_df.iloc[:sample_point,:]\n","                next_task_idx = trial_df[trial_df.iloc[:, 64] == class_y].index\n","                trial_df.drop(trial_df.index[0:next_task_idx[0]], inplace=True)\n","                trial_df.reset_index(drop=True, inplace=True)\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","                class_x,class_y = class_y,class_x\n","\n","    return new_df"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def class_seperator(cleaned_df,class_1,class_2):\n","    df = cleaned_df\n","    sorting_order = {class_1: 0, class_2: 1}\n","\n","    df['sorting_order'] = df.iloc[:, 64].map(sorting_order)\n","    df.sort_values(by=['sorting_order', df.columns[64]], inplace=True)\n","    df.drop('sorting_order', axis=1, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def shuffler(dataset,labels):\n","    print(dataset.shape)\n","    print(labels.shape)\n","    np.random.seed(42)\n","    indices = np.random.permutation(len(dataset))\n","    shuffled_dataset = dataset[indices]\n","    shuffled_labels = labels[indices]\n","    return shuffled_dataset,shuffled_labels\n","    "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def cal_epoch(df_len,sliding_len,window_len):\n","    print(window_len,sliding_len,df_len)\n","    number_of_epochs = int((int(df_len-window_len)/sliding_len)) +1\n","    return number_of_epochs"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def data_label_attacher(cleaned_df,class_1,class_2,random_flag,class_seperator_flag,sliding_time):\n","    SLIDING_POINTS = sliding_time*SAMPLING_RATE\n","    window_time = WINDOW_TIME_LENGTH\n","    new_df_ = cleaned_df.copy()\n","    new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","    X = new_df_.to_numpy()\n","    X = np.transpose(X)\n","    number_of_epochs = cal_epoch(int(int(len(cleaned_df)/SAMPLING_RATE)),sliding_time,window_time)\n","    print(number_of_epochs)\n","    dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","    index = get_group_start_indices(cleaned_df)\n","    index.append(len(cleaned_df))\n","    k = 0  \n","    startIdx = k * WINDOW_SAMPLE_LENGTH\n","    endIdx = (k+1) * WINDOW_SAMPLE_LENGTH \n","    l = 0\n","    label = 1\n","    for i in range(number_of_epochs):\n","        print(i,\"i is\")\n","        print(l,\"l is\")\n","        if(startIdx>=index[l] and endIdx<+index[l+1]):\n","            print(startIdx,endIdx,\"start and end in if\")\n","            slice_X = X[:, startIdx:endIdx]\n","\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","\n","            dataset[i, :, :] = slice_X\n","            labels[i,0] = label\n","\n","        else:\n","            \n","            temp = endIdx-index[l+1]\n","            print(temp,endIdx,index[l+1],\"temp,end,index l+1\")\n","            slice_X = X[:, startIdx:endIdx]\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","            dataset[i, :, :] = slice_X\n","\n","            if(temp<=WINDOW_SAMPLE_LENGTH/2):\n","\n","                labels[i,0] = label\n","            else:\n","                labels[i,0] = not(label)\n","\n","            if(startIdx>=index[l+1]):\n","                l+=1\n","                label = not(label)\n","\n","                \n","\n","            \n","\n","        startIdx+=SLIDING_POINTS\n","        endIdx+=SLIDING_POINTS\n","    \n","\n","\n","\n","\n","\n","\n","\n","\n","        # a = df_len - wdinow_len\n","        # a/sliding_len\n","        # b = a%sliding_len\n","\n","\n","\n","\n","\n","####################################################\n","\n","\n","    # new_df_ = cleaned_df.copy()\n","    # new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","    # X = new_df_.to_numpy()\n","    # X = np.transpose(X)\n","    # number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n","    # number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/SLIDING_POINTS) +1\n","\n","    \n","    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    # labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    \n","    # #Initialization\n","    # if class_seperator_flag:\n","    #     seperated_class_df = class_seperator(cleaned_df,class_1,class_2)\n","    #     new_df_ = seperated_class_df.copy()\n","    #     new_df_.drop(seperated_class_df.columns[-1], axis=1, inplace=True)\n","    #     X = new_df_.to_numpy()\n","    #     X = np.transpose(X)\n","    #     empty_field_count = np.count_nonzero(X == 0)\n","    #     print(\"Number of empty fields in X:\", empty_field_count)\n","    #     # zero_indices = np.where(X == 0)\n","    #     # print(\"befor filling\",len(zero_indices[0]))\n","    #     # X[zero_indices] += 0.001\n","    #     # zero_indices = np.where(X == 0)\n","    #     # print(\"after filling\",len(zero_indices[0]))\n","    #     number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n","    #     print(number_of_epochs)\n","    # else :  \n","    #     new_df_ = cleaned_df.copy()\n","    #     new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","    #     X = new_df_.to_numpy()\n","    #     X = np.transpose(X)\n","    #     empty_field_count = np.count_nonzero(X == 0)\n","    #     print(\"Number of empty fields in X:\", empty_field_count)\n","    #     # zero_indices = np.where(X == 0)\n","    #     # print(\"befor filling\",len(zero_indices[0]))\n","    #     # X[zero_indices] += 0.001\n","    #     # zero_indices = np.where(X == 0)\n","    #     # print(\"after filling\",len(zero_indices[0]))\n","\n","    #     number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n","\n","    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    # labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","    # if class_seperator_flag:\n","    #     i = 0  \n","    #     startIdx = i * WINDOW_SAMPLE_LENGTH\n","    #     endIdx = (i+1) * WINDOW_SAMPLE_LENGTH \n","    #     while(endIdx<=int(len(new_df_))/2):\n","    #         slice_X = X[:, startIdx:endIdx]\n","\n","    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","    #         slice_X *= kaiser_window\n","\n","    #         dataset[i, :, :] = slice_X\n","    #         labels[i,0] = 0\n","    #         # if (seperated_class_df.iloc[startIdx, 64] == class_1):\n","    #         #     labels[i,0] = 0\n","    #         # elif(seperated_class_df.iloc[startIdx, 64] == class_2):\n","    #         #     labels[i,0] = 1\n","    #         # else:\n","    #         #     labels[i,0] = 2\n","    #         startIdx+=TR_SLIDING_POINTS\n","    #         endIdx+=TR_SLIDING_POINTS\n","    #         i+=1\n","    #     # print(int(len(new_df_))/2,\"len\")    \n","    #     # print(endIdx,\"endIdx\")    \n","    #     # print(seperated_class_df.iloc[endIdx-2:endIdx+2,64])\n","       \n","    #     j = i\n","        \n","    #     startIdx = endIdx-TR_SLIDING_POINTS\n","    #     endIdx = startIdx+WINDOW_SAMPLE_LENGTH\n","    #     print(j, \"j is this\")\n","    #     while(endIdx<=int(len(new_df_))):\n","    #         slice_X = X[:, startIdx:endIdx]\n","\n","    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","    #         slice_X *= kaiser_window\n","\n","    #         dataset[j, :, :] = slice_X\n","    #         labels[j,0] = 1\n","    #         # if (cleaned_df.iloc[startIdx, 64] == class_1):\n","    #         #     labels[j,0] = 0\n","    #         # elif(cleaned_df.iloc[startIdx, 64] == class_2):\n","    #         #     labels[j,0] = 1\n","    #         # else:\n","    #         #     labels[j,0] = 2\n","    #         startIdx+=TR_SLIDING_POINTS\n","    #         endIdx+=TR_SLIDING_POINTS\n","    #         j+=1\n","    #     print(j, \"j is this\")\n","    #     # dataset,labels = shuffler(dataset,labels)\n","\n","    # else:\n","    #     i = 0  \n","    #     start_idx = i * WINDOW_SAMPLE_LENGTH\n","    #     end_idx = (i+1) * WINDOW_SAMPLE_LENGTH \n","    #     while (end_idx<=int(len(new_df_))):\n","    #         slice_X = X[:, start_idx:end_idx]\n","\n","    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","    #         slice_X *= kaiser_window\n","            \n","    #         dataset[i, :, :] = slice_X\n","    #         if (cleaned_df.iloc[start_idx, 64] == class_1):\n","    #             labels[i,0] = 0\n","    #         elif(cleaned_df.iloc[start_idx, 64] == class_2):\n","    #             labels[i,0] = 1\n","    #         else:\n","    #             labels[i,0] = 2\n","    #         start_idx+=SLIDING_POINTS\n","    #         end_idx+=SLIDING_POINTS\n","    #         i+=1\n","    #     # dataset,labels = shuffler(dataset,labels)\n","\n","\n","\n","#####################################################\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    #For training and test purpose\n","    # if random_flag:\n","    #     randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n","    # else:\n","    #     randomlist = list(range(number_of_epochs))\n","    #Labeling the data\n","\n","\n","\n","    # for i in range(number_of_epochs):\n","    #     start_idx = randomlist[i] * WINDOW_SAMPLE_LENGTH + SLIDING_POINTS\n","    #     end_idx = (randomlist[i] + 1) * WINDOW_SAMPLE_LENGTH\n","    #     slice_X = X[:, start_idx:end_idx]\n","\n","    #     # hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hamming_window\n","\n","    #     # hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hanning_window\n","\n","    #     # blackman_window = blackman(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= blackman_window\n","\n","    #     # kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= kaiser_window\n","\n","    #     # gaussian_window = gaussian(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= gaussian_window\n","\n","\n","    #     dataset[i, :, :] = slice_X\n","    #     if (cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_1):\n","    #         labels[i,0] = 0\n","    #     elif(cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_2):\n","    #         labels[i,0] = 1\n","    #     else:\n","    #         labels[i,0] = 2\n","    \n","    # empty_field_count = np.count_nonzero(dataset == 0)\n","    # print(\"Number of empty fields in dataset:\", empty_field_count,\"dataset shape\",dataset.shape)\n","    print(labels)\n","    return dataset,labels\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def trial_cutter(data, class_1):\n","    df = data.copy()\n","    Begin_trigger = \"Begin\" + \"_\" + class_1\n","    End_trigger = \"End\" + \"_\" + class_1\n","    Begin_idx = df[df.iloc[:, 64] == Begin_trigger].index\n","    End_idx = df[df.iloc[:, 64] == End_trigger].index\n","    trial_df = df.iloc[Begin_idx[0]+1:End_idx[0],:]\n","    trial_df.reset_index(drop=True, inplace=True)\n","    trial_df.head()\n","    return trial_df"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def Begin_End_trigger_modifier(data):\n","    df = data.copy()\n","    Begin_indexes = df[df.iloc[:, 64] == 'Begin'].index\n","    End_indexes = df[df.iloc[:, 64] == 'End'].index\n","    if(len(Begin_indexes)==len(End_indexes)):\n","        for i in range(len(Begin_indexes)):\n","            index = Begin_indexes[i]+1\n","            val = df.iloc[index,64]\n","            df.iloc[Begin_indexes[i],64] = \"Begin\" + \"_\" + str(val)\n","            df.iloc[End_indexes[i],64]   =  \"End\" + \"_\" + str(val)\n","    else:\n","        print(\"Trigger seinding Exception\")\n","    \n","    return df"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def preprocessor(data_,class_1,class_2,tasks_time,set_type,clean_flag,sliding_time):\n","    CLASS_1 = class_1\n","    CLASS_2 = class_2\n","    df = data_.copy()\n","    modified_df = Begin_End_trigger_modifier(df)\n","    trial_df = trial_cutter(modified_df,CLASS_1)\n","    print(trial_df.shape,\"trial_df\")\n","    indexes = get_group_start_indices(trial_df)\n","    print(indexes,'tasks index starting point')\n","    if clean_flag:\n","        cleaned_df = data_cleaner(trial_df,CLASS_1,CLASS_2,tasks_time)\n","        final_df = cleaned_df.copy()\n","    else:\n","        final_df = trial_df.copy()\n","    print(final_df.shape,\"final_df\")\n","\n","    if set_type ==\"TRAIN\":\n","        random_flag = True\n","    elif set_type ==\"TEST\":\n","        random_flag = False\n","    else:\n","        print(\"Error in set type\")\n","\n","  \n","    final_data, final_labels = data_label_attacher(final_df,CLASS_1,CLASS_2,random_flag,clean_flag,sliding_time)\n","      \n","    print(final_data.shape,\"final_data shape\")\n","    print(final_labels.shape,\"final_labels shape\")\n","    \n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def trials_set_builder(data_dict,blocks_set,set_label,class_1,class_2,clean_flag,sliding_time):\n","    counter = 0\n","\n","    for b_num in blocks_set:\n","        trial_num = trial_order[b_num].index(class_1)\n","        task_times,rest_times = get_task_rest_times(b_num)\n","        print(task_times[trial_num],rest_times[trial_num])\n","        trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","        print(trial_times)\n","        data = data_dict[b_num]\n","        df = data.copy()\n","        # last_column = df.pop(df.columns[-1])\n","        # df.drop(df.columns[-1], axis=1, inplace=True)\n","        # eeg_data = df.to_numpy().T  # Transpose to have channels in columns\n","\n","        # channel_names = [f'Ch{i+1}' for i in range(63)]\n","\n","        # # Create MNE-Python RawArray object\n","        # info = mne.create_info(ch_names=channel_names, sfreq=sampling_freq, ch_types='eeg')\n","        # raw = mne.io.RawArray(eeg_data, info)\n","\n","        # # Apply ICA\n","        # ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n","        # ica.fit(raw)\n","        # ica_components = ica.get_components()\n","\n","        # # Convert the ICA components to a DataFrame\n","        # df2 = pd.DataFrame(data=ica_components.T, columns=channel_names)\n","        # df2 = df2.assign(LastColumn=last_column)\n","        # # df = data.copy(deep=False)\n","        dataset,labels = preprocessor(df,class_1,class_2,trial_times,set_label,clean_flag,sliding_time)\n","        # print(dataset.shape)\n","\n","        if counter == 0 :\n","            final_data = dataset\n","            final_labels = labels\n","            print(\"Before concatenation - final_data shape:\", final_data.shape, \"dataset shape:\", dataset.shape)\n","        else:\n","            final_data = np.vstack((final_data, dataset))\n","            final_labels = np.vstack((final_labels, labels))\n","            print(\"After concatenation - final_data shape:\", final_data.shape, \"final_labels shape:\", final_labels.shape)\n","\n","        counter+=1 \n","    # empty_field_count = np.count_nonzero(final_data == 0)\n","    # print(\"Number of empty fields in final_data:\", empty_field_count,\"final_data shape\",final_data.shape)\n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def find_duplicates(data_list):\n","    counted_values = Counter(data_list)\n","    duplicate_values = {value: count for value, count in counted_values.items() if count > 1}\n","    return duplicate_values"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def Statistical_analysor(p_num_list,data_dicts_list):\n","\n","    with open('/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Classification/Statistics.txt', 'w') as file:\n","        for p in range(len(p_num_list)):\n","            file.write(f'Particpant: {p+3} '+'\\n')\n","            for b in range(7):\n","                file.write(f'Block: {b+1} '+'\\n')\n","                data_pd = data_dicts_list[p][b]\n","                data = data_pd.iloc[:, :-1]\n","                data_np = data.values\n","                eeg_data = data_np\n","                print(\"Data type:\", type(eeg_data))\n","                print(\"Shape:\", eeg_data.shape)\n","                eeg_data = np.array(eeg_data)\n","                mean_values = np.mean(eeg_data, axis=0)\n","                variance_values = np.var(eeg_data, axis=0)\n","                std_deviation_values = []\n","                \n","                for i in range(num_channels):\n","                    print(f\"Channel {i + 1}:\")\n","                    print(f\"Mean: {mean_values[i]}\")\n","                    print(f\"Variance: {variance_values[i]}\")\n","                    std_deviation_values.append(np.sqrt(variance_values[i]))\n","                    print(f\"Standard Deviation: {std_deviation_values[i]}\")\n","                    print()\n","                    file.write(f'Channel {i+1}: '+'\\n')\n","                    file.write(f\"Mean: {mean_values[i]}\"+\"\\n\")\n","                    file.write(f\"Variance: {variance_values[i]}\"+\"\\n\")\n","                    file.write(f\"Standard Deviation: {std_deviation_values[i]}\"+\"\\n\\n\")\n","                \n","                lists_to_check = {\n","                'mean_values': mean_values,\n","                'variance_values': variance_values,\n","                'std_deviation_values': std_deviation_values\n","                }\n","                for list_name, data_list in lists_to_check.items():\n","                    duplicate_values = find_duplicates(data_list)\n","                    if duplicate_values:\n","                        print(f\"Duplicate values and their counts for {list_name}:\")\n","                        file.write(f\"Duplicate values and their counts for {list_name}:\"+\"\\n\")\n","                        for value, count in duplicate_values.items():\n","                            print(f\"Value: {value}, Count: {count}\")\n","                            file.write(f\"Value: {value}, Count: {count}\"+\"\\n\")\n","                    else:\n","                        print(f\"No duplicate values found in the {list_name} list.\")\n","                        file.write(f\"No duplicate values found in the {list_name} list.\"+\"\\n\")\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def custom_accuracy(y_true, y_pred):\n","    mismatches = []\n","    total = len(y_true)\n","    mismatch_count = 0\n","    \n","    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n","        if true_label != pred_label:\n","            mismatches.append(i)\n","            mismatch_count += 1\n","            \n","    accuracy = 1 - (mismatch_count / total)\n","    \n","    return accuracy, mismatch_count, mismatches"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def majority_vote_sliding_with_next(prediction_list, window_size=3):\n","    majority_votes = []\n","    \n","    for i in range(len(prediction_list) - window_size + 1):\n","        window = prediction_list[i:i+window_size]\n","        window_tuple = tuple(window)\n","        counts = Counter(window_tuple)\n","        majority = counts.most_common(1)[0][0]\n","        majority_votes.append(majority)\n","        \n","    return majority_votes\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def majority_vote_sliding_with_prev(prediction_list, window_size=3):\n","    majority_votes = []\n","    \n","    for i in range(len(prediction_list)):\n","        if i >= window_size - 1:\n","            start_index = i - window_size + 1\n","            window = prediction_list[start_index:i+1]\n","            counts = Counter(window)\n","            majority = counts.most_common(1)[0][0]\n","            majority_votes.append(majority)\n","        \n","    return majority_votes"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def majority_vote_sliding_with_prev_v2(prediction_list, window_size=3):\n","    majority_votes = []\n","    \n","    for i in range(len(prediction_list)):\n","        start_index = max(0, i - window_size + 1)\n","        window = prediction_list[start_index:i+1]\n","        counts = Counter(window)\n","        majority = counts.most_common(1)[0][0]\n","        majority_votes.append(majority)\n","        \n","    return majority_votes"]},{"cell_type":"markdown","metadata":{},"source":["# Reading Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["block_list = [0,1,2,3,4,5,6]\n","p_num_list = [10]\n","data_dicts_list = []\n","for p_num in p_num_list:\n","    print(f'reading P{p_num}')\n","    data_dict = data_reader(f'../../Participants/P{p_num}/', p_num, block_list)\n","    data_dicts_list.append(data_dict)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reading P10\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n"]}],"source":["block_list = [0,1,2,3,4,5,6]\n","p_num_list = [10]\n","data_dicts_list = []\n","for p_num in p_num_list:\n","    print(f'reading P{p_num}')\n","    data_dict = data_reader(f'/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/P{p_num}/',p_num,block_list)\n","    data_dicts_list.append(data_dict)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Running Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n","class_1_list = ['Hand','Feet','Tongue','Mis']\n","class_2 = 'Rest'\n","p_num_list = [8]\n","train_blocks_set = [0,1,2,3,4]\n","test_blocks_set = [5,6]\n","sliding_time_tr = 4\n","sliding_time_te = 4\n","vote_window = 4\n","params = {\n","    'max_depth': 5,\n","    'min_child_weight': 1,\n","    'gamma': 0,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'learning_rate': 0.1,\n","}\n","\n","\n","p = 0\n","for p_num in p_num_list:\n","    import time\n","    start_time = time.time()\n","    for class_1 in class_1_list:\n","        X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blocks_set,'TRAIN',class_1,class_2,True,sliding_time_tr)\n","        X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blocks_set,'TEST',class_1,class_2,True,sliding_time_te)\n","\n","        print(X_tr.shape,Y_tr.shape,\"train shape\")\n","        print(X_te.shape,Y_te.shape,\"test shape\")\n","\n","        [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te)\n","        selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n","\n","        train_acc_list = []\n","        test_acc_list = []\n","\n","        clf = XGBClassifier()\n","        for r in range(1):\n","            clf.fit(train_features[:, selected_features], Y_tr[:,0])\n","\n","            y_pr_te = clf.predict(test_features[:, selected_features])\n","            y_pr_tr = clf.predict(train_features[:,selected_features])\n","\n","            accuracy_te = accuracy_score(Y_te, y_pr_te)\n","            test_acc_list.append(accuracy_te)\n","            for i in range(len(Y_te)):\n","                print(f\"Test_Real: {Y_te[i][0]}   Test_Predication: {y_pr_te[i]}\")\n","            # print(Y_te.shape,y_pr_te.shape,\"shape \")\n","            y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n","            Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n","\n","            for i in range(len(Y_te)):\n","                print(f\"Test_Real_Vote: {Y_te_Vote[i]}   Test_Predication_vote: {y_pr_te_Vote[i]}\")\n","\n","\n","            acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n","            print(acc,num_of_mismatches,mismatches_list, \"acc, num_of_mismatches ,mismatches_list\",class_1)\n","\n","            accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n","            train_acc_list.append(accuracy_tr)\n","\n","\n","\n","        end_time = time.time()\n","        running_time = end_time-start_time\n","        participant = p_num\n","        class1 = class_1\n","        class2 = class_2\n","        running_time = running_time\n","        test_acc = np.average(test_acc_list)\n","        train_acc = np.average(train_acc_list)\n","        test_size = X_te.shape\n","        train_size = X_tr.shape\n","        train_block = '01234'\n","        test_block = '56'\n","\n","\n","\n","\n","\n","        # new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block]\n","\n","        # new_row_df = pd.DataFrame([new_row], columns=column_names)\n","        # rf = pd.read_csv(PATH +'P'+str(p_num)+'.csv')\n","        # cf = pd.concat([rf, new_row_df], ignore_index=True)\n","        # cf.to_csv(PATH +'P'+str(p_num)+'.csv',index=False)\n","\n","\n","\n","        print(train_acc_list,\"train\",class_1)\n","        print(test_acc_list,\"test\",class_1)\n","        \n","    i+=1\n","\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["# Side Analysis"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# #Frame Maker\n","# PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n","# df = pd.read_csv(PATH+'frame.csv')\n","# p_num_list = [3]\n","# for p_num in p_num_list:\n","#     df.to_csv(PATH+'P'+str(p_num)+'.csv',index=False)\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Majority Votes: [1, 1, 1, 0, 0, 1, 1, 0]\n","Majority Votes Previous: [1, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n"]}],"source":["prediction_list = [1, 1, 1, 1, 0, 0, 1, 1, 0, 0]  # Replace with your actual prediction list\n","\n","result = majority_vote_sliding_with_next(prediction_list)\n","result2 = majority_vote_sliding_with_prev_v2(prediction_list)\n","print(\"Majority Votes:\", result)\n","print(\"Majority Votes Previous:\", result2)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.8571428571428571]\n","[0.8065476190476191]\n","[0.7767857142857143]\n","[0.75]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>b1234</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hand</td>\n","      <td>0.857143</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Feet</td>\n","      <td>0.806548</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tongue</td>\n","      <td>0.776786</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mis</td>\n","      <td>0.750000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    class     b1234\n","0    Hand  0.857143\n","1    Feet  0.806548\n","2  Tongue  0.776786\n","3     Mis  0.750000"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["p_num_list = [3,4,5,6,7,9]\n","vf = pd.DataFrame(columns=column_names) \n","for p_num in p_num_list:\n","    rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n","    vf = pd.concat([vf, rf], ignore_index=True)\n","vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n","# vf.tail()\n","    \n","columnNames = ['class','b1234']\n","kf = pd.DataFrame(columns=columnNames)\n","kf.to_csv(PATH+'AverageAcc.csv',index=False)\n","\n","vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n","df = vf\n","\n","class_list=['Hand','Feet','Tongue','Mis']\n","blk_list = [1234]\n","for class_ in class_list:\n","    avg_list = []\n","    for blk in blk_list:\n","        gf = df[(df['train_block'] == blk) & (df['class1'] == class_)]\n","        avg = gf['test_acc'].mean()\n","        avg_list.append(avg)\n","    print(avg_list)    \n","    new_row = [class_, avg_list[0]] \n","    new_row_df = pd.DataFrame([new_row], columns=columnNames)\n","    rf = pd.read_csv(PATH + 'AverageAcc.csv')\n","    cf = pd.concat([rf, new_row_df], ignore_index=True)\n","    cf.to_csv(PATH +'AverageAcc.csv',index=False)  \n","kf = pd.read_csv(PATH +'AverageAcc.csv') \n","kf.head()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      4         for     b\n","4      5     example     b\n","5      6    purposes     b\n","6      7          in     a\n","7      8        this     a\n","8      9        case     a\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n","his\n","    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      7          in     a\n","4      8        this     a\n","5      9        case     a\n","6      4         for     b\n","7      5     example     b\n","8      6    purposes     b\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n"]}],"source":["# Assuming df is your DataFrame with the last column named 'label'\n","data = {'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n","        'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter'],\n","        'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b']}\n","\n","df = pd.DataFrame(data)\n","\n","print(df)\n","print(\"his\")\n","# Define a custom sorting order based on the desired grouping\n","sorting_order = {'a': 0, 'b': 1}\n","\n","# Create a new column with the sorting order\n","df['sorting_order'] = df.iloc[:, 2].map(sorting_order)\n","\n","# Sort the DataFrame based on the new column and the original order within each group\n","df.sort_values(by=['sorting_order', df.columns[2]], inplace=True)\n","\n","# Drop the temporary sorting column\n","df.drop('sorting_order', axis=1, inplace=True)\n","\n","# Optional: Reset the index if needed\n","df.reset_index(drop=True, inplace=True)\n","\n","# Display the sorted DataFrame\n","print(df)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3, 3, 3, 4]\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','b'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","x=0\n","i=0\n","class_1 = 'a'\n","class_2 = 'b'\n","sampleList = []\n","while i<len(df):\n","    if (df.iloc[i,2]==class_1):\n","        x+=1\n","    else:\n","        i-=1\n","        sampleList.append(x)\n","        x=0\n","        class_1,class_2 = class_2,class_1\n","    i+=1\n","sampleList.append(x)\n","print(sampleList)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 3, 6, 8]\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'it', 'does', 'not matter'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'b', 'b', 'b']\n","}\n","df = pd.DataFrame(data)\n","\n","print(get_group_start_indices(df))\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["group\n","2    3\n","4    4\n","dtype: int64\n","2\n","3\n","group\n","1    3\n","3    3\n","dtype: int64\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','c'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Identify consecutive groups of 'a's by creating a new group ID each time 'label' changes from 'b' to 'a'\n","df['group'] = (df['label'] != df['label'].shift(1)).cumsum()\n","\n","# Count occurrences of 'a' within each group\n","group_counts = df[df['label'] == 'a'].groupby('group').size()\n","\n","group_counts_b = df[df['label'] == 'b'].groupby('group').size()\n","print(group_counts_b)\n","print(group_counts_b.index[0])\n","print(group_counts_b.iloc[0])\n","print(group_counts)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["p_num = 6\n","b_num = 7\n","path = f'../../Participants/P{p_num}/'\n","mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","df_1 = pd.DataFrame(mat['Data'])\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","[6191, 10157, 8157, 4065, 10161, 8156, 4060, 6014]\n","[8156, 8156, 6178, 10157, 10156, 4064, 4063, 6015]\n","[10158, 6176, 8165, 10155, 4073, 8156, 6184, 4016]\n","[10165, 10155, 6183, 6177, 4060, 4062, 8162, 8016]\n"]}],"source":["extra_samples_block_counter(df_1,trial_order[0])"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","Tongue\n","[3093, 5078, 4078, 2030, 5078, 4079, 2032, 3007]\n","Feet\n","[4078, 4078, 3087, 5079, 5078, 2030, 2035, 3007]\n","Mis\n","[5080, 3089, 4076, 5079, 2036, 4077, 3093, 2007]\n","Hand\n","[5080, 5078, 3092, 3088, 2035, 2030, 4077, 4007]\n","hi\n","Feet\n","[3091, 4082, 2034, 3093, 5082, 2034, 4079, 5007]\n","Mis\n","[4083, 2033, 5079, 5082, 2031, 3090, 3087, 4007]\n","Hand\n","[2033, 5078, 5079, 4078, 4079, 2036, 3090, 3007]\n","Tongue\n","[2033, 3089, 3092, 4078, 5082, 5082, 4079, 2008]\n","hi\n","Hand\n","[4078, 2035, 2036, 5082, 3089, 4083, 5083, 3008]\n","Feet\n","[5077, 3088, 4078, 2035, 3088, 5077, 2033, 4007]\n","Tongue\n","[3088, 4082, 5082, 3087, 2031, 5079, 4077, 2007]\n","Mis\n","[2037, 2035, 3093, 3091, 4076, 5079, 5081, 4007]\n","hi\n","Tongue\n","[3087, 5081, 4082, 2035, 5077, 4077, 2031, 3008]\n","Mis\n","[4082, 4083, 3089, 5156, 5078, 2111, 2026, 3007]\n","Hand\n","[5105, 3088, 4106, 5077, 2065, 4076, 3122, 2007]\n","Feet\n","[5077, 5116, 3090, 3121, 2026, 2049, 4079, 4008]\n","hi\n","Mis\n","[4077, 2034, 2061, 3092, 5171, 4082, 3165, 5008]\n","Feet\n","[3201, 4078, 4077, 5177, 2030, 3166, 5083, 2007]\n","Hand\n","[5079, 3132, 2026, 4084, 3125, 2031, 4124, 5006]\n","Tongue\n","[2035, 5102, 5097, 2037, 3105, 3118, 4112, 4007]\n","hi\n","Feet\n","[4095, 3110, 3110, 2034, 2040, 4114, 5094, 5007]\n","Hand\n","[5089, 4087, 4090, 2039, 3101, 5087, 2047, 3008]\n","Tongue\n","[2029, 5086, 4086, 3095, 5085, 4088, 3106, 2008]\n","Mis\n","[3101, 2042, 2037, 4086, 4084, 3094, 5089, 5007]\n","hi\n","Hand\n","[4085, 4090, 2043, 2038, 3096, 3095, 5090, 5007]\n","Tongue\n","[5089, 3097, 2043, 5089, 4084, 2043, 3095, 4008]\n","Mis\n","[2030, 5086, 4100, 4086, 3100, 3095, 5090, 2008]\n","Feet\n","[4084, 2037, 5084, 4084, 3101, 5090, 2038, 3008]\n"]}],"source":["for b in range(7):\n","    extra_samples_block_counter(data_dicts_list[-1][b],trial_order[b],b)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMHUhd3A17lB+9o7HFw3Jl4","provenance":[]},"kernelspec":{"display_name":"kernel2","language":"python","name":"kernel2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
