{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696289517013,"user":{"displayName":"Mahdi Moeini","userId":"03671813669356560168"},"user_tz":-210},"id":"za0kvkt7u2Z5","outputId":"78ba6e7e-7a2c-4096-e595-beca84ab98ba"},"outputs":[],"source":["import sys\n","import mne\n","import scipy.io as sp\n","from scipy import interpolate\n","import numpy as np\n","import random\n","import pandas as pd\n","import multiprocessing as mp\n","import concurrent.futures\n","from mne.decoding import CSP\n","import pymrmr\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier as RF\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import logging\n","from scipy.io import loadmat\n","from scipy.signal import hamming\n","from scipy.signal import hann\n","from scipy.signal import blackman\n","from scipy.signal import kaiser\n","from scipy.signal import gaussian\n","from sklearn.decomposition import FastICA\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","import lightgbm as lgb\n","from catboost import CatBoostClassifier\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","from pyriemann.estimation import Covariances\n","from pyriemann.tangentspace import TangentSpace\n","from pyriemann.classification import MDM\n","import medusa\n","import medusa.bci.mi_paradigms\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","\n","\n","# Set display options for NumPy\n","np.set_printoptions(threshold=np.inf)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["WINDOW_TIME_LENGTH = 4\n","SAMPLING_RATE = 250\n","TR_SLIDING_WINDOW_TIME = 2\n","WINDOW_SAMPLE_LENGTH = WINDOW_TIME_LENGTH*SAMPLING_RATE\n","NUMBER_OF_CHANNELS = 64\n","SLIDING_TIME = 4 \n","SLIDING_POINTS = SLIDING_TIME*SAMPLING_RATE\n","TR_SLIDING_POINTS = TR_SLIDING_WINDOW_TIME*SAMPLING_RATE\n","beta = 1.5\n","\n","num_channels = 64\n","epoch_length = 1000\n","sampling_freq = 250\n","number_of_runs = 10\n","# number_of_splits = 10\n","number_of_components = 10\n","number_of_selected_features = 10\n","number_of_processes = 10\n","number_of_bands = 9\n","# rf = pd.DataFrame()\n","column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n","# rf = rf.reindex(columns=column_names)\n","\n","trial_order=[['Tongue','Feet','Mis','Hand'],\n","            ['Feet','Mis','Hand','Tongue'],\n","            ['Hand','Feet','Tongue','Mis'],\n","            ['Tongue','Mis','Hand','Feet'],\n","            ['Mis','Feet','Hand','Tongue'],\n","            ['Feet','Hand','Tongue','Mis'],\n","            ['Hand','Tongue','Mis','Feet'],\n","            ['Tongue','Feet','Mis','Hand'],\n","            ['Mis','Tongue','Hand','Feet']]\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_task_rest_times(b_num):\n","    if b_num == 0:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 1:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]\n","        \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]\n","        \n","    elif b_num == 2:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    elif b_num == 3:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 4:\n","        task_time = [[16, 8, 20, 12],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16],\n","                    [8, 20, 12, 16]]\n","        \n","        rest_time = [[8, 12, 16, 20],\n","                    [16, 20, 12, 8],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16]]\n","        \n","    elif b_num == 5:\n","        task_time = [[16, 12, 8, 20],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12],\n","                    [12, 8, 16, 20]]\n","\n","        rest_time = [[12, 8, 16, 20],\n","                    [16, 8, 20, 12],\n","                    [20, 12, 16, 8],\n","                    [8, 16, 12, 20]]\n","        \n","    elif b_num == 6:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 8, 16, 12],\n","                    [8, 16, 12, 20],\n","                    [16, 20, 12, 8]]\n","\n","        rest_time = [[16, 8, 12, 20],\n","                    [12, 20, 8, 16],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12]]     \n","    elif b_num ==7:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]   \n","               \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]  \n","    \n","    elif b_num == 8:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    else:\n","        print(\"Error in block number\")\n","\n","    return task_time,rest_time\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def find_zero_order(matrix_3d):\n","    print(matrix_3d.shape,'matrix shape')\n","    depth, rows, cols = matrix_3d.shape\n","    \n","    for i in range(min(rows, cols)):\n","        print(i,'i isssssssssssss')\n","        sub_matrix = matrix_3d[:, :i+1, :i+1]\n","        determinant = np.linalg.det(sub_matrix)\n","        print(\"det\",determinant,i+1)\n","        \n","    #     if determinant == 0:\n","    #         return i + 1  # Return the order where the leading minor becomes zero\n","    \n","    # return -1  # Return -1 if all leading minors are non-zero"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def trial_times_genertor(task_times,rest_times):\n","    block_times = [item for pair in zip(task_times, rest_times) for item in pair]\n","    return block_times\n","    "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","def fill_zeros_with_average(matrix):\n","    # Iterate through the matrix\n","    for i in range(matrix.shape[0]):\n","        for j in range(matrix.shape[1]):\n","            for k in range(matrix.shape[2]):\n","                if matrix[i, j, k] == 0:\n","                    # Find the neighboring non-zero elements\n","                    neighbors = []\n","                    if i > 0 and matrix[i - 1, j, k] != 0:\n","                        neighbors.append(matrix[i - 1, j, k])\n","                    if i < matrix.shape[0] - 1 and matrix[i + 1, j, k] != 0:\n","                        neighbors.append(matrix[i + 1, j, k])\n","                    if j > 0 and matrix[i, j - 1, k] != 0:\n","                        neighbors.append(matrix[i, j - 1, k])\n","                    if j < matrix.shape[1] - 1 and matrix[i, j + 1, k] != 0:\n","                        neighbors.append(matrix[i, j + 1, k])\n","                    if k > 0 and matrix[i, j, k - 1] != 0:\n","                        neighbors.append(matrix[i, j, k - 1])\n","                    if k < matrix.shape[2] - 1 and matrix[i, j, k + 1] != 0:\n","                        neighbors.append(matrix[i, j, k + 1])\n","\n","                    # Fill the zero with the average of neighboring non-zero values\n","                    if neighbors:\n","                        matrix[i, j, k] = sum(neighbors) / len(neighbors)\n","\n","    return matrix"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def fill_zeros_with_interpolation(arr):\n","    filled_arr = arr.copy()  # Create a copy to avoid modifying the original array\n","    for i in range(len(arr)):\n","        non_zero_indices = np.where(arr[i] != 0)[0]\n","        zero_indices = np.where(arr[i] == 0)[0]\n","\n","        # Interpolate zero values based on surrounding non-zero values\n","        filled_arr[i, zero_indices] = np.interp(zero_indices, non_zero_indices, arr[i, non_zero_indices])\n","\n","    return filled_arr"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def leading_minor_order_13(matrix_3d):\n","    order = 13  # Set the order of the leading minor\n","    \n","    # Extract the submatrix of order 13 from the top-left corner\n","    leading_submatrix = matrix_3d[:, :order, :order]\n","    \n","    # Calculate the determinant of the submatrix\n","    determinant = np.linalg.det(leading_submatrix)\n","    \n","    \n","    return determinant"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def calc_csp_v3(x_train,y_train,x_test):\n","    csp = medusa.CSP(10)\n","    csp_fit = csp.fit(x_train,y_train)\n","    train_feat = csp_fit.transform(x_train)\n","    test_feat = csp_fit.transform(x_test)\n","    return train_feat, test_feat\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def calc_csp_v2(x_train, y_train, x_test):\n","\n","    cov_matrices = Covariances().fit_transform(x_train)\n","    epsilon = 0.001  # Small regularization parameter\n","    cov_matrices_regularized = cov_matrices + epsilon * np.eye(cov_matrices.shape[1])\n","\n","    x_train = cov_matrices_regularized\n","\n","    csp = TangentSpace(metric='euclid', n_components=number_of_components)\n","    csp_fit = csp.fit(x_train, y_train)\n","    train_feat = csp_fit.transform(x_train)\n","    test_feat = csp_fit.transform(x_test)\n","\n","    return train_feat, test_feat\n","    "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def calc_csp(x_train, y_train, x_test):\n","    # csp = CSP(n_components=number_of_components, reg='ledoit_wolf', log=True)\n","    csp = CSP(number_of_components)\n","\n","\n","\n","    # reshaped_matrix = np.reshape(matrix_3d, (matrix_3d.shape[0], -1))\n","\n","    # # Calculate the rank of the reshaped 2D matrix\n","    # rank = np.linalg.matrix_rank(reshaped_matrix)\n","\n","    # # Determine if the matrix is full rank\n","    # if rank == min(reshaped_matrix.shape):\n","    #     print(\"The 3D matrix is full rank.\")\n","    # else:\n","    #     print(\"The 3D matrix is not full rank.\")\n","    \n","    # find_zero_order(x_train)\n","    # print(\"det\",np.linalg.det(x_train))\n","\n","    \n","    # data = x_train\n","    # for i in range(data.shape[1]):\n","    #     for j in range(data.shape[2]):\n","    #         nonzero_indices = np.where(data[:, i, j] != 0)[0]\n","    #         zero_indices = np.where(data[:, i, j] == 0)[0]\n","    #         if len(nonzero_indices) > 1:  # Interpolate only if there are non-zero values\n","    #             data[zero_indices, i, j] = np.interp(zero_indices, nonzero_indices, data[nonzero_indices, i, j])\n","    \n","    # x_train = data\n","\n","    \n","    # x_train = fill_zeros_with_average(x_train)\n","    # x_train = np.add(x_train, 0.000001)\n","\n","\n","\n","    nan_count = np.isnan(x_train).sum()\n","    print(\"Number of NaN values:\", nan_count)\n","\n","    empty_field_count = np.count_nonzero(x_train == 0)\n","    print(\"Number of empty fields:\", empty_field_count)\n","\n","    zeros_locations_3d = np.where(x_train == 0)\n","    # print(\"Locations of zeros:\", zeros_locations)\n","    \n","\n","# Printing indices and corresponding values\n","    # for depth_idx, row_idx, col_idx in zip(zeros_locations_3d[0], zeros_locations_3d[1], zeros_locations_3d[2]):\n","    #     value_at_zero_location = x_train[depth_idx, row_idx, col_idx]\n","    #     print(f\"Zero found at position ({depth_idx}, {row_idx}, {col_idx}) with value {value_at_zero_location}\")\n","\n","\n","    csp_fit = csp.fit(x_train, y_train)\n","    train_feat = csp_fit.transform(x_train)\n","    test_feat = csp_fit.transform(x_test)\n","    return train_feat, test_feat"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def class_extractor(number_of_epochs, class_1, class_2, data, labels):\n","    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n","    Final_labels = np.zeros((size,1)).astype(int)\n","    dataset = np.zeros((size,num_channels, epoch_length))\n","    index = 0\n","    for i in range(number_of_epochs):\n","        if labels[i,0] == class_1 or labels[i,0] == class_2:\n","            dataset[index,:,:] = data[i,:,:]\n","            Final_labels[index,0] = labels[i,0]\n","            index = index + 1\n","        else:\n","            continue\n","            \n","    return dataset, Final_labels"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def feature_extractor(dataset, labels, number_of_bands, test_data):\n","\n","    low_cutoff = 0\n","    \n","    for b in range(number_of_bands):\n","        logging.getLogger('mne').setLevel(logging.WARNING)\n","        low_cutoff += 4\n","        data = dataset.copy()\n","        data_test = test_data.copy()\n","\n","        # empty_field_count = np.count_nonzero(data == 0)\n","        # print(\"Number of empty fields in data:\", empty_field_count,\"data shape\",data.shape)   \n","\n","        # empty_field_count = np.count_nonzero(data_test == 0)\n","        # print(\"Number of empty fields in data_test:\", empty_field_count,\"data_test shape\",data_test.shape)    \n","\n","        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","\n","        # empty_field_count = np.count_nonzero(filtered_data == 0)\n","        # print(\"Number of empty fields in filtered_data:\", empty_field_count,\"filtered_data shape\",filtered_data.shape)   \n","\n","        # empty_field_count = np.count_nonzero(filtered_data_test == 0)\n","        # print(\"Number of empty fields in filtered_data_test:\", empty_field_count,\"filtered_data_test shape\",filtered_data_test.shape)  \n","\n","\n","\n","        # # Reshape data to (samples, features)\n","        # num_samples_train, num_rows, num_cols = filtered_data.shape\n","        # num_samples_test, _, _ = filtered_data_test.shape\n","        # flattened_train_data = filtered_data.reshape(num_samples_train, -1)\n","        # flattened_test_data = filtered_data_test.reshape(num_samples_test, -1)\n","        \n","        # # Apply PCA\n","        # pca = PCA(n_components=10)\n","        # filtered_data_pca = pca.fit_transform(flattened_train_data)\n","        # filtered_data_test_pca = pca.transform(flattened_test_data)      \n","        # filtered_data_pca_3d = filtered_data_pca.reshape(num_samples_train, num_rows, num_cols)\n","        # filtered_data_test_pca_3d = filtered_data_test_pca.reshape(num_samples_test, num_rows, num_cols)\n","\n","        # filtered_data = filtered_data_pca_3d\n","        # filtered_data_test = filtered_data_test_pca_3d\n","\n","\n","        #PCA\n","        # from mne.decoding import UnsupervisedSpatialFilter\n","        # from sklearn.decomposition import PCA, FastICA\n","\n","        # pca = UnsupervisedSpatialFilter(PCA(64), average=False)\n","        # pca_fit = pca.fit(filtered_data)\n","        # filtered_data = pca_fit.transform(filtered_data)\n","        # filtered_data_test = pca_fit.transform(filtered_data_test)\n","        # train_feats = filtered_data\n","        # test_feats = filtered_data_test\n","\n","        # filtered_data = data\n","        # filtered_data_test = data_test\n","        \n","        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test)\n","        if b == 0:\n","            train_features = train_feats\n","            test_features = test_feats\n","        else:\n","            train_features = np.concatenate((train_features, train_feats), axis = 1)\n","            test_features = np.concatenate((test_features, test_feats), axis = 1)\n","    \n","    return train_features, test_features"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def feature_selector(train_features, labels, number_of_selected_features):\n","    X = pd.DataFrame(train_features)\n","    y = pd.DataFrame(labels)\n","    K = number_of_selected_features\n","    \n","    df = pd.concat([y,X], axis = 1)\n","    df.columns = df.columns.astype(str)\n","        \n","    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n","    return selected_features"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def data_reader(path,p_num,block_list):\n","    data_dict = {}\n","    for b_num in block_list:\n","        print(b_num)\n","        mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","        df = pd.DataFrame(mat['Data'])\n","        # ddf = dd.from_pandas(df, npartitions=10)\n","        data_dict[b_num] = df\n","    return data_dict\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def interpolate_zeros(matrix):\n","    # Create a copy of the matrix to work with\n","    matrix_copy = matrix.copy()\n","\n","    # Create indices meshgrid for non-zero elements\n","    nonzero_indices = np.argwhere(matrix != 0)\n","    nonzero_rows, nonzero_cols = nonzero_indices[:, 0], nonzero_indices[:, 1]\n","\n","    # Create interpolation function for rows and columns separately\n","    f_rows = interpolate.interp2d(nonzero_cols, nonzero_rows, matrix[nonzero_rows, nonzero_cols], kind='linear')\n","    f_cols = interpolate.interp2d(nonzero_cols, nonzero_rows, matrix[nonzero_rows, nonzero_cols].T, kind='linear')\n","\n","    # Find zero indices\n","    zero_indices = np.argwhere(matrix == 0)\n","\n","    for idx in zero_indices:\n","        row, col = idx\n","        # Interpolate zero values using the interpolation functions\n","        matrix_copy[row, col] = (f_rows(col, row) + f_cols(row, col)) / 2\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def extra_samples_counter(df,class_1,class_2):\n","    x=0\n","    i=0\n","    sampleList = []\n","    while i<len(df):\n","        if (df.iloc[i,64]==class_1):\n","            x+=1\n","        else:\n","            i-=1\n","            sampleList.append(x)\n","            x=0\n","            class_1,class_2 = class_2,class_1\n","        i+=1\n","    sampleList.append(x)\n","    print(sampleList)\n","    "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def extra_samples_block_counter(df,trial_order,b_num):\n","\n","    df.drop(df[df.iloc[:,64].isin(['Begin', 'End'])].index, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","    print('hi')\n","    \n","    df['group'] = (df.iloc[:,64] != df.iloc[:,64].shift(1)).cumsum()\n","    # group_counts_Tongue = df[df.iloc[:,64] == 'Tongue'].groupby('group').size()\n","    # group_counts_Feet = df[df.iloc[:,64] == 'Feet'].groupby('group').size()\n","    # group_counts_Hand = df[df.iloc[:,64] == 'Hand'].groupby('group').size()\n","    # group_counts_Mis = df[df.iloc[:,64] == 'Mis'].groupby('group').size()\n","    # group_counts_Rest = df[df.iloc[:,64] == 'Rest'].groupby('group').size()\n","\n","    \n","    group_counts_Rest = df[df.iloc[:,64] == 'Rest'].groupby('group').size()\n","    with open('sampleList.txt', 'a') as file:\n","        file.write(f'block {b_num+1} '+'\\n')\n","        for j in range (len(trial_order)):\n","            print(trial_order[j])\n","            trial_num = j\n","            task_times,rest_times = get_task_rest_times(b_num)\n","            trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","            trial_samples = [item*SAMPLING_RATE for item in trial_times]\n","            group_counts_task = df[df.iloc[:,64] == trial_order[j]].groupby('group').size()\n","            sampleList = []\n","            for i in range(4):\n","                task = group_counts_task.iloc[i]\n","                rest = group_counts_Rest.iloc[4*j+i]\n","                sampleList.append(task)\n","                sampleList.append(rest)\n","            # extra_samples = [x-y for x,y in zip(sampleList,trial_samples)]\n","            file.write(', '.join(map(str, sampleList)) + f' trial={trial_order[j]} '+'\\n')\n","            print(sampleList)\n","        file.write('\\n\\n')\n","\n","\n","    # print(group_counts_Tongue)\n","    # print(group_counts_Feet)\n","    # print(group_counts_Hand)\n","    # print(group_counts_Mis)\n","    # print(group_counts_Rest)\n","\n","    # print(group_counts_b.index[0])\n","    # print(group_counts_b.iloc[0])\n","    # print(group_counts)\n","\n","\n","    # for j in range(len(trial_order)):\n","    #     print(j)\n","    #     class_2 = 'Rest'\n","    #     class_1 = trial_order[j]\n","    #     sampleList = []\n","    #     x=0\n","    #     i=0\n","\n","\n","    #     while i<len(df):\n","    #         print(i)\n","    #         if (df.iloc[i,64]!=class_1):\n","    #             x+=1\n","    #         else:\n","    #             i-=1\n","    #             sampleList.append(x)\n","    #             x=0\n","    #             class_1,class_2 = class_2,class_1\n","    #         i+=1\n","    #     sampleList.append(x)\n","    #     df.drop(df.index[0:sum(sampleList)], inplace=True)\n","    #     df.reset_index(drop=True, inplace=True)\n","    #     print(sampleList)\n","        # with open('sampleList.txt', 'w') as file:\n","        #     # for item in sampleList:\n","        #     file.write(f\"{sampleList}\\n\")\n","    "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["\n","def data_cleaner(df,class_1,class_2,tasks_time):\n","    # extra_samples_counter(df,class_1,class_2)\n","    # sys.exit() \n","    class_x = class_1\n","    class_y = class_2\n","    new_df = pd.DataFrame()\n","    trial_df = df.copy() \n","    print(tasks_time)\n","    for i in range(len(tasks_time)):\n","        sample_point = tasks_time[i]*SAMPLING_RATE\n","        if(trial_df.iloc[sample_point+1,64] == class_x ):\n","            if(i==len(tasks_time)-1):\n","                temp_df = trial_df.iloc[:sample_point,:]\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","            else:    \n","                temp_df = trial_df.iloc[:sample_point,:]\n","                next_task_idx = trial_df[trial_df.iloc[:, 64] == class_y].index\n","                trial_df.drop(trial_df.index[0:next_task_idx[0]], inplace=True)\n","                trial_df.reset_index(drop=True, inplace=True)\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","                class_x,class_y = class_y,class_x\n","\n","    return new_df"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def class_seperator(cleaned_df,class_1,class_2):\n","    # df = cleaned_df.sort_values(by=cleaned_df.columns[64]).reset_index(drop=True)\n","    # print(seperated_df.head(14003))\n","    # print(cleaned_df.head(5003))\n","\n","    df = cleaned_df\n","    sorting_order = {class_1: 0, class_2: 1}\n","\n","    df['sorting_order'] = df.iloc[:, 64].map(sorting_order)\n","    df.sort_values(by=['sorting_order', df.columns[64]], inplace=True)\n","    df.drop('sorting_order', axis=1, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def shuffler(dataset,labels):\n","    print(dataset.shape)\n","    print(labels.shape)\n","    np.random.seed(42)\n","    indices = np.random.permutation(len(dataset))\n","    shuffled_dataset = dataset[indices]\n","    shuffled_labels = labels[indices]\n","    return shuffled_dataset,shuffled_labels\n","    "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def data_label_attacher(cleaned_df,class_1,class_2,random_flag,class_seperator_flag):\n","\n","    # new_df_ = cleaned_df.copy()\n","    # new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","    # X = new_df_.to_numpy()\n","    # X = np.transpose(X)\n","    # number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n","    # number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n","\n","    \n","    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    # labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    \n","    #Initialization\n","    if class_seperator_flag:\n","        seperated_class_df = class_seperator(cleaned_df,class_1,class_2)\n","        new_df_ = seperated_class_df.copy()\n","        new_df_.drop(seperated_class_df.columns[-1], axis=1, inplace=True)\n","        X = new_df_.to_numpy()\n","        X = np.transpose(X)\n","        empty_field_count = np.count_nonzero(X == 0)\n","        print(\"Number of empty fields in X:\", empty_field_count)\n","        # zero_indices = np.where(X == 0)\n","        # print(\"befor filling\",len(zero_indices[0]))\n","        # X[zero_indices] += 0.001\n","        # zero_indices = np.where(X == 0)\n","        # print(\"after filling\",len(zero_indices[0]))\n","        number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n","        print(number_of_epochs)\n","    else :  \n","        new_df_ = cleaned_df.copy()\n","        new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","        X = new_df_.to_numpy()\n","        X = np.transpose(X)\n","        empty_field_count = np.count_nonzero(X == 0)\n","        print(\"Number of empty fields in X:\", empty_field_count)\n","        # zero_indices = np.where(X == 0)\n","        # print(\"befor filling\",len(zero_indices[0]))\n","        # X[zero_indices] += 0.001\n","        # zero_indices = np.where(X == 0)\n","        # print(\"after filling\",len(zero_indices[0]))\n","\n","        number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n","\n","    dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","    if class_seperator_flag:\n","        i = 0  \n","        startIdx = i * WINDOW_SAMPLE_LENGTH\n","        endIdx = (i+1) * WINDOW_SAMPLE_LENGTH \n","        while(endIdx<=int(len(new_df_))/2):\n","            slice_X = X[:, startIdx:endIdx]\n","\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","\n","            dataset[i, :, :] = slice_X\n","            labels[i,0] = 0\n","            # if (seperated_class_df.iloc[startIdx, 64] == class_1):\n","            #     labels[i,0] = 0\n","            # elif(seperated_class_df.iloc[startIdx, 64] == class_2):\n","            #     labels[i,0] = 1\n","            # else:\n","            #     labels[i,0] = 2\n","            startIdx+=TR_SLIDING_POINTS\n","            endIdx+=TR_SLIDING_POINTS\n","            i+=1\n","        # print(int(len(new_df_))/2,\"len\")    \n","        # print(endIdx,\"endIdx\")    \n","        # print(seperated_class_df.iloc[endIdx-2:endIdx+2,64])\n","       \n","        j = i\n","        \n","        startIdx = endIdx-TR_SLIDING_POINTS\n","        endIdx = startIdx+WINDOW_SAMPLE_LENGTH\n","        print(j, \"j is this\")\n","        while(endIdx<=int(len(new_df_))):\n","            slice_X = X[:, startIdx:endIdx]\n","\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","\n","            dataset[j, :, :] = slice_X\n","            labels[j,0] = 1\n","            # if (cleaned_df.iloc[startIdx, 64] == class_1):\n","            #     labels[j,0] = 0\n","            # elif(cleaned_df.iloc[startIdx, 64] == class_2):\n","            #     labels[j,0] = 1\n","            # else:\n","            #     labels[j,0] = 2\n","            startIdx+=TR_SLIDING_POINTS\n","            endIdx+=TR_SLIDING_POINTS\n","            j+=1\n","        print(j, \"j is this\")\n","        # dataset,labels = shuffler(dataset,labels)\n","\n","    else:\n","        i = 0  \n","        start_idx = i * WINDOW_SAMPLE_LENGTH\n","        end_idx = (i+1) * WINDOW_SAMPLE_LENGTH \n","        while (end_idx<=int(len(new_df_))):\n","            slice_X = X[:, start_idx:end_idx]\n","\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","            \n","            dataset[i, :, :] = slice_X\n","            if (cleaned_df.iloc[start_idx, 64] == class_1):\n","                labels[i,0] = 0\n","            elif(cleaned_df.iloc[start_idx, 64] == class_2):\n","                labels[i,0] = 1\n","            else:\n","                labels[i,0] = 2\n","            start_idx+=SLIDING_POINTS\n","            end_idx+=SLIDING_POINTS\n","            i+=1\n","        # dataset,labels = shuffler(dataset,labels)\n","\n","\n","\n","\n","\n","\n","\n","    #For training and test purpose\n","    # if random_flag:\n","    #     randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n","    # else:\n","    #     randomlist = list(range(number_of_epochs))\n","    #Labeling the data\n","\n","\n","\n","    # for i in range(number_of_epochs):\n","    #     start_idx = randomlist[i] * WINDOW_SAMPLE_LENGTH + SLIDING_POINTS\n","    #     end_idx = (randomlist[i] + 1) * WINDOW_SAMPLE_LENGTH\n","    #     slice_X = X[:, start_idx:end_idx]\n","\n","    #     # hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hamming_window\n","\n","    #     # hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hanning_window\n","\n","    #     # blackman_window = blackman(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= blackman_window\n","\n","    #     # kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= kaiser_window\n","\n","    #     # gaussian_window = gaussian(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= gaussian_window\n","\n","\n","    #     dataset[i, :, :] = slice_X\n","    #     if (cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_1):\n","    #         labels[i,0] = 0\n","    #     elif(cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_2):\n","    #         labels[i,0] = 1\n","    #     else:\n","    #         labels[i,0] = 2\n","    \n","    # empty_field_count = np.count_nonzero(dataset == 0)\n","    # print(\"Number of empty fields in dataset:\", empty_field_count,\"dataset shape\",dataset.shape)\n","    return dataset,labels\n","\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def trial_cutter(data, class_1):\n","    df = data.copy()\n","    Begin_trigger = \"Begin\" + \"_\" + class_1\n","    End_trigger = \"End\" + \"_\" + class_1\n","    Begin_idx = df[df.iloc[:, 64] == Begin_trigger].index\n","    End_idx = df[df.iloc[:, 64] == End_trigger].index\n","    trial_df = df.iloc[Begin_idx[0]+1:End_idx[0],:]\n","    trial_df.reset_index(drop=True, inplace=True)\n","    trial_df.head()\n","    return trial_df"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def Begin_End_trigger_modifier(data):\n","    df = data.copy()\n","    Begin_indexes = df[df.iloc[:, 64] == 'Begin'].index\n","    End_indexes = df[df.iloc[:, 64] == 'End'].index\n","    if(len(Begin_indexes)==len(End_indexes)):\n","        for i in range(len(Begin_indexes)):\n","            index = Begin_indexes[i]+1\n","            val = df.iloc[index,64]\n","            df.iloc[Begin_indexes[i],64] = \"Begin\" + \"_\" + str(val)\n","            df.iloc[End_indexes[i],64]   =  \"End\" + \"_\" + str(val)\n","    else:\n","        print(\"Trigger seinding Exception\")\n","    \n","    return df"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def preprocessor(data_,class_1,class_2,tasks_time,set_type,overlap):\n","    CLASS_1 = class_1\n","    CLASS_2 = class_2\n","    df = data_.copy()\n","    modified_df = Begin_End_trigger_modifier(df)\n","    trial_df = trial_cutter(modified_df,CLASS_1)\n","    print(trial_df.shape,\"trial_df\")\n","    cleaned_df = data_cleaner(trial_df,CLASS_1,CLASS_2,tasks_time)\n","    print(cleaned_df.shape,\"cleaned_df\")\n","\n","    if set_type ==\"TRAIN\":\n","        random_flag = True\n","    elif set_type ==\"TEST\":\n","        random_flag = False\n","    else:\n","        print(\"Error in set type\")\n","\n","  \n","    final_data, final_labels = data_label_attacher(cleaned_df,CLASS_1,CLASS_2,random_flag,overlap)\n","    print(final_data.shape,\"final_data shape\")\n","    print(final_labels.shape,\"final_labels shape\")\n","    \n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def trials_set_builder(data_dict,blocks_set,set_label,class_1,class_2,overlap):\n","    counter = 0\n","\n","    for b_num in blocks_set:\n","        trial_num = trial_order[b_num].index(class_1)\n","        task_times,rest_times = get_task_rest_times(b_num)\n","        print(task_times[trial_num],rest_times[trial_num])\n","        trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","        print(trial_times)\n","        data = data_dict[b_num]\n","        df = data.copy()\n","        # last_column = df.pop(df.columns[-1])\n","        # df.drop(df.columns[-1], axis=1, inplace=True)\n","        # eeg_data = df.to_numpy().T  # Transpose to have channels in columns\n","\n","        # channel_names = [f'Ch{i+1}' for i in range(63)]\n","\n","        # # Create MNE-Python RawArray object\n","        # info = mne.create_info(ch_names=channel_names, sfreq=sampling_freq, ch_types='eeg')\n","        # raw = mne.io.RawArray(eeg_data, info)\n","\n","        # # Apply ICA\n","        # ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n","        # ica.fit(raw)\n","        # ica_components = ica.get_components()\n","\n","        # # Convert the ICA components to a DataFrame\n","        # df2 = pd.DataFrame(data=ica_components.T, columns=channel_names)\n","        # df2 = df2.assign(LastColumn=last_column)\n","        # # df = data.copy(deep=False)\n","        dataset,labels = preprocessor(df,class_1,class_2,trial_times,set_label,overlap)\n","        # print(dataset.shape)\n","\n","        if counter == 0 :\n","            final_data = dataset\n","            final_labels = labels\n","            print(\"Before concatenation - final_data shape:\", final_data.shape, \"dataset shape:\", dataset.shape)\n","        else:\n","            final_data = np.vstack((final_data, dataset))\n","            final_labels = np.vstack((final_labels, labels))\n","            print(\"After concatenation - final_data shape:\", final_data.shape, \"final_labels shape:\", final_labels.shape)\n","\n","        counter+=1 \n","    # empty_field_count = np.count_nonzero(final_data == 0)\n","    # print(\"Number of empty fields in final_data:\", empty_field_count,\"final_data shape\",final_data.shape)\n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reading P8\n","0\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../../Participants/P8/P8B0.mat'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Participants/P8/P8B0.mat'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p_num \u001b[38;5;129;01min\u001b[39;00m p_num_list:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreading P\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdata_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Participants/P\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     data_dicts_list\u001b[38;5;241m.\u001b[39mappend(data_dict)\n","Cell \u001b[0;32mIn[58], line 5\u001b[0m, in \u001b[0;36mdata_reader\u001b[0;34m(path, p_num, block_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_num \u001b[38;5;129;01min\u001b[39;00m block_list:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(b_num)\n\u001b[0;32m----> 5\u001b[0m     mat \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp_num\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb_num\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchars_as_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_as_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_compressed_data_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# ddf = dd.from_pandas(df, npartitions=10)\u001b[39;00m\n","File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n","File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n","File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Participants/P8/P8B0.mat'"]}],"source":["block_list = [0,1,2,3,4,5,6]\n","p_num_list = [8]\n","data_dicts_list = []\n","for p_num in p_num_list:\n","    print(f'reading P{p_num}')\n","    data_dict = data_reader(f'../../Participants/P{p_num}/', p_num, block_list)\n","    data_dicts_list.append(data_dict)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reading P9\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n"]}],"source":["block_list = [0,1,2,3,4,5,6]\n","p_num_list = [9]\n","data_dicts_list = []\n","for p_num in p_num_list:\n","    print(f'reading P{p_num}')\n","    data_dict = data_reader(f'/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/P{p_num}/',p_num,block_list)\n","    data_dicts_list.append(data_dict)\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"2","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Classification/Classifier.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgraham.computecanada.ca/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Classification/Classifier.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(data_dicts_list[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39mshape)\n","\u001b[0;31mKeyError\u001b[0m: 2"]}],"source":["print(data_dicts_list[0][2].shape)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# #Frame Maker\n","PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n","df = pd.read_csv(PATH+'frame.csv')\n","p_num_list = [3]\n","for p_num in p_num_list:\n","    df.to_csv(PATH+'P'+str(p_num)+'.csv',index=False)\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[20, 12, 8, 16] [20, 12, 8, 16]\n","[20, 20, 12, 12, 8, 8, 16, 16]\n","(28483, 65) trial_df\n","[20, 20, 12, 12, 8, 8, 16, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","Before concatenation - final_data shape: (54, 64, 1000) dataset shape: (54, 64, 1000)\n","[8, 20, 16, 12] [20, 16, 8, 12]\n","[8, 20, 20, 16, 16, 8, 12, 12]\n","(28472, 65) trial_df\n","[8, 20, 20, 16, 16, 8, 12, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (108, 64, 1000) final_labels shape: (108, 1)\n","[16, 8, 12, 20] [8, 20, 16, 12]\n","[16, 8, 8, 20, 12, 16, 20, 12]\n","(28472, 65) trial_df\n","[16, 8, 8, 20, 12, 16, 20, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (162, 64, 1000) final_labels shape: (162, 1)\n","[20, 16, 8, 12] [12, 20, 16, 8]\n","[20, 12, 16, 20, 8, 16, 12, 8]\n","(28934, 65) trial_df\n","[20, 12, 16, 20, 8, 16, 12, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (216, 64, 1000) final_labels shape: (216, 1)\n","[20, 8, 12, 16] [12, 16, 8, 20]\n","[20, 12, 8, 16, 12, 8, 16, 20]\n","(28617, 65) trial_df\n","[20, 12, 8, 16, 12, 8, 16, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (270, 64, 1000) final_labels shape: (270, 1)\n","[20, 16, 12, 8] [16, 8, 20, 12]\n","[20, 16, 16, 8, 12, 20, 8, 12]\n","(28564, 65) trial_df\n","[20, 16, 16, 8, 12, 20, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[16, 8, 12, 20] [16, 8, 12, 20]\n","[16, 16, 8, 8, 12, 12, 20, 20]\n","(28559, 65) trial_df\n","[16, 16, 8, 8, 12, 12, 20, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","(270, 64, 1000) (270, 1) train shape\n","(56, 64, 1000) (56, 1) test shape\n","Number of NaN values: 0\n","Number of empty fields: 169\n","Number of NaN values: 0\n","Number of empty fields: 81\n","Number of NaN values: 0\n","Number of empty fields: 633\n","Number of NaN values: 0\n","Number of empty fields: 696\n","Number of NaN values: 0\n","Number of empty fields: 0\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 5\n","Number of NaN values: 0\n","Number of empty fields: 1581\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency, max-relevance, and min-redundancy,\"\n","      Hanchuan Peng, Fuhui Long, and Chris Ding, \n","      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n","      Vol. 27, No. 8, pp.1226-1238, 2005.\n","\n","\n","*** MaxRel features ***\n","Order \t Fea \t Name \t Score\n","1 \t 13 \t 12 \t 0.704\n","2 \t 5 \t 4 \t 0.517\n","3 \t 22 \t 21 \t 0.504\n","4 \t 12 \t 11 \t 0.377\n","5 \t 14 \t 13 \t 0.366\n","6 \t 44 \t 43 \t 0.350\n","7 \t 79 \t 78 \t 0.291\n","8 \t 8 \t 7 \t 0.271\n","9 \t 33 \t 32 \t 0.255\n","10 \t 6 \t 5 \t 0.228\n","\n","*** mRMR features *** \n","Order \t Fea \t Name \t Score\n","1 \t 13 \t 12 \t 0.704\n","2 \t 7 \t 6 \t 0.067\n","3 \t 5 \t 4 \t 0.167\n","4 \t 12 \t 11 \t 0.122\n","5 \t 79 \t 78 \t 0.124\n","6 \t 14 \t 13 \t 0.116\n","7 \t 22 \t 21 \t 0.090\n","8 \t 8 \t 7 \t 0.077\n","9 \t 18 \t 17 \t 0.071\n","10 \t 44 \t 43 \t 0.078\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency, max-relevance, and min-redundancy,\"\n","      Hanchuan Peng, Fuhui Long, and Chris Ding, \n","      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n","      Vol. 27, No. 8, pp.1226-1238, 2005.\n","\n","\n","*** MaxRel features ***\n","Order \t Fea \t Name \t Score\n","1 \t 42 \t 41 \t 0.622\n","2 \t 21 \t 20 \t 0.569\n","3 \t 11 \t 10 \t 0.536\n","4 \t 32 \t 31 \t 0.414\n","5 \t 52 \t 51 \t 0.391\n","6 \t 53 \t 52 \t 0.366\n","7 \t 6 \t 5 \t 0.358\n","8 \t 2 \t 1 \t 0.306\n","9 \t 55 \t 54 \t 0.265\n","10 \t 5 \t 4 \t 0.246\n","\n","*** mRMR features *** \n","Order \t Fea \t Name \t Score\n","1 \t 42 \t 41 \t 0.622\n","2 \t 6 \t 5 \t 0.196\n","3 \t 21 \t 20 \t 0.236\n","4 \t 2 \t 1 \t 0.153\n","5 \t 52 \t 51 \t 0.172\n","6 \t 32 \t 31 \t 0.158\n","7 \t 53 \t 52 \t 0.150\n","8 \t 11 \t 10 \t 0.132\n","9 \t 17 \t 16 \t 0.119\n","10 \t 55 \t 54 \t 0.121\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency, max-relevance, and min-redundancy,\"\n","      Hanchuan Peng, Fuhui Long, and Chris Ding, \n","      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n","      Vol. 27, No. 8, pp.1226-1238, 2005.\n","\n","\n","*** MaxRel features ***\n","Order \t Fea \t Name \t Score\n","1 \t 54 \t 53 \t 0.314\n","2 \t 44 \t 43 \t 0.309\n","3 \t 64 \t 63 \t 0.291\n","4 \t 2 \t 1 \t 0.287\n","5 \t 47 \t 46 \t 0.278\n","6 \t 87 \t 86 \t 0.272\n","7 \t 8 \t 7 \t 0.272\n","8 \t 74 \t 73 \t 0.265\n","9 \t 77 \t 76 \t 0.247\n","10 \t 82 \t 81 \t 0.246\n","\n","*** mRMR features *** \n","Order \t Fea \t Name \t Score\n","1 \t 54 \t 53 \t 0.314\n","2 \t 5 \t 4 \t 0.190\n","3 \t 7 \t 6 \t 0.153\n","4 \t 8 \t 7 \t 0.157\n","5 \t 47 \t 46 \t 0.148\n","6 \t 2 \t 1 \t 0.133\n","7 \t 56 \t 55 \t 0.102\n","8 \t 19 \t 18 \t 0.106\n","9 \t 44 \t 43 \t 0.101\n","10 \t 14 \t 13 \t 0.097\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency, max-relevance, and min-redundancy,\"\n","      Hanchuan Peng, Fuhui Long, and Chris Ding, \n","      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n","      Vol. 27, No. 8, pp.1226-1238, 2005.\n","\n","\n","*** MaxRel features ***\n","Order \t Fea \t Name \t Score\n","1 \t 74 \t 73 \t 0.462\n","2 \t 82 \t 81 \t 0.414\n","3 \t 84 \t 83 \t 0.396\n","4 \t 63 \t 62 \t 0.370\n","5 \t 64 \t 63 \t 0.338\n","6 \t 54 \t 53 \t 0.338\n","7 \t 86 \t 85 \t 0.332\n","8 \t 76 \t 75 \t 0.323\n","9 \t 65 \t 64 \t 0.313\n","10 \t 83 \t 82 \t 0.283\n","\n","*** mRMR features *** \n","Order \t Fea \t Name \t Score\n","1 \t 74 \t 73 \t 0.462\n","2 \t 28 \t 27 \t 0.108\n","3 \t 5 \t 4 \t 0.131\n","4 \t 14 \t 13 \t 0.115\n","5 \t 76 \t 75 \t 0.112\n","6 \t 84 \t 83 \t 0.111\n","7 \t 9 \t 8 \t 0.086\n","8 \t 19 \t 18 \t 0.098\n","9 \t 54 \t 53 \t 0.102\n","10 \t 26 \t 25 \t 0.090\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency[1.0] train\n","[0.9285714285714286] test\n","[16, 12, 20, 8] [16, 20, 8, 12]\n","[16, 16, 12, 20, 20, 8, 8, 12]\n","(28478, 65) trial_df\n","[16, 16, 12, 20, 20, 8, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","Before concatenation - final_data shape: (54, 64, 1000) dataset shape: (54, 64, 1000)\n","[12, 8, 20, 16] [16, 12, 8, 20]\n","[12, 16, 8, 12, 20, 8, 16, 20]\n","(28429, 65) trial_df\n","[12, 16, 8, 12, 20, 8, 16, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (108, 64, 1000) final_labels shape: (108, 1)\n","[20, 16, 12, 8] [12, 8, 20, 16]\n","[20, 12, 16, 8, 12, 20, 8, 16]\n","(28486, 65) trial_df\n","[20, 12, 16, 8, 12, 20, 8, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (162, 64, 1000) final_labels shape: (162, 1)\n","[20, 12, 8, 16] [20, 12, 8, 16]\n","[20, 20, 12, 12, 8, 8, 16, 16]\n","(28782, 65) trial_df\n","[20, 20, 12, 12, 8, 8, 16, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (216, 64, 1000) final_labels shape: (216, 1)\n","[12, 16, 8, 20] [16, 20, 12, 8]\n","[12, 16, 16, 20, 8, 12, 20, 8]\n","(28885, 65) trial_df\n","[12, 16, 16, 20, 8, 12, 20, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (270, 64, 1000) final_labels shape: (270, 1)\n","[16, 12, 8, 20] [12, 8, 16, 20]\n","[16, 12, 12, 8, 8, 16, 20, 20]\n","(28588, 65) trial_df\n","[16, 12, 12, 8, 8, 16, 20, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[16, 20, 12, 8] [8, 16, 20, 12]\n","[16, 8, 20, 16, 12, 20, 8, 12]\n","(28532, 65) trial_df\n","[16, 8, 20, 16, 12, 20, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","(270, 64, 1000) (270, 1) train shape\n","(56, 64, 1000) (56, 1) test shape\n","Number of NaN values: 0\n","Number of empty fields: 183\n","Number of NaN values: 0\n","Number of empty fields: 71\n","Number of NaN values: 0\n","Number of empty fields: 694\n","Number of NaN values: 0\n","Number of empty fields: 662\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 0\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 2\n","Number of NaN values: 0\n","Number of empty fields: 1662\n","[1.0] train\n","[0.875] test\n","[12, 16, 20, 8] [20, 8, 16, 12]\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28525, 65) trial_df\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","Before concatenation - final_data shape: (54, 64, 1000) dataset shape: (54, 64, 1000)\n","[8, 12, 20, 16] [12, 16, 20, 8]\n","[8, 12, 12, 16, 20, 20, 16, 8]\n","(28527, 65) trial_df\n","[8, 12, 12, 16, 20, 20, 16, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (108, 64, 1000) final_labels shape: (108, 1)\n","[12, 20, 8, 16] [16, 12, 20, 8]\n","[12, 16, 20, 12, 8, 20, 16, 8]\n","(28529, 65) trial_df\n","[12, 16, 20, 12, 8, 20, 16, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (162, 64, 1000) final_labels shape: (162, 1)\n","[12, 16, 20, 8] [20, 8, 16, 12]\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28478, 65) trial_df\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (216, 64, 1000) final_labels shape: (216, 1)\n","[8, 20, 12, 16] [20, 8, 12, 16]\n","[8, 20, 20, 8, 12, 12, 16, 16]\n","(28708, 65) trial_df\n","[8, 20, 20, 8, 12, 12, 16, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (270, 64, 1000) final_labels shape: (270, 1)\n","[8, 16, 20, 12] [20, 12, 16, 8]\n","[8, 20, 16, 12, 20, 16, 12, 8]\n","(28598, 65) trial_df\n","[8, 20, 16, 12, 20, 16, 12, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[20, 8, 16, 12] [12, 20, 8, 16]\n","[20, 12, 8, 20, 16, 8, 12, 16]\n","(28541, 65) trial_df\n","[20, 12, 8, 20, 16, 8, 12, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","(270, 64, 1000) (270, 1) train shape\n","(56, 64, 1000) (56, 1) test shape\n","Number of NaN values: 0\n","Number of empty fields: 157\n","Number of NaN values: 0\n","Number of empty fields: 66\n","Number of NaN values: 0\n","Number of empty fields: 671\n","Number of NaN values: 0\n","Number of empty fields: 668\n","Number of NaN values: 0\n","Number of empty fields: 0\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 0\n","Number of NaN values: 0\n","Number of empty fields: 2\n","Number of NaN values: 0\n","Number of empty fields: 1581\n","[1.0] train\n","[0.7678571428571429] test\n","[20, 16, 8, 12] [12, 20, 16, 8]\n","[20, 12, 16, 20, 8, 16, 12, 8]\n","(28535, 65) trial_df\n","[20, 12, 16, 20, 8, 16, 12, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","Before concatenation - final_data shape: (54, 64, 1000) dataset shape: (54, 64, 1000)\n","[16, 20, 8, 12] [8, 20, 12, 16]\n","[16, 8, 20, 20, 8, 12, 12, 16]\n","(28483, 65) trial_df\n","[16, 8, 20, 20, 8, 12, 12, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (108, 64, 1000) final_labels shape: (108, 1)\n","[8, 12, 16, 20] [8, 12, 20, 16]\n","[8, 8, 12, 12, 16, 20, 20, 16]\n","(28493, 65) trial_df\n","[8, 8, 12, 12, 16, 20, 20, 16]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (162, 64, 1000) final_labels shape: (162, 1)\n","[16, 12, 20, 8] [16, 20, 8, 12]\n","[16, 16, 12, 20, 20, 8, 8, 12]\n","(28774, 65) trial_df\n","[16, 16, 12, 20, 20, 8, 8, 12]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (216, 64, 1000) final_labels shape: (216, 1)\n","[16, 8, 20, 12] [8, 12, 16, 20]\n","[16, 8, 8, 12, 20, 16, 12, 20]\n","(28628, 65) trial_df\n","[16, 8, 8, 12, 20, 16, 12, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","54\n","27 j is this\n","54 j is this\n","(54, 64, 1000) final_data shape\n","(54, 1) final_labels shape\n","After concatenation - final_data shape: (270, 64, 1000) final_labels shape: (270, 1)\n","[12, 8, 16, 20] [8, 16, 12, 20]\n","[12, 8, 8, 16, 16, 12, 20, 20]\n","(28542, 65) trial_df\n","[12, 8, 8, 16, 16, 12, 20, 20]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[8, 16, 12, 20] [20, 16, 12, 8]\n","[8, 20, 16, 16, 12, 12, 20, 8]\n","(28632, 65) trial_df\n","[8, 20, 16, 16, 12, 12, 20, 8]\n","(28000, 65) cleaned_df\n","Number of empty fields in X: 0\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","(270, 64, 1000) (270, 1) train shape\n","(56, 64, 1000) (56, 1) test shape\n","Number of NaN values: 0\n","Number of empty fields: 161\n","Number of NaN values: 0\n","Number of empty fields: 89\n","Number of NaN values: 0\n","Number of empty fields: 670\n","Number of NaN values: 0\n","Number of empty fields: 668\n","Number of NaN values: 0\n","Number of empty fields: 1\n","Number of NaN values: 0\n","Number of empty fields: 3\n","Number of NaN values: 0\n","Number of empty fields: 0\n","Number of NaN values: 0\n","Number of empty fields: 2\n","Number of NaN values: 0\n","Number of empty fields: 1687\n","[1.0] train\n","[0.7678571428571429] test\n"]}],"source":["PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n","class_1_list = ['Hand','Feet','Tongue','Mis']\n","class_2 = 'Rest'\n","p_num_list = [9]\n","train_blocks_set = [0,1,2,3,4]\n","test_blocks_set = [5,6]\n","Train_overlap = 0\n","Test_overlap = 0\n","\n","\n","i = 0\n","for p_num in p_num_list:\n","    import time\n","    start_time = time.time()\n","    for class_1 in class_1_list:\n","        X_tr, Y_tr = trials_set_builder(data_dicts_list[i],train_blocks_set,'TRAIN',class_1,class_2,True)\n","        X_te, Y_te = trials_set_builder(data_dicts_list[i],test_blocks_set,'TEST',class_1,class_2,False)\n","\n","        print(X_tr.shape,Y_tr.shape,\"train shape\")\n","        print(X_te.shape,Y_te.shape,\"test shape\")\n","\n","        [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te)\n","        selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n","\n","        train_acc_list = []\n","        test_acc_list = []\n","\n","        clf = XGBClassifier()\n","        for r in range(1):\n","            clf.fit(train_features[:, selected_features], Y_tr[:,0])\n","\n","            y_pr_te = clf.predict(test_features[:, selected_features])\n","            y_pr_tr = clf.predict(train_features[:,selected_features])\n","\n","            accuracy_te = accuracy_score(Y_te, y_pr_te)\n","            test_acc_list.append(accuracy_te)\n","\n","            accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n","            train_acc_list.append(accuracy_tr)\n","\n","\n","\n","        end_time = time.time()\n","        running_time = end_time-start_time\n","        participant = p_num\n","        class1 = class_1\n","        class2 = class_2\n","        running_time = running_time\n","        test_acc = np.average(test_acc_list)\n","        train_acc = np.average(train_acc_list)\n","        test_size = X_te.shape\n","        train_size = X_tr.shape\n","        train_block = '01234'\n","        test_block = '56'\n","\n","\n","\n","\n","\n","        # new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block]\n","\n","        # new_row_df = pd.DataFrame([new_row], columns=column_names)\n","        # rf = pd.read_csv(PATH +'P'+str(p_num)+'.csv')\n","        # cf = pd.concat([rf, new_row_df], ignore_index=True)\n","        # cf.to_csv(PATH +'P'+str(p_num)+'.csv',index=False)\n","\n","\n","\n","        print(train_acc_list,\"train\")\n","        print(test_acc_list,\"test\")\n","        \n","    i+=1\n","\n","        \n","\n","\n","\n","# block_order_tr = ['Tongue','Feet','Mis','Hand']\n","# block_order_tr2 = ['Tongue','Mis','Hand','Feet']\n","# block_order_te = ['Feet','Hand','Tongue','Mis']\n","# CLASS_1 = \"Hand\"\n","# CLASS_2 = \"Rest\"\n","# tasks_time_tr = [16,16,12,20,20,8,8,12]\n","# tasks_time_tr2 = [20,20,12,12,8,8,16,16]\n","# tasks_time_te = [16,12,12,8,8,16,20,20]\n","\n","# df_tr = data_tr_.copy()\n","# df_tr2 = data_tr2_.copy()\n","# df_te = data_te_.copy()\n","# data_tr,labels_tr = preprocessor(df_tr,CLASS_1,CLASS_2,tasks_time_tr,\"TRAIN\")\n","# data_tr2,labels_tr2 = preprocessor(df_tr2,CLASS_1,CLASS_2,tasks_time_tr2,\"TRAIN\")\n","# data_te,labels_te = preprocessor(df_te,CLASS_1,CLASS_2,tasks_time_te,\"TEST\")\n","# data_tr = np.vstack((data_tr, data_tr2))\n","# labels_tr = np.vstack((labels_tr, labels_tr2))\n","# print(data_tr.shape)\n","# print(labels_tr.shape)\n","# print(data_te.shape)\n","# print(labels_te.shape)\n","\n","\n","\n","\n","\n","# print(data_tr.shape,labels_tr.shape)\n","# print(data_te.shape,labels_te.shape)\n","# print(labels_te)\n","# print(indexes)\n","# print(Begin_indexes)\n","# print(End_indexes)\n","# print(df.iloc[1,64])\n","\n","\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>participant</th>\n","      <th>class1</th>\n","      <th>class2</th>\n","      <th>running_time</th>\n","      <th>test_acc</th>\n","      <th>train_acc</th>\n","      <th>test_size</th>\n","      <th>train_size</th>\n","      <th>train_block</th>\n","      <th>test_block</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>5</td>\n","      <td>Mis</td>\n","      <td>Rest</td>\n","      <td>111.924240</td>\n","      <td>0.785714</td>\n","      <td>1.0</td>\n","      <td>(56, 64, 1000)</td>\n","      <td>(140, 64, 1000)</td>\n","      <td>1234</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>6</td>\n","      <td>Hand</td>\n","      <td>Rest</td>\n","      <td>28.528704</td>\n","      <td>0.785714</td>\n","      <td>1.0</td>\n","      <td>(56, 64, 1000)</td>\n","      <td>(140, 64, 1000)</td>\n","      <td>1234</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>6</td>\n","      <td>Feet</td>\n","      <td>Rest</td>\n","      <td>56.803421</td>\n","      <td>0.767857</td>\n","      <td>1.0</td>\n","      <td>(56, 64, 1000)</td>\n","      <td>(140, 64, 1000)</td>\n","      <td>1234</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>6</td>\n","      <td>Tongue</td>\n","      <td>Rest</td>\n","      <td>84.431607</td>\n","      <td>0.875000</td>\n","      <td>1.0</td>\n","      <td>(56, 64, 1000)</td>\n","      <td>(140, 64, 1000)</td>\n","      <td>1234</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>6</td>\n","      <td>Mis</td>\n","      <td>Rest</td>\n","      <td>112.289006</td>\n","      <td>0.785714</td>\n","      <td>1.0</td>\n","      <td>(56, 64, 1000)</td>\n","      <td>(140, 64, 1000)</td>\n","      <td>1234</td>\n","      <td>56</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   participant  class1 class2  running_time  test_acc  train_acc  \\\n","7            5     Mis   Rest    111.924240  0.785714        1.0   \n","8            6    Hand   Rest     28.528704  0.785714        1.0   \n","9            6    Feet   Rest     56.803421  0.767857        1.0   \n","10           6  Tongue   Rest     84.431607  0.875000        1.0   \n","11           6     Mis   Rest    112.289006  0.785714        1.0   \n","\n","         test_size       train_size train_block test_block  \n","7   (56, 64, 1000)  (140, 64, 1000)        1234         56  \n","8   (56, 64, 1000)  (140, 64, 1000)        1234         56  \n","9   (56, 64, 1000)  (140, 64, 1000)        1234         56  \n","10  (56, 64, 1000)  (140, 64, 1000)        1234         56  \n","11  (56, 64, 1000)  (140, 64, 1000)        1234         56  "]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.8571428571428571]\n","[0.8065476190476191]\n","[0.7767857142857143]\n","[0.75]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>b1234</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hand</td>\n","      <td>0.857143</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Feet</td>\n","      <td>0.806548</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tongue</td>\n","      <td>0.776786</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mis</td>\n","      <td>0.750000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    class     b1234\n","0    Hand  0.857143\n","1    Feet  0.806548\n","2  Tongue  0.776786\n","3     Mis  0.750000"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["p_num_list = [3,4,5,6,7,9]\n","vf = pd.DataFrame(columns=column_names) \n","for p_num in p_num_list:\n","    rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n","    vf = pd.concat([vf, rf], ignore_index=True)\n","vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n","# vf.tail()\n","    \n","columnNames = ['class','b1234']\n","kf = pd.DataFrame(columns=columnNames)\n","kf.to_csv(PATH+'AverageAcc.csv',index=False)\n","\n","vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n","df = vf\n","\n","class_list=['Hand','Feet','Tongue','Mis']\n","blk_list = [1234]\n","for class_ in class_list:\n","    avg_list = []\n","    for blk in blk_list:\n","        gf = df[(df['train_block'] == blk) & (df['class1'] == class_)]\n","        avg = gf['test_acc'].mean()\n","        avg_list.append(avg)\n","    print(avg_list)    \n","    new_row = [class_, avg_list[0]] \n","    new_row_df = pd.DataFrame([new_row], columns=columnNames)\n","    rf = pd.read_csv(PATH + 'AverageAcc.csv')\n","    cf = pd.concat([rf, new_row_df], ignore_index=True)\n","    cf.to_csv(PATH +'AverageAcc.csv',index=False)  \n","kf = pd.read_csv(PATH +'AverageAcc.csv') \n","kf.head()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      4         for     b\n","4      5     example     b\n","5      6    purposes     b\n","6      7          in     a\n","7      8        this     a\n","8      9        case     a\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n","his\n","    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      7          in     a\n","4      8        this     a\n","5      9        case     a\n","6      4         for     b\n","7      5     example     b\n","8      6    purposes     b\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n"]}],"source":["import pandas as pd\n","\n","# Assuming df is your DataFrame with the last column named 'label'\n","data = {'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n","        'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter'],\n","        'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b']}\n","\n","df = pd.DataFrame(data)\n","\n","print(df)\n","print(\"his\")\n","# Define a custom sorting order based on the desired grouping\n","sorting_order = {'a': 0, 'b': 1}\n","\n","# Create a new column with the sorting order\n","df['sorting_order'] = df.iloc[:, 2].map(sorting_order)\n","\n","# Sort the DataFrame based on the new column and the original order within each group\n","df.sort_values(by=['sorting_order', df.columns[2]], inplace=True)\n","\n","# Drop the temporary sorting column\n","df.drop('sorting_order', axis=1, inplace=True)\n","\n","# Optional: Reset the index if needed\n","df.reset_index(drop=True, inplace=True)\n","\n","# Display the sorted DataFrame\n","print(df)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3, 3, 3, 4]\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','b'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","x=0\n","i=0\n","class_1 = 'a'\n","class_2 = 'b'\n","sampleList = []\n","while i<len(df):\n","    if (df.iloc[i,2]==class_1):\n","        x+=1\n","    else:\n","        i-=1\n","        sampleList.append(x)\n","        x=0\n","        class_1,class_2 = class_2,class_1\n","    i+=1\n","sampleList.append(x)\n","print(sampleList)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["group\n","2    3\n","4    4\n","dtype: int64\n","2\n","3\n","group\n","1    3\n","3    3\n","dtype: int64\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','c'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Identify consecutive groups of 'a's by creating a new group ID each time 'label' changes from 'b' to 'a'\n","df['group'] = (df['label'] != df['label'].shift(1)).cumsum()\n","\n","# Count occurrences of 'a' within each group\n","group_counts = df[df['label'] == 'a'].groupby('group').size()\n","\n","group_counts_b = df[df['label'] == 'b'].groupby('group').size()\n","print(group_counts_b)\n","print(group_counts_b.index[0])\n","print(group_counts_b.iloc[0])\n","print(group_counts)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["p_num = 6\n","b_num = 7\n","path = f'../../Participants/P{p_num}/'\n","mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","df_1 = pd.DataFrame(mat['Data'])\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","[6191, 10157, 8157, 4065, 10161, 8156, 4060, 6014]\n","[8156, 8156, 6178, 10157, 10156, 4064, 4063, 6015]\n","[10158, 6176, 8165, 10155, 4073, 8156, 6184, 4016]\n","[10165, 10155, 6183, 6177, 4060, 4062, 8162, 8016]\n"]}],"source":["extra_samples_block_counter(df_1,trial_order[0])"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","Tongue\n","[3093, 5078, 4078, 2030, 5078, 4079, 2032, 3007]\n","Feet\n","[4078, 4078, 3087, 5079, 5078, 2030, 2035, 3007]\n","Mis\n","[5080, 3089, 4076, 5079, 2036, 4077, 3093, 2007]\n","Hand\n","[5080, 5078, 3092, 3088, 2035, 2030, 4077, 4007]\n","hi\n","Feet\n","[3091, 4082, 2034, 3093, 5082, 2034, 4079, 5007]\n","Mis\n","[4083, 2033, 5079, 5082, 2031, 3090, 3087, 4007]\n","Hand\n","[2033, 5078, 5079, 4078, 4079, 2036, 3090, 3007]\n","Tongue\n","[2033, 3089, 3092, 4078, 5082, 5082, 4079, 2008]\n","hi\n","Hand\n","[4078, 2035, 2036, 5082, 3089, 4083, 5083, 3008]\n","Feet\n","[5077, 3088, 4078, 2035, 3088, 5077, 2033, 4007]\n","Tongue\n","[3088, 4082, 5082, 3087, 2031, 5079, 4077, 2007]\n","Mis\n","[2037, 2035, 3093, 3091, 4076, 5079, 5081, 4007]\n","hi\n","Tongue\n","[3087, 5081, 4082, 2035, 5077, 4077, 2031, 3008]\n","Mis\n","[4082, 4083, 3089, 5156, 5078, 2111, 2026, 3007]\n","Hand\n","[5105, 3088, 4106, 5077, 2065, 4076, 3122, 2007]\n","Feet\n","[5077, 5116, 3090, 3121, 2026, 2049, 4079, 4008]\n","hi\n","Mis\n","[4077, 2034, 2061, 3092, 5171, 4082, 3165, 5008]\n","Feet\n","[3201, 4078, 4077, 5177, 2030, 3166, 5083, 2007]\n","Hand\n","[5079, 3132, 2026, 4084, 3125, 2031, 4124, 5006]\n","Tongue\n","[2035, 5102, 5097, 2037, 3105, 3118, 4112, 4007]\n","hi\n","Feet\n","[4095, 3110, 3110, 2034, 2040, 4114, 5094, 5007]\n","Hand\n","[5089, 4087, 4090, 2039, 3101, 5087, 2047, 3008]\n","Tongue\n","[2029, 5086, 4086, 3095, 5085, 4088, 3106, 2008]\n","Mis\n","[3101, 2042, 2037, 4086, 4084, 3094, 5089, 5007]\n","hi\n","Hand\n","[4085, 4090, 2043, 2038, 3096, 3095, 5090, 5007]\n","Tongue\n","[5089, 3097, 2043, 5089, 4084, 2043, 3095, 4008]\n","Mis\n","[2030, 5086, 4100, 4086, 3100, 3095, 5090, 2008]\n","Feet\n","[4084, 2037, 5084, 4084, 3101, 5090, 2038, 3008]\n"]}],"source":["for b in range(7):\n","    extra_samples_block_counter(data_dicts_list[-1][b],trial_order[b],b)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMHUhd3A17lB+9o7HFw3Jl4","provenance":[]},"kernelspec":{"display_name":"kernel2","language":"python","name":"kernel2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
