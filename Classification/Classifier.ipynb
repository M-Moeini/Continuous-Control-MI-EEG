{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696289517013,"user":{"displayName":"Mahdi Moeini","userId":"03671813669356560168"},"user_tz":-210},"id":"za0kvkt7u2Z5","outputId":"78ba6e7e-7a2c-4096-e595-beca84ab98ba"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mahdi146/jupyter2/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n","  from pandas import MultiIndex, Int64Index\n"]}],"source":["import sys\n","import mne\n","import scipy.io as sp\n","import numpy as np\n","import random\n","import pandas as pd\n","import multiprocessing as mp\n","import concurrent.futures\n","from mne.decoding import CSP\n","import pymrmr\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier as RF\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import logging\n","from scipy.io import loadmat\n","from scipy.signal import hamming\n","from scipy.signal import hann\n","from scipy.signal import blackman\n","from scipy.signal import kaiser\n","from scipy.signal import gaussian\n","from sklearn.decomposition import FastICA\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","import lightgbm as lgb\n","from catboost import CatBoostClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","\n","# Set display options for NumPy\n","np.set_printoptions(threshold=np.inf)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["WINDOW_TIME_LENGTH = 4\n","SAMPLING_RATE = 250\n","TR_SLIDING_WINDOW_TIME = 2\n","WINDOW_SAMPLE_LENGTH = WINDOW_TIME_LENGTH*SAMPLING_RATE\n","NUMBER_OF_CHANNELS = 64\n","SLIDING_TIME = 4 \n","SLIDING_POINTS = SLIDING_TIME*SAMPLING_RATE\n","TR_SLIDING_POINTS = TR_SLIDING_WINDOW_TIME*SAMPLING_RATE\n","beta = 1.5\n","\n","num_channels = 64\n","epoch_length = 1000\n","sampling_freq = 250\n","number_of_runs = 10\n","# number_of_splits = 10\n","number_of_components = 10\n","number_of_selected_features = 10\n","number_of_processes = 10\n","number_of_bands = 9\n","# rf = pd.DataFrame()\n","column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n","# rf = rf.reindex(columns=column_names)\n","\n","trial_order=[['Tongue','Feet','Mis','Hand'],\n","            ['Feet','Mis','Hand','Tongue'],\n","            ['Hand','Feet','Tongue','Mis'],\n","            ['Tongue','Mis','Hand','Feet'],\n","            ['Mis','Feet','Hand','Tongue'],\n","            ['Feet','Hand','Tongue','Mis'],\n","            ['Hand','Tongue','Mis','Feet'],\n","            ['Tongue','Feet','Mis','Hand'],\n","            ['Mis','Tongue','Hand','Feet']]\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_task_rest_times(b_num):\n","    if b_num == 0:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 1:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]\n","        \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]\n","        \n","    elif b_num == 2:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    elif b_num == 3:\n","        task_time = [[12, 16, 20, 8],\n","                    [16, 12, 20, 8],\n","                    [20, 16, 8, 12],\n","                    [20, 12, 8, 16]]\n","        \n","        rest_time = [[20, 8, 16, 12],\n","                    [16, 20, 8, 12],\n","                    [12, 20, 16, 8],\n","                    [20, 12, 8, 16]]\n","        \n","    elif b_num == 4:\n","        task_time = [[16, 8, 20, 12],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16],\n","                    [8, 20, 12, 16]]\n","        \n","        rest_time = [[8, 12, 16, 20],\n","                    [16, 20, 12, 8],\n","                    [12, 16, 8, 20],\n","                    [20, 8, 12, 16]]\n","        \n","    elif b_num == 5:\n","        task_time = [[16, 12, 8, 20],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12],\n","                    [12, 8, 16, 20]]\n","\n","        rest_time = [[12, 8, 16, 20],\n","                    [16, 8, 20, 12],\n","                    [20, 12, 16, 8],\n","                    [8, 16, 12, 20]]\n","        \n","    elif b_num == 6:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 8, 16, 12],\n","                    [8, 16, 12, 20],\n","                    [16, 20, 12, 8]]\n","\n","        rest_time = [[16, 8, 12, 20],\n","                    [12, 20, 8, 16],\n","                    [20, 16, 12, 8],\n","                    [8, 16, 20, 12]]     \n","    elif b_num ==7:\n","        task_time = [[12, 8, 20, 16],\n","                    [16, 20, 8, 12],\n","                    [8, 20, 16, 12],\n","                    [8, 12, 20, 16]]   \n","               \n","        rest_time = [[16, 12, 8, 20],\n","                    [8, 20, 12, 16],\n","                    [20, 16, 8, 12],\n","                    [12, 16, 20, 8]]  \n","    \n","    elif b_num == 8:\n","        task_time = [[16, 8, 12, 20],\n","                    [20, 16, 12, 8],\n","                    [12, 20, 8, 16],\n","                    [8, 12, 16, 20]]\n","        \n","        rest_time = [[8, 20, 16, 12],\n","                    [12, 8, 20, 16],\n","                    [16, 12, 20, 8],\n","                    [8, 12, 20, 16]]\n","        \n","    else:\n","        print(\"Error in block number\")\n","\n","    return task_time,rest_time\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def trial_times_genertor(task_times,rest_times):\n","    block_times = [item for pair in zip(task_times, rest_times) for item in pair]\n","    return block_times\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def calc_csp(x_train, y_train, x_test):\n","    csp = CSP(number_of_components)\n","    csp_fit = csp.fit(x_train, y_train)\n","    train_feat = csp_fit.transform(x_train)\n","    test_feat = csp_fit.transform(x_test)\n","    return train_feat, test_feat"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def class_extractor(number_of_epochs, class_1, class_2, data, labels):\n","    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n","    Final_labels = np.zeros((size,1)).astype(int)\n","    dataset = np.zeros((size,num_channels, epoch_length))\n","    index = 0\n","    for i in range(number_of_epochs):\n","        if labels[i,0] == class_1 or labels[i,0] == class_2:\n","            dataset[index,:,:] = data[i,:,:]\n","            Final_labels[index,0] = labels[i,0]\n","            index = index + 1\n","        else:\n","            continue\n","            \n","    return dataset, Final_labels"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def feature_extractor(dataset, labels, number_of_bands, test_data):\n","\n","    low_cutoff = 0\n","    \n","    for b in range(number_of_bands):\n","        logging.getLogger('mne').setLevel(logging.WARNING)\n","        low_cutoff += 4\n","        data = dataset.copy()\n","        data_test = test_data.copy()\n","        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n","        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test)\n","        if b == 0:\n","            train_features = train_feats\n","            test_features = test_feats\n","        else:\n","            train_features = np.concatenate((train_features, train_feats), axis = 1)\n","            test_features = np.concatenate((test_features, test_feats), axis = 1)\n","    \n","    return train_features, test_features"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def feature_selector(train_features, labels, number_of_selected_features):\n","    X = pd.DataFrame(train_features)\n","    y = pd.DataFrame(labels)\n","    K = number_of_selected_features\n","    \n","    df = pd.concat([y,X], axis = 1)\n","    df.columns = df.columns.astype(str)\n","        \n","    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n","    return selected_features"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def data_reader(path,p_num,block_list):\n","    data_dict = {}\n","    for b_num in block_list:\n","        print(b_num)\n","        mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","        df = pd.DataFrame(mat['Data'])\n","        # ddf = dd.from_pandas(df, npartitions=10)\n","        data_dict[b_num] = df\n","    return data_dict\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def extra_samples_counter(df,class_1,class_2):\n","    x=0\n","    i=0\n","    sampleList = []\n","    while i<len(df):\n","        if (df.iloc[i,64]==class_1):\n","            x+=1\n","        else:\n","            i-=1\n","            sampleList.append(x)\n","            x=0\n","            class_1,class_2 = class_2,class_1\n","        i+=1\n","    sampleList.append(x)\n","    print(sampleList)\n","    "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def extra_samples_block_counter(df,trial_order,b_num):\n","\n","    df.drop(df[df.iloc[:,64].isin(['Begin', 'End'])].index, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","    print('hi')\n","    \n","    df['group'] = (df.iloc[:,64] != df.iloc[:,64].shift(1)).cumsum()\n","    # group_counts_Tongue = df[df.iloc[:,64] == 'Tongue'].groupby('group').size()\n","    # group_counts_Feet = df[df.iloc[:,64] == 'Feet'].groupby('group').size()\n","    # group_counts_Hand = df[df.iloc[:,64] == 'Hand'].groupby('group').size()\n","    # group_counts_Mis = df[df.iloc[:,64] == 'Mis'].groupby('group').size()\n","    # group_counts_Rest = df[df.iloc[:,64] == 'Rest'].groupby('group').size()\n","\n","    \n","    group_counts_Rest = df[df.iloc[:,64] == 'Rest'].groupby('group').size()\n","    with open('sampleList.txt', 'a') as file:\n","        file.write(f'block {b_num+1} '+'\\n')\n","        for j in range (len(trial_order)):\n","            print(trial_order[j])\n","            trial_num = j\n","            task_times,rest_times = get_task_rest_times(b_num)\n","            trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","            trial_samples = [item*SAMPLING_RATE for item in trial_times]\n","            group_counts_task = df[df.iloc[:,64] == trial_order[j]].groupby('group').size()\n","            sampleList = []\n","            for i in range(4):\n","                task = group_counts_task.iloc[i]\n","                rest = group_counts_Rest.iloc[4*j+i]\n","                sampleList.append(task)\n","                sampleList.append(rest)\n","            # extra_samples = [x-y for x,y in zip(sampleList,trial_samples)]\n","            file.write(', '.join(map(str, sampleList)) + f' trial={trial_order[j]} '+'\\n')\n","            print(sampleList)\n","        file.write('\\n\\n')\n","\n","\n","    # print(group_counts_Tongue)\n","    # print(group_counts_Feet)\n","    # print(group_counts_Hand)\n","    # print(group_counts_Mis)\n","    # print(group_counts_Rest)\n","\n","    # print(group_counts_b.index[0])\n","    # print(group_counts_b.iloc[0])\n","    # print(group_counts)\n","\n","\n","    # for j in range(len(trial_order)):\n","    #     print(j)\n","    #     class_2 = 'Rest'\n","    #     class_1 = trial_order[j]\n","    #     sampleList = []\n","    #     x=0\n","    #     i=0\n","\n","\n","    #     while i<len(df):\n","    #         print(i)\n","    #         if (df.iloc[i,64]!=class_1):\n","    #             x+=1\n","    #         else:\n","    #             i-=1\n","    #             sampleList.append(x)\n","    #             x=0\n","    #             class_1,class_2 = class_2,class_1\n","    #         i+=1\n","    #     sampleList.append(x)\n","    #     df.drop(df.index[0:sum(sampleList)], inplace=True)\n","    #     df.reset_index(drop=True, inplace=True)\n","    #     print(sampleList)\n","        # with open('sampleList.txt', 'w') as file:\n","        #     # for item in sampleList:\n","        #     file.write(f\"{sampleList}\\n\")\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","def data_cleaner(df,class_1,class_2,tasks_time):\n","    # extra_samples_counter(df,class_1,class_2)\n","    # sys.exit() \n","    class_x = class_1\n","    class_y = class_2\n","    new_df = pd.DataFrame()\n","    trial_df = df.copy() \n","    print(tasks_time)\n","    for i in range(len(tasks_time)):\n","        sample_point = tasks_time[i]*SAMPLING_RATE\n","        if(trial_df.iloc[sample_point+1,64] == class_x ):\n","            if(i==len(tasks_time)-1):\n","                temp_df = trial_df.iloc[:sample_point,:]\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","            else:    \n","                temp_df = trial_df.iloc[:sample_point,:]\n","                next_task_idx = trial_df[trial_df.iloc[:, 64] == class_y].index\n","                trial_df.drop(trial_df.index[0:next_task_idx[0]], inplace=True)\n","                trial_df.reset_index(drop=True, inplace=True)\n","                new_df = pd.concat([new_df, temp_df], axis=0)\n","                new_df.reset_index(drop=True, inplace=True)\n","                class_x,class_y = class_y,class_x\n","\n","    return new_df"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def class_seperator(cleaned_df,class_1,class_2):\n","    # df = cleaned_df.sort_values(by=cleaned_df.columns[64]).reset_index(drop=True)\n","    # print(seperated_df.head(14003))\n","    # print(cleaned_df.head(5003))\n","\n","    df = cleaned_df\n","    sorting_order = {class_1: 0, class_2: 1}\n","\n","    df['sorting_order'] = df.iloc[:, 64].map(sorting_order)\n","    df.sort_values(by=['sorting_order', df.columns[64]], inplace=True)\n","    df.drop('sorting_order', axis=1, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def shuffler(dataset,labels):\n","    print(dataset.shape)\n","    print(labels.shape)\n","    np.random.seed(42)\n","    indices = np.random.permutation(len(dataset))\n","    shuffled_dataset = dataset[indices]\n","    shuffled_labels = labels[indices]\n","    return shuffled_dataset,shuffled_labels\n","    "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def data_label_attacher(cleaned_df,class_1,class_2,random_flag,class_seperator_flag):\n","    \n","    #Initialization\n","    if class_seperator_flag:\n","        seperated_class_df = class_seperator(cleaned_df,class_1,class_2)\n","        new_df_ = seperated_class_df.copy()\n","        new_df_.drop(seperated_class_df.columns[-1], axis=1, inplace=True)\n","        X = new_df_.to_numpy()\n","        X = np.transpose(X)\n","        number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n","        print(number_of_epochs)\n","    else :  \n","        new_df_ = cleaned_df.copy()\n","        new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n","        X = new_df_.to_numpy()\n","        X = np.transpose(X)\n","        number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n","\n","    dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n","    labels = np.zeros((number_of_epochs,1)).astype(int)\n","\n","    if class_seperator_flag:\n","        i = 0  \n","        startIdx = i * WINDOW_SAMPLE_LENGTH\n","        endIdx = (i+1) * WINDOW_SAMPLE_LENGTH \n","        while(endIdx<=int(len(new_df_))/2):\n","            slice_X = X[:, startIdx:endIdx]\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","            dataset[i, :, :] = slice_X\n","            labels[i,0] = 0\n","            # if (seperated_class_df.iloc[startIdx, 64] == class_1):\n","            #     labels[i,0] = 0\n","            # elif(seperated_class_df.iloc[startIdx, 64] == class_2):\n","            #     labels[i,0] = 1\n","            # else:\n","            #     labels[i,0] = 2\n","            startIdx+=TR_SLIDING_POINTS\n","            endIdx+=TR_SLIDING_POINTS\n","            i+=1\n","        # print(int(len(new_df_))/2,\"len\")    \n","        # print(endIdx,\"endIdx\")    \n","        # print(seperated_class_df.iloc[endIdx-2:endIdx+2,64])\n","       \n","        j = i\n","        \n","        startIdx = endIdx-TR_SLIDING_POINTS\n","        endIdx = startIdx+WINDOW_SAMPLE_LENGTH\n","        print(j, \"j is this\")\n","        while(endIdx<=int(len(new_df_))):\n","            slice_X = X[:, startIdx:endIdx]\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","            dataset[j, :, :] = slice_X\n","            labels[j,0] = 1\n","            # if (cleaned_df.iloc[startIdx, 64] == class_1):\n","            #     labels[j,0] = 0\n","            # elif(cleaned_df.iloc[startIdx, 64] == class_2):\n","            #     labels[j,0] = 1\n","            # else:\n","            #     labels[j,0] = 2\n","            startIdx+=TR_SLIDING_POINTS\n","            endIdx+=TR_SLIDING_POINTS\n","            j+=1\n","        print(j, \"j is this\")\n","        # dataset,labels = shuffler(dataset,labels)\n","\n","    else:\n","        i = 0  \n","        start_idx = i * WINDOW_SAMPLE_LENGTH\n","        end_idx = (i+1) * WINDOW_SAMPLE_LENGTH \n","        while (end_idx<=int(len(new_df_))):\n","            slice_X = X[:, start_idx:end_idx]\n","            kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n","            slice_X *= kaiser_window\n","            dataset[i, :, :] = slice_X\n","            if (cleaned_df.iloc[start_idx, 64] == class_1):\n","                labels[i,0] = 0\n","            elif(cleaned_df.iloc[start_idx, 64] == class_2):\n","                labels[i,0] = 1\n","            else:\n","                labels[i,0] = 2\n","            start_idx+=SLIDING_POINTS\n","            end_idx+=SLIDING_POINTS\n","            i+=1\n","        # dataset,labels = shuffler(dataset,labels)\n","\n","\n","\n","\n","\n","\n","\n","    #For training and test purpose\n","    # if random_flag:\n","    #     randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n","    # else:\n","    #     randomlist = list(range(number_of_epochs))\n","    #Labeling the data\n","\n","\n","\n","    # for i in range(number_of_epochs):\n","    #     start_idx = randomlist[i] * WINDOW_SAMPLE_LENGTH + SLIDING_POINTS\n","    #     end_idx = (randomlist[i] + 1) * WINDOW_SAMPLE_LENGTH\n","    #     slice_X = X[:, start_idx:end_idx]\n","\n","    #     # hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hamming_window\n","\n","    #     # hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= hanning_window\n","\n","    #     # blackman_window = blackman(WINDOW_SAMPLE_LENGTH)\n","    #     # slice_X *= blackman_window\n","\n","    #     # kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= kaiser_window\n","\n","    #     # gaussian_window = gaussian(WINDOW_SAMPLE_LENGTH,0.5)\n","    #     # slice_X *= gaussian_window\n","\n","\n","    #     dataset[i, :, :] = slice_X\n","    #     if (cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_1):\n","    #         labels[i,0] = 0\n","    #     elif(cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_2):\n","    #         labels[i,0] = 1\n","    #     else:\n","    #         labels[i,0] = 2\n","\n","    return dataset,labels\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def trial_cutter(data, class_1):\n","    df = data.copy()\n","    Begin_trigger = \"Begin\" + \"_\" + class_1\n","    End_trigger = \"End\" + \"_\" + class_1\n","    Begin_idx = df[df.iloc[:, 64] == Begin_trigger].index\n","    End_idx = df[df.iloc[:, 64] == End_trigger].index\n","    trial_df = df.iloc[Begin_idx[0]+1:End_idx[0],:]\n","    trial_df.reset_index(drop=True, inplace=True)\n","    trial_df.head()\n","    return trial_df"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def Begin_End_trigger_modifier(data):\n","    df = data.copy()\n","    Begin_indexes = df[df.iloc[:, 64] == 'Begin'].index\n","    End_indexes = df[df.iloc[:, 64] == 'End'].index\n","    if(len(Begin_indexes)==len(End_indexes)):\n","        for i in range(len(Begin_indexes)):\n","            index = Begin_indexes[i]+1\n","            val = df.iloc[index,64]\n","            df.iloc[Begin_indexes[i],64] = \"Begin\" + \"_\" + str(val)\n","            df.iloc[End_indexes[i],64]   =  \"End\" + \"_\" + str(val)\n","    else:\n","        print(\"Trigger seinding Exception\")\n","    \n","    return df"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def preprocessor(data_,class_1,class_2,tasks_time,set_type,class_seperator_flag):\n","    CLASS_1 = class_1\n","    CLASS_2 = class_2\n","    df = data_.copy()\n","    modified_df = Begin_End_trigger_modifier(df)\n","    trial_df = trial_cutter(modified_df,CLASS_1)\n","    print(trial_df.shape,\"trial_df\")\n","    cleaned_df = data_cleaner(trial_df,CLASS_1,CLASS_2,tasks_time)\n","    print(cleaned_df.shape,\"cleaned_df\")\n","\n","    if set_type ==\"TRAIN\":\n","        random_flag = True\n","    elif set_type ==\"TEST\":\n","        random_flag = False\n","    else:\n","        print(\"Error in set type\")\n","\n","  \n","    final_data, final_labels = data_label_attacher(cleaned_df,CLASS_1,CLASS_2,random_flag,class_seperator_flag)\n","    print(final_data.shape,\"final_data shape\")\n","    print(final_labels.shape,\"final_labels shape\")\n","    \n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def trials_set_builder(data_dict,blocks_set,set_label,class_1,class_2,class_seperator_flag):\n","    counter = 0\n","\n","    for b_num in blocks_set:\n","        trial_num = trial_order[b_num].index(class_1)\n","        task_times,rest_times = get_task_rest_times(b_num)\n","        print(task_times[trial_num],rest_times[trial_num])\n","        trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n","        print(trial_times)\n","        data = data_dict[b_num]\n","        df = data.copy()\n","        # last_column = df.pop(df.columns[-1])\n","        # df.drop(df.columns[-1], axis=1, inplace=True)\n","        # eeg_data = df.to_numpy().T  # Transpose to have channels in columns\n","\n","        # channel_names = [f'Ch{i+1}' for i in range(63)]\n","\n","        # # Create MNE-Python RawArray object\n","        # info = mne.create_info(ch_names=channel_names, sfreq=sampling_freq, ch_types='eeg')\n","        # raw = mne.io.RawArray(eeg_data, info)\n","\n","        # # Apply ICA\n","        # ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n","        # ica.fit(raw)\n","        # ica_components = ica.get_components()\n","\n","        # # Convert the ICA components to a DataFrame\n","        # df2 = pd.DataFrame(data=ica_components.T, columns=channel_names)\n","        # df2 = df2.assign(LastColumn=last_column)\n","        # # df = data.copy(deep=False)\n","        dataset,labels = preprocessor(df,class_1,class_2,trial_times,set_label,class_seperator_flag)\n","        # print(dataset.shape)\n","\n","        if counter == 0 :\n","            final_data = dataset\n","            final_labels = labels\n","            print(\"Before concatenation - final_data shape:\", final_data.shape, \"dataset shape:\", dataset.shape)\n","        else:\n","            final_data = np.vstack((final_data, dataset))\n","            final_labels = np.vstack((final_labels, labels))\n","            print(\"After concatenation - final_data shape:\", final_data.shape, \"final_labels shape:\", final_labels.shape)\n","\n","        counter+=1 \n","    return final_data,final_labels"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","3\n","4\n","5\n","6\n","7\n","8\n"]}],"source":["blcok_list = [0,1,3,4,5,6,7,8]\n","p_num = 1\n","data_dict = data_reader('../../Participants/P1/',p_num,blcok_list)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n"]}],"source":["blcok_list = [0,1,2,3,4,5,6]\n","p_num = 4\n","data_dict_3 = data_reader(f'../../Participants/P{p_num}/',p_num,blcok_list)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[12, 16, 20, 8] [20, 8, 16, 12]\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28481, 65) trial_df\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[8, 12, 20, 16] [12, 16, 20, 8]\n","[8, 12, 12, 16, 20, 20, 16, 8]\n","(28539, 65) trial_df\n","[8, 12, 12, 16, 20, 20, 16, 8]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","[12, 20, 8, 16] [16, 12, 20, 8]\n","[12, 16, 20, 12, 8, 20, 16, 8]\n","(28526, 65) trial_df\n","[12, 16, 20, 12, 8, 20, 16, 8]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n","[12, 16, 20, 8] [20, 8, 16, 12]\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28470, 65) trial_df\n","[12, 20, 16, 8, 20, 16, 8, 12]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n","[8, 20, 12, 16] [20, 8, 12, 16]\n","[8, 20, 20, 8, 12, 12, 16, 16]\n","(28776, 65) trial_df\n","[8, 20, 20, 8, 12, 12, 16, 16]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n","[8, 16, 20, 12] [20, 12, 16, 8]\n","[8, 20, 16, 12, 20, 16, 12, 8]\n","(28757, 65) trial_df\n","[8, 20, 16, 12, 20, 16, 12, 8]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","Before concatenation - final_data shape: (28, 64, 1000) dataset shape: (28, 64, 1000)\n","[20, 8, 16, 12] [12, 20, 8, 16]\n","[20, 12, 8, 20, 16, 8, 12, 16]\n","(28545, 65) trial_df\n","[20, 12, 8, 20, 16, 8, 12, 16]\n","(28000, 65) cleaned_df\n","(28, 64, 1000) final_data shape\n","(28, 1) final_labels shape\n","After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n","(140, 64, 1000) (140, 1) train shape\n","(56, 64, 1000) (56, 1) test shape\n","\n","\n"," *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n","     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n","     the paper \n","     \"Feature selection based on mutual information: criteria of \n","      max-dependency, max-relevance, and min-redundancy,\"\n","      Hanchuan Peng, Fuhui Long, and Chris Ding, \n","      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n","      Vol. 27, No. 8, pp.1226-1238, 2005.\n","\n","\n","*** MaxRel features ***\n","Order \t Fea \t Name \t Score\n","1 \t 33 \t 32 \t 0.472\n","2 \t 14 \t 13 \t 0.425\n","3 \t 23 \t 22 \t 0.371\n","4 \t 35 \t 34 \t 0.358\n","5 \t 25 \t 24 \t 0.309\n","6 \t 15 \t 14 \t 0.309\n","7 \t 38 \t 37 \t 0.307\n","8 \t 16 \t 15 \t 0.304\n","9 \t 48 \t 47 \t 0.281\n","10 \t 11 \t 10 \t 0.277\n","\n","*** mRMR features *** \n","Order \t Fea \t Name \t Score\n","1 \t 33 \t 32 \t 0.472\n","2 \t 18 \t 17 \t 0.078\n","3 \t 14 \t 13 \t 0.144\n","4 \t 48 \t 47 \t 0.078\n","5 \t 38 \t 37 \t 0.085\n","6 \t 83 \t 82 \t 0.081\n","7 \t 35 \t 34 \t 0.093\n","8 \t 17 \t 16 \t 0.089\n","9 \t 16 \t 15 \t 0.095\n","10 \t 23 \t 22 \t 0.087\n","[1.0] train\n","[0.7142857142857143] test\n"]}],"source":["\n","\n","PATH = '../../Participants/P3/'\n","class_1 = 'Tongue'\n","class_2 = 'Rest'\n","b_num = 0\n","p_num = 3\n","train_blocks_set = [0,1,2,3,4]\n","test_blocks_set = [5,6]\n","\n","X_tr, Y_tr = trials_set_builder(data_dict_3,train_blocks_set,'TRAIN',class_1,class_2,False)\n","X_te, Y_te = trials_set_builder(data_dict_3,test_blocks_set,'TEST',class_1,class_2,False)\n","\n","print(X_tr.shape,Y_tr.shape,\"train shape\")\n","print(X_te.shape,Y_te.shape,\"test shape\")\n","\n","[train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te)\n","selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n","\n","train_acc_list = []\n","test_acc_list = []\n","\n","clf = XGBClassifier()\n","for r in range(1):\n","    clf.fit(train_features[:, selected_features], Y_tr[:,0])\n","\n","    y_pr_te = clf.predict(test_features[:, selected_features])\n","    y_pr_tr = clf.predict(train_features[:,selected_features])\n","\n","    accuracy_te = accuracy_score(Y_te, y_pr_te)\n","    test_acc_list.append(accuracy_te)\n","\n","    accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n","    train_acc_list.append(accuracy_tr)\n","\n","print(train_acc_list,\"train\")\n","print(test_acc_list,\"test\")\n","\n","\n","    \n","\n","\n","\n","# block_order_tr = ['Tongue','Feet','Mis','Hand']\n","# block_order_tr2 = ['Tongue','Mis','Hand','Feet']\n","# block_order_te = ['Feet','Hand','Tongue','Mis']\n","# CLASS_1 = \"Hand\"\n","# CLASS_2 = \"Rest\"\n","# tasks_time_tr = [16,16,12,20,20,8,8,12]\n","# tasks_time_tr2 = [20,20,12,12,8,8,16,16]\n","# tasks_time_te = [16,12,12,8,8,16,20,20]\n","\n","# df_tr = data_tr_.copy()\n","# df_tr2 = data_tr2_.copy()\n","# df_te = data_te_.copy()\n","# data_tr,labels_tr = preprocessor(df_tr,CLASS_1,CLASS_2,tasks_time_tr,\"TRAIN\")\n","# data_tr2,labels_tr2 = preprocessor(df_tr2,CLASS_1,CLASS_2,tasks_time_tr2,\"TRAIN\")\n","# data_te,labels_te = preprocessor(df_te,CLASS_1,CLASS_2,tasks_time_te,\"TEST\")\n","# data_tr = np.vstack((data_tr, data_tr2))\n","# labels_tr = np.vstack((labels_tr, labels_tr2))\n","# print(data_tr.shape)\n","# print(labels_tr.shape)\n","# print(data_te.shape)\n","# print(labels_te.shape)\n","\n","\n","\n","\n","\n","# print(data_tr.shape,labels_tr.shape)\n","# print(data_te.shape,labels_te.shape)\n","# print(labels_te)\n","# print(indexes)\n","# print(Begin_indexes)\n","# print(End_indexes)\n","# print(df.iloc[1,64])\n","\n","\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      4         for     b\n","4      5     example     b\n","5      6    purposes     b\n","6      7          in     a\n","7      8        this     a\n","8      9        case     a\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n","his\n","    col1        col2 label\n","0      1        some     a\n","1      2      random     a\n","2      3        data     a\n","3      7          in     a\n","4      8        this     a\n","5      9        case     a\n","6      4         for     b\n","7      5     example     b\n","8      6    purposes     b\n","9     10          it     b\n","10    11        does     b\n","11    12  not matter     b\n"]}],"source":["import pandas as pd\n","\n","# Assuming df is your DataFrame with the last column named 'label'\n","data = {'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n","        'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter'],\n","        'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b']}\n","\n","df = pd.DataFrame(data)\n","\n","print(df)\n","print(\"his\")\n","# Define a custom sorting order based on the desired grouping\n","sorting_order = {'a': 0, 'b': 1}\n","\n","# Create a new column with the sorting order\n","df['sorting_order'] = df.iloc[:, 2].map(sorting_order)\n","\n","# Sort the DataFrame based on the new column and the original order within each group\n","df.sort_values(by=['sorting_order', df.columns[2]], inplace=True)\n","\n","# Drop the temporary sorting column\n","df.drop('sorting_order', axis=1, inplace=True)\n","\n","# Optional: Reset the index if needed\n","df.reset_index(drop=True, inplace=True)\n","\n","# Display the sorted DataFrame\n","print(df)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3, 3, 3, 4]\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','b'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","x=0\n","i=0\n","class_1 = 'a'\n","class_2 = 'b'\n","sampleList = []\n","while i<len(df):\n","    if (df.iloc[i,2]==class_1):\n","        x+=1\n","    else:\n","        i-=1\n","        sampleList.append(x)\n","        x=0\n","        class_1,class_2 = class_2,class_1\n","    i+=1\n","sampleList.append(x)\n","print(sampleList)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["group\n","2    3\n","4    4\n","dtype: int64\n","2\n","3\n","group\n","1    3\n","3    3\n","dtype: int64\n"]}],"source":["data = {\n","    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n","    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','c'],\n","    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Identify consecutive groups of 'a's by creating a new group ID each time 'label' changes from 'b' to 'a'\n","df['group'] = (df['label'] != df['label'].shift(1)).cumsum()\n","\n","# Count occurrences of 'a' within each group\n","group_counts = df[df['label'] == 'a'].groupby('group').size()\n","\n","group_counts_b = df[df['label'] == 'b'].groupby('group').size()\n","print(group_counts_b)\n","print(group_counts_b.index[0])\n","print(group_counts_b.iloc[0])\n","print(group_counts)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["p_num = 4\n","b_num = 7\n","path = f'../../Participants/P{p_num}/'\n","mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n","df_1 = pd.DataFrame(mat['Data'])\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","[6191, 10157, 8157, 4065, 10161, 8156, 4060, 6014]\n","[8156, 8156, 6178, 10157, 10156, 4064, 4063, 6015]\n","[10158, 6176, 8165, 10155, 4073, 8156, 6184, 4016]\n","[10165, 10155, 6183, 6177, 4060, 4062, 8162, 8016]\n"]}],"source":["extra_samples_block_counter(df_1,trial_order[0])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","Tongue\n","[3096, 5078, 4079, 2032, 5081, 4078, 2030, 3007]\n","Feet\n","[4078, 4078, 3089, 5079, 5078, 2032, 2031, 3008]\n","Mis\n","[5079, 3088, 4082, 5078, 2036, 4078, 3092, 2008]\n","Hand\n","[5082, 5078, 3091, 3089, 2030, 2031, 4081, 4008]\n","hi\n","Feet\n","[3089, 4081, 2030, 3090, 5077, 2034, 4082, 5008]\n","Mis\n","[4080, 2030, 5078, 5078, 2036, 3088, 3089, 4007]\n","Hand\n","[2035, 5077, 5078, 4077, 4081, 2031, 3091, 3008]\n","Tongue\n","[2032, 3092, 3092, 4078, 5076, 5079, 4082, 2008]\n","hi\n","Hand\n","[4082, 2033, 2032, 5078, 3090, 4079, 5077, 3007]\n","Feet\n","[5078, 3088, 4078, 2030, 3091, 5082, 2032, 4007]\n","Tongue\n","[3085, 4079, 5076, 3094, 2029, 5078, 4078, 2007]\n","Mis\n","[2032, 2032, 3088, 3089, 4077, 5081, 5079, 4008]\n","hi\n","Tongue\n","[3089, 5077, 4077, 2032, 5077, 4079, 2031, 3008]\n","Mis\n","[4077, 4078, 3089, 5162, 5079, 2131, 2027, 3007]\n","Hand\n","[5109, 3095, 4111, 5081, 2063, 4081, 3135, 2006]\n","Feet\n","[5077, 5111, 3089, 3119, 2027, 2043, 4078, 4007]\n","hi\n","Mis\n","[4080, 2036, 2066, 3090, 5116, 4078, 3177, 5008]\n","Feet\n","[3130, 4082, 4088, 5132, 2029, 3144, 5079, 2007]\n","Hand\n","[5079, 3136, 2026, 4082, 3133, 2026, 4182, 5008]\n","Tongue\n","[2060, 5197, 5094, 2044, 3155, 3106, 4113, 4007]\n","hi\n","Feet\n","[4096, 3106, 3112, 2041, 2041, 4089, 5094, 5007]\n","Hand\n","[5089, 4087, 4091, 2045, 3106, 5092, 2060, 3007]\n","Tongue\n","[2043, 5086, 4088, 3101, 5085, 4171, 3176, 2007]\n","Mis\n","[3100, 2044, 2043, 4091, 4089, 3096, 5087, 5008]\n","hi\n","Hand\n","[4088, 4085, 2040, 2038, 4096, 3096, 5088, 5007]\n","Tongue\n","[5085, 3098, 2039, 5089, 4090, 2037, 3100, 4007]\n","Mis\n","[2029, 5084, 4038, 4084, 3099, 3093, 5084, 2007]\n","Feet\n","[4090, 2044, 5089, 4087, 3093, 5037, 2039, 3006]\n"]}],"source":["for b in range(7):\n","    extra_samples_block_counter(data_dict_3[b],trial_order[b],b)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMHUhd3A17lB+9o7HFw3Jl4","provenance":[]},"kernelspec":{"display_name":"kernel2","language":"python","name":"kernel2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
