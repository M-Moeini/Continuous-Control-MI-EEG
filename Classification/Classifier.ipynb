{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696289517013,
     "user": {
      "displayName": "Mahdi Moeini",
      "userId": "03671813669356560168"
     },
     "user_tz": -210
    },
    "id": "za0kvkt7u2Z5",
    "outputId": "78ba6e7e-7a2c-4096-e595-beca84ab98ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi146/jupyter2/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import mne\n",
    "import scipy.io as sp\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "from mne.decoding import CSP\n",
    "import pymrmr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import hamming\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import blackman\n",
    "from scipy.signal import kaiser\n",
    "from scipy.signal import gaussian\n",
    "from sklearn.decomposition import FastICA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pyriemann.estimation import Covariances\n",
    "# from pyriemann.tangentspace import TangentSpace\n",
    "# from pyriemann.classification import MDM\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import medfilt\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "# Set display options for NumPy\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_TIME_LENGTH = 4\n",
    "SAMPLING_RATE = 250\n",
    "NUMBER_OF_CHANNELS = 64\n",
    "beta = 1.5\n",
    "\n",
    "epoch_length = 1000\n",
    "sampling_freq = 250\n",
    "number_of_runs = 10\n",
    "number_of_components = 10\n",
    "number_of_selected_features = 10\n",
    "number_of_processes = 10\n",
    "number_of_bands = 9\n",
    "column_names = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block']\n",
    "column_names_v2 = ['participant', 'class1', 'class2','running_time','test_acc','train_acc','test_size','train_size','train_block','test_block','test_acc_vote']\n",
    "\n",
    "\n",
    "trial_order=[['Tongue','Feet','Mis','Hand'],\n",
    "            ['Feet','Mis','Hand','Tongue'],\n",
    "            ['Hand','Feet','Tongue','Mis'],\n",
    "            ['Tongue','Mis','Hand','Feet'],\n",
    "            ['Mis','Feet','Hand','Tongue'],\n",
    "            ['Feet','Hand','Tongue','Mis'],\n",
    "            ['Hand','Tongue','Mis','Feet'],\n",
    "            ['Tongue','Feet','Mis','Hand'],\n",
    "            ['Mis','Tongue','Hand','Feet']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_rest_times(b_num):\n",
    "    if b_num == 0:\n",
    "        task_time = [[12, 16, 20, 8],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "        rest_time = [[20, 8, 16, 12],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [12, 20, 16, 8],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "    elif b_num == 1:\n",
    "        task_time = [[12, 8, 20, 16],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [8, 20, 16, 12],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "        rest_time = [[16, 12, 8, 20],\n",
    "                    [8, 20, 12, 16],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [12, 16, 20, 8]]\n",
    "        \n",
    "    elif b_num == 2:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [8, 12, 16, 20]]\n",
    "        \n",
    "        rest_time = [[8, 20, 16, 12],\n",
    "                    [12, 8, 20, 16],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "    elif b_num == 3:\n",
    "        task_time = [[12, 16, 20, 8],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "        rest_time = [[20, 8, 16, 12],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [12, 20, 16, 8],\n",
    "                    [20, 12, 8, 16]]\n",
    "        \n",
    "    elif b_num == 4:\n",
    "        task_time = [[16, 8, 20, 12],\n",
    "                    [12, 16, 8, 20],\n",
    "                    [20, 8, 12, 16],\n",
    "                    [8, 20, 12, 16]]\n",
    "        \n",
    "        rest_time = [[8, 12, 16, 20],\n",
    "                    [16, 20, 12, 8],\n",
    "                    [12, 16, 8, 20],\n",
    "                    [20, 8, 12, 16]]\n",
    "        \n",
    "    elif b_num == 5:\n",
    "        task_time = [[16, 12, 8, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [8, 16, 20, 12],\n",
    "                    [12, 8, 16, 20]]\n",
    "\n",
    "        rest_time = [[12, 8, 16, 20],\n",
    "                    [16, 8, 20, 12],\n",
    "                    [20, 12, 16, 8],\n",
    "                    [8, 16, 12, 20]]\n",
    "        \n",
    "    elif b_num == 6:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 8, 16, 12],\n",
    "                    [8, 16, 12, 20],\n",
    "                    [16, 20, 12, 8]]\n",
    "\n",
    "        rest_time = [[16, 8, 12, 20],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [8, 16, 20, 12]]     \n",
    "    elif b_num ==7:\n",
    "        task_time = [[12, 8, 20, 16],\n",
    "                    [16, 20, 8, 12],\n",
    "                    [8, 20, 16, 12],\n",
    "                    [8, 12, 20, 16]]   \n",
    "               \n",
    "        rest_time = [[16, 12, 8, 20],\n",
    "                    [8, 20, 12, 16],\n",
    "                    [20, 16, 8, 12],\n",
    "                    [12, 16, 20, 8]]  \n",
    "    \n",
    "    elif b_num == 8:\n",
    "        task_time = [[16, 8, 12, 20],\n",
    "                    [20, 16, 12, 8],\n",
    "                    [12, 20, 8, 16],\n",
    "                    [8, 12, 16, 20]]\n",
    "        \n",
    "        rest_time = [[8, 20, 16, 12],\n",
    "                    [12, 8, 20, 16],\n",
    "                    [16, 12, 20, 8],\n",
    "                    [8, 12, 20, 16]]\n",
    "        \n",
    "    else:\n",
    "        raise(\"Error in block number\")\n",
    "    \n",
    "\n",
    "    return task_time,rest_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_times_genertor(task_times,rest_times):\n",
    "    block_times = [item for pair in zip(task_times, rest_times) for item in pair]\n",
    "    return block_times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_zeros_with_average(matrix):\n",
    "    # Iterate through the matrix\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            for k in range(matrix.shape[2]):\n",
    "                if matrix[i, j, k] == 0:\n",
    "                    # Find the neighboring non-zero elements\n",
    "                    neighbors = []\n",
    "                    if i > 0 and matrix[i - 1, j, k] != 0:\n",
    "                        neighbors.append(matrix[i - 1, j, k])\n",
    "                    if i < matrix.shape[0] - 1 and matrix[i + 1, j, k] != 0:\n",
    "                        neighbors.append(matrix[i + 1, j, k])\n",
    "                    if j > 0 and matrix[i, j - 1, k] != 0:\n",
    "                        neighbors.append(matrix[i, j - 1, k])\n",
    "                    if j < matrix.shape[1] - 1 and matrix[i, j + 1, k] != 0:\n",
    "                        neighbors.append(matrix[i, j + 1, k])\n",
    "                    if k > 0 and matrix[i, j, k - 1] != 0:\n",
    "                        neighbors.append(matrix[i, j, k - 1])\n",
    "                    if k < matrix.shape[2] - 1 and matrix[i, j, k + 1] != 0:\n",
    "                        neighbors.append(matrix[i, j, k + 1])\n",
    "\n",
    "                    # Fill the zero with the average of neighboring non-zero values\n",
    "                    if neighbors:\n",
    "                        matrix[i, j, k] = sum(neighbors) / len(neighbors)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_csp(x_train, y_train, x_test,number_of_components):\n",
    "    # csp = CSP(n_components=number_of_components, reg='ledoit_wolf', log=True)\n",
    "    csp = CSP(number_of_components)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # x_train = fill_zeros_with_average(x_train)\n",
    "    # x_train = np.add(x_train, 0.000001)\n",
    "\n",
    "\n",
    "\n",
    "    # nan_count = np.isnan(x_train).sum()\n",
    "    # print(\"Number of NaN values:\", nan_count)\n",
    "\n",
    "    # empty_field_count = np.count_nonzero(x_train == 0)\n",
    "    # print(\"Number of empty fields:\", empty_field_count)\n",
    "\n",
    "    # zeros_locations_3d = np.where(x_train == 0)\n",
    "    # print(\"Locations of zeros:\", zeros_locations)\n",
    "    \n",
    "# Printing indices and corresponding values\n",
    "    # for depth_idx, row_idx, col_idx in zip(zeros_locations_3d[0], zeros_locations_3d[1], zeros_locations_3d[2]):\n",
    "    #     value_at_zero_location = x_train[depth_idx, row_idx, col_idx]\n",
    "    #     print(f\"Zero found at position ({depth_idx}, {row_idx}, {col_idx}) with value {value_at_zero_location}\")\n",
    "\n",
    "\n",
    "    csp_fit = csp.fit(x_train, y_train)\n",
    "    train_feat = csp_fit.transform(x_train)\n",
    "    test_feat = csp_fit.transform(x_test)\n",
    "    return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_extractor(number_of_epochs, class_1, class_2, data, labels):\n",
    "    size = sum(labels[:,0] == class_1) + sum(labels[:,0] == class_2)\n",
    "    Final_labels = np.zeros((size,1)).astype(int)\n",
    "    dataset = np.zeros((size,num_channels, epoch_length))\n",
    "    index = 0\n",
    "    for i in range(number_of_epochs):\n",
    "        if labels[i,0] == class_1 or labels[i,0] == class_2:\n",
    "            dataset[index,:,:] = data[i,:,:]\n",
    "            Final_labels[index,0] = labels[i,0]\n",
    "            index = index + 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return dataset, Final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(dataset, labels, number_of_bands, test_data,number_of_components):\n",
    "\n",
    "    low_cutoff = 0\n",
    "    \n",
    "    for b in range(number_of_bands):\n",
    "        logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "        low_cutoff += 4\n",
    "        data = dataset.copy()\n",
    "        data_test = test_data.copy() \n",
    "        print(\"Frequency range: \",low_cutoff)\n",
    "        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "        filtered_data_test = mne.filter.filter_data(test_data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, n_jobs = 4)\n",
    "\n",
    "        #PCA\n",
    "        # from mne.decoding import UnsupervisedSpatialFilter\n",
    "        # from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "        # pca = UnsupervisedSpatialFilter(PCA(64), average=False)\n",
    "        # pca_fit = pca.fit(filtered_data)\n",
    "        # filtered_data = pca_fit.transform(filtered_data)\n",
    "        # filtered_data_test = pca_fit.transform(filtered_data_test)\n",
    "        # train_feats = filtered_data\n",
    "        # test_feats = filtered_data_test\n",
    "\n",
    "        # filtered_data = data\n",
    "        # filtered_data_test = data_test\n",
    "        \n",
    "        [train_feats, test_feats] = calc_csp(filtered_data, labels[:,0], filtered_data_test,number_of_components)\n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(train_features, labels, number_of_selected_features):\n",
    "    X = pd.DataFrame(train_features)\n",
    "    y = pd.DataFrame(labels)\n",
    "    K = number_of_selected_features\n",
    "    \n",
    "    df = pd.concat([y,X], axis = 1)\n",
    "    df.columns = df.columns.astype(str)\n",
    "        \n",
    "    selected_features = list(map(int, pymrmr.mRMR(df, 'MID', K)))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(path,p_num,block_list):\n",
    "    data_dict = {}\n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "        df = pd.DataFrame(mat['Data'])\n",
    "        data_dict[b_num] = df\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_start_indices(dataframe):\n",
    "    group_indices = []\n",
    "    current_label = None\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        if row.iloc[-1] != current_label:\n",
    "            group_indices.append(idx)\n",
    "            current_label = row.iloc[-1]\n",
    "\n",
    "    return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_samples_counter(df,class_1,class_2):\n",
    "    x=0\n",
    "    i=0\n",
    "    sampleList = []\n",
    "    while i<len(df):\n",
    "        if (df.iloc[i,-1]==class_1):\n",
    "            x+=1\n",
    "        else:\n",
    "            i-=1\n",
    "            sampleList.append(x)\n",
    "            x=0\n",
    "            class_1,class_2 = class_2,class_1\n",
    "        i+=1\n",
    "    sampleList.append(x)\n",
    "    print(sampleList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_samples_block_counter(df,trial_order,b_num):\n",
    "\n",
    "    df.drop(df[df.iloc[:,-1].isin(['Begin', 'End'])].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)    \n",
    "    df['group'] = (df.iloc[:,-1] != df.iloc[:,-1].shift(1)).cumsum()\n",
    "\n",
    "    \n",
    "    group_counts_Rest = df[df.iloc[:,-1] == 'Rest'].groupby('group').size()\n",
    "    with open('sampleList.txt', 'a') as file:\n",
    "        file.write(f'block {b_num+1} '+'\\n')\n",
    "        for j in range (len(trial_order)):\n",
    "            print(trial_order[j])\n",
    "            trial_num = j\n",
    "            task_times,rest_times = get_task_rest_times(b_num)\n",
    "            trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n",
    "            trial_samples = [item*SAMPLING_RATE for item in trial_times]\n",
    "            group_counts_task = df[df.iloc[:,-1] == trial_order[j]].groupby('group').size()\n",
    "            sampleList = []\n",
    "            for i in range(4):\n",
    "                task = group_counts_task.iloc[i]\n",
    "                rest = group_counts_Rest.iloc[4*j+i]\n",
    "                sampleList.append(task)\n",
    "                sampleList.append(rest)\n",
    "            # extra_samples = [x-y for x,y in zip(sampleList,trial_samples)]\n",
    "            file.write(', '.join(map(str, sampleList)) + f' trial={trial_order[j]} '+'\\n')\n",
    "            print(sampleList)\n",
    "        file.write('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(df,class_1,class_2,tasks_time):\n",
    "    # sys.exit() \n",
    "    class_x = class_1\n",
    "    class_y = class_2\n",
    "    new_df = pd.DataFrame()\n",
    "    trial_df = df.copy() \n",
    "    # print(tasks_time)\n",
    "    for i in range(len(tasks_time)):\n",
    "        sample_point = tasks_time[i]*SAMPLING_RATE\n",
    "        if(trial_df.iloc[sample_point+1,-1] == class_x ):\n",
    "            if(i==len(tasks_time)-1):\n",
    "                temp_df = trial_df.iloc[:sample_point,:]\n",
    "                new_df = pd.concat([new_df, temp_df], axis=0)\n",
    "                new_df.reset_index(drop=True, inplace=True)\n",
    "            else:    \n",
    "                temp_df = trial_df.iloc[:sample_point,:]\n",
    "                next_task_idx = trial_df[trial_df.iloc[:, -1] == class_y].index\n",
    "                trial_df.drop(trial_df.index[0:next_task_idx[0]], inplace=True)\n",
    "                trial_df.reset_index(drop=True, inplace=True)\n",
    "                new_df = pd.concat([new_df, temp_df], axis=0)\n",
    "                new_df.reset_index(drop=True, inplace=True)\n",
    "                class_x,class_y = class_y,class_x\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_seperator(cleaned_df,class_1,class_2):\n",
    "    df = cleaned_df\n",
    "    sorting_order = {class_1: 0, class_2: 1}\n",
    "\n",
    "    df['sorting_order'] = df.iloc[:, -1].map(sorting_order)\n",
    "    df.sort_values(by=['sorting_order', df.columns[-1]], inplace=True)\n",
    "    df.drop('sorting_order', axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(dataset,labels):\n",
    "    print(dataset.shape)\n",
    "    print(labels.shape)\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    shuffled_dataset = dataset[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    return shuffled_dataset,shuffled_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_epoch(df_len,sliding_len,window_len):\n",
    "    # print(window_len,sliding_len,df_len)\n",
    "    number_of_epochs = int((int(df_len-window_len)/sliding_len)) +1\n",
    "    return number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label_attacher(cleaned_df,class_1,class_2,random_flag,class_seperator_flag,sliding_time,window_time_length,window_type,number_of_channels):\n",
    "    SLIDING_POINTS = int(sliding_time*SAMPLING_RATE)\n",
    "    window_time = window_time_length\n",
    "    WINDOW_SAMPLE_LENGTH = window_time*SAMPLING_RATE\n",
    "    new_df_ = cleaned_df.copy()\n",
    "    new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    X = new_df_.to_numpy()\n",
    "    X = np.transpose(X)\n",
    "    number_of_epochs = cal_epoch(int(int(len(cleaned_df)/SAMPLING_RATE)),sliding_time,window_time)\n",
    "    # print(number_of_epochs)\n",
    "    dataset = np.zeros((number_of_epochs,number_of_channels,WINDOW_SAMPLE_LENGTH))\n",
    "    labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "    index = get_group_start_indices(cleaned_df)\n",
    "    index.append(len(cleaned_df))\n",
    "    k = 0  \n",
    "    startIdx = int(k * WINDOW_SAMPLE_LENGTH)\n",
    "    endIdx = int((k+1) * WINDOW_SAMPLE_LENGTH )\n",
    "    l = 0\n",
    "    label = 1\n",
    "    for i in range(number_of_epochs):\n",
    "        \n",
    "        if(startIdx>=index[l] and endIdx<=index[l+1]):\n",
    "            # print(startIdx,endIdx,index[l],index[l+1],\"start, end, index[l], index[l+1] in if\")\n",
    "            slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "            if window_type == \"Kaiser\":\n",
    "                kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "                slice_X *= kaiser_window\n",
    "\n",
    "            elif window_type == \"Hamming\":\n",
    "                hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hamming_window\n",
    "            \n",
    "            elif window_type == \"Hanning\":\n",
    "                hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hanning_window\n",
    "            \n",
    "            elif window_type == \"Rec\":\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Window type is wrong!\")\n",
    "\n",
    "            dataset[i, :, :] = slice_X\n",
    "            labels[i,0] = label\n",
    "            # print(\"i is: \",i)\n",
    "            # print(\"label is: \",label)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            temp = endIdx-index[l+1]\n",
    "            # print(temp,endIdx,index[l+1],\"temp,end,index l+1\")\n",
    "            slice_X = X[:, startIdx:endIdx]\n",
    "            if window_type == \"Kaiser\":\n",
    "                kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "                slice_X *= kaiser_window\n",
    "\n",
    "            elif window_type == \"Hamming\":\n",
    "                hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hamming_window\n",
    "            \n",
    "            elif window_type == \"Hanning\":\n",
    "                hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "                slice_X *= hanning_window\n",
    "            \n",
    "            elif window_type == \"Rec\":\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Window type is wrong!\")\n",
    "            dataset[i, :, :] = slice_X\n",
    "\n",
    "            if(temp<WINDOW_SAMPLE_LENGTH/2):\n",
    "                # print(\"i is: \",i)\n",
    "                # print(\"label is: \",label)\n",
    "                labels[i,0] = label\n",
    "            else:\n",
    "                labels[i,0] = int(not(label))\n",
    "                # print(\"i is: \",i)\n",
    "                # print(\"label is: \",int(not(label)))\n",
    "\n",
    "            if(startIdx>=index[l+1]):\n",
    "                l+=1\n",
    "                # print(f\"label changed in i = {i}\")\n",
    "                label = int(not(label))\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        startIdx+=SLIDING_POINTS\n",
    "        endIdx+=SLIDING_POINTS\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # a = df_len - wdinow_len\n",
    "        # a/sliding_len\n",
    "        # b = a%sliding_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "    # new_df_ = cleaned_df.copy()\n",
    "    # new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    # X = new_df_.to_numpy()\n",
    "    # X = np.transpose(X)\n",
    "    # number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n",
    "    # number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/SLIDING_POINTS) +1\n",
    "\n",
    "    \n",
    "    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n",
    "    # labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # #Initialization\n",
    "    # if class_seperator_flag:\n",
    "    #     seperated_class_df = class_seperator(cleaned_df,class_1,class_2)\n",
    "    #     new_df_ = seperated_class_df.copy()\n",
    "    #     new_df_.drop(seperated_class_df.columns[-1], axis=1, inplace=True)\n",
    "    #     X = new_df_.to_numpy()\n",
    "    #     X = np.transpose(X)\n",
    "    #     empty_field_count = np.count_nonzero(X == 0)\n",
    "    #     print(\"Number of empty fields in X:\", empty_field_count)\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"befor filling\",len(zero_indices[0]))\n",
    "    #     # X[zero_indices] += 0.001\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"after filling\",len(zero_indices[0]))\n",
    "    #     number_of_epochs = int((int(len(new_df_))-WINDOW_SAMPLE_LENGTH)/TR_SLIDING_POINTS)\n",
    "    #     print(number_of_epochs)\n",
    "    # else :  \n",
    "    #     new_df_ = cleaned_df.copy()\n",
    "    #     new_df_.drop(cleaned_df.columns[-1], axis=1, inplace=True)\n",
    "    #     X = new_df_.to_numpy()\n",
    "    #     X = np.transpose(X)\n",
    "    #     empty_field_count = np.count_nonzero(X == 0)\n",
    "    #     print(\"Number of empty fields in X:\", empty_field_count)\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"befor filling\",len(zero_indices[0]))\n",
    "    #     # X[zero_indices] += 0.001\n",
    "    #     # zero_indices = np.where(X == 0)\n",
    "    #     # print(\"after filling\",len(zero_indices[0]))\n",
    "\n",
    "    #     number_of_epochs = int(len(new_df_)/WINDOW_SAMPLE_LENGTH)\n",
    "\n",
    "    # dataset = np.zeros((number_of_epochs,NUMBER_OF_CHANNELS,WINDOW_SAMPLE_LENGTH))\n",
    "    # labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "    # if class_seperator_flag:\n",
    "    #     i = 0  \n",
    "    #     startIdx = i * WINDOW_SAMPLE_LENGTH\n",
    "    #     endIdx = (i+1) * WINDOW_SAMPLE_LENGTH \n",
    "    #     while(endIdx<=int(len(new_df_))/2):\n",
    "    #         slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "\n",
    "    #         dataset[i, :, :] = slice_X\n",
    "    #         labels[i,0] = 0\n",
    "    #         # if (seperated_class_df.iloc[startIdx, 64] == class_1):\n",
    "    #         #     labels[i,0] = 0\n",
    "    #         # elif(seperated_class_df.iloc[startIdx, 64] == class_2):\n",
    "    #         #     labels[i,0] = 1\n",
    "    #         # else:\n",
    "    #         #     labels[i,0] = 2\n",
    "    #         startIdx+=TR_SLIDING_POINTS\n",
    "    #         endIdx+=TR_SLIDING_POINTS\n",
    "    #         i+=1\n",
    "    #     # print(int(len(new_df_))/2,\"len\")    \n",
    "    #     # print(endIdx,\"endIdx\")    \n",
    "    #     # print(seperated_class_df.iloc[endIdx-2:endIdx+2,64])\n",
    "       \n",
    "    #     j = i\n",
    "        \n",
    "    #     startIdx = endIdx-TR_SLIDING_POINTS\n",
    "    #     endIdx = startIdx+WINDOW_SAMPLE_LENGTH\n",
    "    #     print(j, \"j is this\")\n",
    "    #     while(endIdx<=int(len(new_df_))):\n",
    "    #         slice_X = X[:, startIdx:endIdx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "\n",
    "    #         dataset[j, :, :] = slice_X\n",
    "    #         labels[j,0] = 1\n",
    "    #         # if (cleaned_df.iloc[startIdx, 64] == class_1):\n",
    "    #         #     labels[j,0] = 0\n",
    "    #         # elif(cleaned_df.iloc[startIdx, 64] == class_2):\n",
    "    #         #     labels[j,0] = 1\n",
    "    #         # else:\n",
    "    #         #     labels[j,0] = 2\n",
    "    #         startIdx+=TR_SLIDING_POINTS\n",
    "    #         endIdx+=TR_SLIDING_POINTS\n",
    "    #         j+=1\n",
    "    #     print(j, \"j is this\")\n",
    "    #     # dataset,labels = shuffler(dataset,labels)\n",
    "\n",
    "    # else:\n",
    "    #     i = 0  \n",
    "    #     start_idx = i * WINDOW_SAMPLE_LENGTH\n",
    "    #     end_idx = (i+1) * WINDOW_SAMPLE_LENGTH \n",
    "    #     while (end_idx<=int(len(new_df_))):\n",
    "    #         slice_X = X[:, start_idx:end_idx]\n",
    "\n",
    "    #         kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,beta)\n",
    "    #         slice_X *= kaiser_window\n",
    "            \n",
    "    #         dataset[i, :, :] = slice_X\n",
    "    #         if (cleaned_df.iloc[start_idx, 64] == class_1):\n",
    "    #             labels[i,0] = 0\n",
    "    #         elif(cleaned_df.iloc[start_idx, 64] == class_2):\n",
    "    #             labels[i,0] = 1\n",
    "    #         else:\n",
    "    #             labels[i,0] = 2\n",
    "    #         start_idx+=SLIDING_POINTS\n",
    "    #         end_idx+=SLIDING_POINTS\n",
    "    #         i+=1\n",
    "    #     # dataset,labels = shuffler(dataset,labels)\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #For training and test purpose\n",
    "    # if random_flag:\n",
    "    #     randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n",
    "    # else:\n",
    "    #     randomlist = list(range(number_of_epochs))\n",
    "    #Labeling the data\n",
    "\n",
    "\n",
    "\n",
    "    # for i in range(number_of_epochs):\n",
    "    #     start_idx = randomlist[i] * WINDOW_SAMPLE_LENGTH + SLIDING_POINTS\n",
    "    #     end_idx = (randomlist[i] + 1) * WINDOW_SAMPLE_LENGTH\n",
    "    #     slice_X = X[:, start_idx:end_idx]\n",
    "\n",
    "    #     # hamming_window = hamming(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= hamming_window\n",
    "\n",
    "    #     # hanning_window = hann(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= hanning_window\n",
    "\n",
    "    #     # blackman_window = blackman(WINDOW_SAMPLE_LENGTH)\n",
    "    #     # slice_X *= blackman_window\n",
    "\n",
    "    #     # kaiser_window = kaiser(WINDOW_SAMPLE_LENGTH,0.5)\n",
    "    #     # slice_X *= kaiser_window\n",
    "\n",
    "    #     # gaussian_window = gaussian(WINDOW_SAMPLE_LENGTH,0.5)\n",
    "    #     # slice_X *= gaussian_window\n",
    "\n",
    "\n",
    "    #     dataset[i, :, :] = slice_X\n",
    "    #     if (cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_1):\n",
    "    #         labels[i,0] = 0\n",
    "    #     elif(cleaned_df.iloc[randomlist[i] * WINDOW_SAMPLE_LENGTH, 64] == class_2):\n",
    "    #         labels[i,0] = 1\n",
    "    #     else:\n",
    "    #         labels[i,0] = 2\n",
    "    \n",
    "    # empty_field_count = np.count_nonzero(dataset == 0)\n",
    "    # print(\"Number of empty fields in dataset:\", empty_field_count,\"dataset shape\",dataset.shape)\n",
    "    # print(labels)\n",
    "    return dataset,labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_cutter(data, class_1):\n",
    "    df = data.copy()\n",
    "    Begin_trigger = \"Begin\" + \"_\" + class_1\n",
    "    End_trigger = \"End\" + \"_\" + class_1\n",
    "    Begin_idx = df[df.iloc[:, -1] == Begin_trigger].index\n",
    "    End_idx = df[df.iloc[:, -1] == End_trigger].index\n",
    "    trial_df = df.iloc[Begin_idx[0]+1:End_idx[0],:]\n",
    "    trial_df.reset_index(drop=True, inplace=True)\n",
    "    trial_df.head()\n",
    "    return trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Begin_End_trigger_modifier(data):\n",
    "    df = data.copy()\n",
    "    Begin_indexes = df[df.iloc[:, -1] == 'Begin'].index\n",
    "    End_indexes = df[df.iloc[:, -1] == 'End'].index\n",
    "    if(len(Begin_indexes)==len(End_indexes)):\n",
    "        for i in range(len(Begin_indexes)):\n",
    "            index = Begin_indexes[i]+1\n",
    "            val = df.iloc[index,-1]\n",
    "            df.iloc[Begin_indexes[i],-1] = \"Begin\" + \"_\" + str(val)\n",
    "            df.iloc[End_indexes[i],-1]   =  \"End\" + \"_\" + str(val)\n",
    "    else:\n",
    "        raise ValueError(\"Trigger seinding Exception\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(data_,class_1,class_2,tasks_time,set_type,clean_flag,sliding_time,window_time_length,window_type,number_of_channels):\n",
    "    CLASS_1 = class_1\n",
    "    CLASS_2 = class_2\n",
    "    df = data_.copy()\n",
    "    modified_df = Begin_End_trigger_modifier(df)\n",
    "    trial_df = trial_cutter(modified_df,CLASS_1)\n",
    "    # print(trial_df.shape,\"trial_df\")\n",
    "    indexes = get_group_start_indices(trial_df)\n",
    "    # print(indexes,'tasks index starting point')\n",
    "    if clean_flag:\n",
    "        cleaned_df = data_cleaner(trial_df,CLASS_1,CLASS_2,tasks_time)\n",
    "        final_df = cleaned_df.copy()\n",
    "    else:\n",
    "        final_df = trial_df.copy()\n",
    "    # print(final_df.shape,\"final_df\")\n",
    "\n",
    "    if set_type ==\"TRAIN\":\n",
    "        random_flag = True\n",
    "    elif set_type ==\"TEST\":\n",
    "        random_flag = False\n",
    "    else:\n",
    "        raise(\"Error in set type\")\n",
    "\n",
    "  \n",
    "    final_data, final_labels = data_label_attacher(final_df,CLASS_1,CLASS_2,random_flag,clean_flag,sliding_time,window_time_length,window_type,number_of_channels)\n",
    "      \n",
    "    # print(final_data.shape,\"final_data shape\")\n",
    "    # print(final_labels.shape,\"final_labels shape\")\n",
    "    \n",
    "    return final_data,final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trials_set_builder(data_dict,blocks_set,set_label,class_1,class_2,clean_flag,sliding_time,window_time_length,window_type,channels_to_remove,number_of_channels):\n",
    "                                   \n",
    "    counter = 0\n",
    "\n",
    "    for b_num in blocks_set:\n",
    "        trial_num = trial_order[b_num].index(class_1)\n",
    "        task_times,rest_times = get_task_rest_times(b_num)\n",
    "        # print(task_times[trial_num],rest_times[trial_num])\n",
    "        trial_times = trial_times_genertor(task_times[trial_num],rest_times[trial_num])\n",
    "        # print(trial_times)\n",
    "        # print(data_dict[b_num].shape,\"shape befor removal\")\n",
    "\n",
    "        # data = remove_outliers_across_channels(data_dict[b_num],10)\n",
    "        # data = remove_outliers(data_dict[b_num])\n",
    "        data = data_dict[b_num]\n",
    "        # data = apply_median_filter(data,9)\n",
    "        if class_1== 'Tongue' or class_1 == 'Mis':\n",
    "            data = channel_remover(data,channels_to_remove)\n",
    "            number_of_channels =  NUMBER_OF_CHANNELS-len(channels_to_remove)\n",
    "        else:\n",
    "            number_of_channels = NUMBER_OF_CHANNELS\n",
    "        # print(number_of_channels,\"number_of_channels\")\n",
    "        # print(data.shape,\"shape after removal\")\n",
    "        # sys.exit()\n",
    "        df = data.copy()\n",
    "        # last_column = df.pop(df.columns[-1])\n",
    "        # df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "        # eeg_data = df.to_numpy().T  # Transpose to have channels in columns\n",
    "\n",
    "        # channel_names = [f'Ch{i+1}' for i in range(63)]\n",
    "\n",
    "        # # Create MNE-Python RawArray object\n",
    "        # info = mne.create_info(ch_names=channel_names, sfreq=sampling_freq, ch_types='eeg')\n",
    "        # raw = mne.io.RawArray(eeg_data, info)\n",
    "\n",
    "        # # Apply ICA\n",
    "        # ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "        # ica.fit(raw)\n",
    "        # ica_components = ica.get_components()\n",
    "\n",
    "        # # Convert the ICA components to a DataFrame\n",
    "        # df2 = pd.DataFrame(data=ica_components.T, columns=channel_names)\n",
    "        # df2 = df2.assign(LastColumn=last_column)\n",
    "        # # df = data.copy(deep=False)\n",
    "        dataset,labels = preprocessor(df,class_1,class_2,trial_times,set_label,clean_flag,sliding_time,window_time_length,window_type,number_of_channels)\n",
    "        # print(dataset.shape)\n",
    "\n",
    "        if counter == 0 :\n",
    "            final_data = dataset\n",
    "            final_labels = labels\n",
    "            # print(\"Before concatenation - final_data shape:\", final_data.shape, \"dataset shape:\", dataset.shape)\n",
    "        else:\n",
    "            final_data = np.vstack((final_data, dataset))\n",
    "            final_labels = np.vstack((final_labels, labels))\n",
    "            print(\"After concatenation - final_data shape:\", final_data.shape, \"final_labels shape:\", final_labels.shape)\n",
    "\n",
    "        counter+=1 \n",
    "    # empty_field_count = np.count_nonzero(final_data == 0)\n",
    "    # print(\"Number of empty fields in final_data:\", empty_field_count,\"final_data shape\",final_data.shape)\n",
    "    return final_data,final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(data_list):\n",
    "    counted_values = Counter(data_list)\n",
    "    duplicate_values = {value: count for value, count in counted_values.items() if count > 1}\n",
    "    return duplicate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Statistical_analysor(p_num_list,data_dicts_list):\n",
    "\n",
    "    with open('/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Classification/Statistics.txt', 'w') as file:\n",
    "        for p in range(len(p_num_list)):\n",
    "            file.write(f'Particpant: {p+3} '+'\\n')\n",
    "            for b in range(7):\n",
    "                file.write(f'Block: {b+1} '+'\\n')\n",
    "                data_pd = data_dicts_list[p][b]\n",
    "                data = data_pd.iloc[:, :-1]\n",
    "                data_np = data.values\n",
    "                eeg_data = data_np\n",
    "                print(\"Data type:\", type(eeg_data))\n",
    "                print(\"Shape:\", eeg_data.shape)\n",
    "                eeg_data = np.array(eeg_data)\n",
    "                mean_values = np.mean(eeg_data, axis=0)\n",
    "                variance_values = np.var(eeg_data, axis=0)\n",
    "                std_deviation_values = []\n",
    "                \n",
    "                for i in range(num_channels):\n",
    "                    print(f\"Channel {i + 1}:\")\n",
    "                    print(f\"Mean: {mean_values[i]}\")\n",
    "                    print(f\"Variance: {variance_values[i]}\")\n",
    "                    std_deviation_values.append(np.sqrt(variance_values[i]))\n",
    "                    print(f\"Standard Deviation: {std_deviation_values[i]}\")\n",
    "                    print()\n",
    "                    file.write(f'Channel {i+1}: '+'\\n')\n",
    "                    file.write(f\"Mean: {mean_values[i]}\"+\"\\n\")\n",
    "                    file.write(f\"Variance: {variance_values[i]}\"+\"\\n\")\n",
    "                    file.write(f\"Standard Deviation: {std_deviation_values[i]}\"+\"\\n\\n\")\n",
    "                \n",
    "                lists_to_check = {\n",
    "                'mean_values': mean_values,\n",
    "                'variance_values': variance_values,\n",
    "                'std_deviation_values': std_deviation_values\n",
    "                }\n",
    "                for list_name, data_list in lists_to_check.items():\n",
    "                    duplicate_values = find_duplicates(data_list)\n",
    "                    if duplicate_values:\n",
    "                        print(f\"Duplicate values and their counts for {list_name}:\")\n",
    "                        file.write(f\"Duplicate values and their counts for {list_name}:\"+\"\\n\")\n",
    "                        for value, count in duplicate_values.items():\n",
    "                            print(f\"Value: {value}, Count: {count}\")\n",
    "                            file.write(f\"Value: {value}, Count: {count}\"+\"\\n\")\n",
    "                    else:\n",
    "                        print(f\"No duplicate values found in the {list_name} list.\")\n",
    "                        file.write(f\"No duplicate values found in the {list_name} list.\"+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(y_true, y_pred):\n",
    "    mismatches = []\n",
    "    total = len(y_true)\n",
    "    mismatch_count = 0\n",
    "    \n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            mismatches.append(i)\n",
    "            mismatch_count += 1\n",
    "            \n",
    "    accuracy = 1 - (mismatch_count / total)\n",
    "    \n",
    "    return accuracy, mismatch_count, mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_next(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list) - window_size + 1):\n",
    "        window = prediction_list[i:i+window_size]\n",
    "        window_tuple = tuple(window)\n",
    "        counts = Counter(window_tuple)\n",
    "        majority = counts.most_common(1)[0][0]\n",
    "        majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_prev(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        if i >= window_size - 1:\n",
    "            start_index = i - window_size + 1\n",
    "            window = prediction_list[start_index:i+1]\n",
    "            counts = Counter(window)\n",
    "            majority = counts.most_common(1)[0][0]\n",
    "            majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_sliding_with_prev_v2(prediction_list, window_size=3):\n",
    "    majority_votes = []\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        start_index = max(0, i - window_size + 1)\n",
    "        window = prediction_list[start_index:i+1]\n",
    "        counts = Counter(window)\n",
    "        majority = counts.most_common(1)[0][0]\n",
    "        majority_votes.append(majority)\n",
    "        \n",
    "    return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_remover(df, channels):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop(df.columns[channels], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(new_row, path):\n",
    "\n",
    "    absolute_path = os.path.abspath(path)\n",
    "    try:\n",
    "        rf = pd.read_csv(absolute_path)\n",
    "    except FileNotFoundError:\n",
    "        rf = pd.DataFrame(columns=column_names_v2)\n",
    "\n",
    "    new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "    cf.to_csv(absolute_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_average(path,p_num_list,class_list):\n",
    "    PATH = path\n",
    "    vf = pd.DataFrame(columns=column_names_v2) \n",
    "    for p_num in p_num_list:\n",
    "        print(p_num)\n",
    "        rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n",
    "        vf = pd.concat([vf, rf], ignore_index=True)\n",
    "    vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n",
    "\n",
    "    columnNames = ['class','test_acc','vote_acc']\n",
    "    kf = pd.DataFrame(columns=columnNames)\n",
    "    kf.to_csv(PATH+'AverageAcc.csv',index=False)\n",
    "    vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n",
    "    df = vf\n",
    "    blk_list = [1234]\n",
    "    for class_ in class_list:\n",
    "        gf = df[(df['class1'] == class_)]\n",
    "        avg_test = gf['test_acc'].mean()\n",
    "        avg_vote = gf['test_acc_vote'].mean() \n",
    "        new_row = [class_, avg_test,avg_vote] \n",
    "        new_row_df = pd.DataFrame([new_row], columns=columnNames)\n",
    "        rf = pd.read_csv(PATH + 'AverageAcc.csv')\n",
    "        cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "        cf.to_csv(PATH +'AverageAcc.csv',index=False)  \n",
    "    kf = pd.read_csv(PATH +'AverageAcc.csv') \n",
    "    print(kf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/'\n",
    "def pickle_reader(path, p_num, block_list):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        file_path = os.path.join(path, f\"P{p_num}/P{p_num}B{b_num}.pkl\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "\n",
    "                data_dict[b_num] = data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {file_path}: {e}\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Reading 14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/Pickels_Participants/'\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "block_list = [0,1,2,3,4,5,6]\n",
    "data_dict_list = []\n",
    "for p_num in p_num_list:\n",
    "    print(\"Reading\",p_num)\n",
    "    data_dict = pickle_reader(path,p_num,block_list)\n",
    "    data_dict_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113998, 65)\n"
     ]
    }
   ],
   "source": [
    "print(data_dict_list[4][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mat_reader\n",
    "\n",
    "\n",
    "# block_list = [0,1,2,3,4,5,6]\n",
    "# p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "# data_dicts_list = []\n",
    "# for p_num in p_num_list:\n",
    "#     print(f'reading P{p_num}')\n",
    "#     data_dict = data_reader(f'../../Participants/P{p_num}/', p_num, block_list)\n",
    "#     data_dicts_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_list = [0,1,2,3,4,5,6]\n",
    "# p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "# data_dicts_list = []\n",
    "# for p_num in p_num_list:\n",
    "#     print(f'reading P{p_num}')\n",
    "#     data_dict = data_reader(f'/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/P{p_num}/',p_num,block_list)\n",
    "#     data_dicts_list.append(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runs_iterattor(runs,clf,train_features,Y_tr,test_features,Y_te,test_acc_list,train_acc_list,vote_acc_list):\n",
    "\n",
    "#     for r in range(runs):\n",
    "#         clf.fit(train_features, Y_tr)\n",
    "        \n",
    "#         y_pr_te = clf.predict(test_features)\n",
    "#         y_pr_tr = clf.predict(train_features)\n",
    "\n",
    "#         accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#         test_acc_list.append(accuracy_te)\n",
    "\n",
    "#         accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#         train_acc_list.append(accuracy_tr)\n",
    "\n",
    "#         y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "#         Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "#         vote_acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "#         vote_acc_list.append(vote_acc)\n",
    "#     return train_acc_list,test_acc_list,vote_acc_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def class_iterator(class_1_list,data_dicts_list,train_blk_set,test_blocks_set,class_2,clean_flag,sliding_time_tr,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels,p,p_num,train_blk_name,test_blk_name,path):\n",
    "\n",
    "#     for class_1 in class_1_list:\n",
    "#         import time\n",
    "#         start_time = time.time()\n",
    "#         X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blk_set,'TRAIN',class_1,class_2,clean_flag,sliding_time_tr,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "#         X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blocks_set,'TEST',class_1,class_2,clean_flag,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "\n",
    "#         print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "#         print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "#         [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te,number_of_components)\n",
    "#         print(train_features.shape, \"train_features shape\")\n",
    "#         print(test_features.shape, \"test_features shape\")\n",
    "#         selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "\n",
    "#         clf = classifier\n",
    "#         runs = 1\n",
    "#         train_acc_list = []\n",
    "#         test_acc_list = []\n",
    "#         vote_acc_list = []\n",
    "\n",
    "#         train_acc_list,test_acc_list,vote_acc_list = runs_iterattor(runs,clf,train_features[:,selected_features],Y_tr[:,0],test_features[:, selected_features],Y_te,test_acc_list,train_acc_list,vote_acc_list)\n",
    "        \n",
    "\n",
    "#         end_time = time.time()\n",
    "#         running_time = end_time-start_time\n",
    "#         participant = p_num\n",
    "#         class1 = class_1\n",
    "#         class2 = class_2\n",
    "#         running_time = running_time\n",
    "#         test_acc = np.average(test_acc_list)\n",
    "#         train_acc = np.average(train_acc_list)\n",
    "#         vote_acc = np.average(vote_acc_list)\n",
    "#         test_size = X_te.shape\n",
    "#         train_size = X_tr.shape\n",
    "#         train_block = train_blk_name\n",
    "#         test_block = test_blk_name\n",
    "#         new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,vote_acc]\n",
    "\n",
    "#         save_csv(new_row,path)\n",
    "\n",
    "\n",
    "#         print(train_acc_list,\"train\",class_1)\n",
    "#         print(test_acc_list,\"test\",class_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/'\n",
    "# class_1_list = ['Hand','Feet','Tongue','Mis']\n",
    "# class_2 = 'Rest'\n",
    "# p_num_list = [9]\n",
    "# train_blocks_set = [0,1,2,3,4]\n",
    "# test_blocks_set = [5,6]\n",
    "# window_time_length = 4\n",
    "# sliding_time_tr = 4\n",
    "# sliding_time_te = 4\n",
    "# vote_window = 4\n",
    "# number_of_selected_features = 10\n",
    "# # channels_to_remove = [list(range(1,56))]\n",
    "# channels_to_remove = []\n",
    "# number_of_channels = NUMBER_OF_CHANNELS\n",
    "# params = {\n",
    "#     'max_depth': 5,\n",
    "#     'min_child_weight': 1,\n",
    "#     'gamma': 0,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'learning_rate': 0.1,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "def classifier(data_dicts_list):\n",
    "    # classifier_dic = {\"XGB\":XGBClassifier(),\"LDA\":LDA(),\"RF\":RF()}\n",
    "    # number_of_components_list = [10,12,14,8]\n",
    "    # number_of_selected_features_list = [10,15,20]\n",
    "    # overlapflag = False\n",
    "    # window_time_length_list = [4,2,1]\n",
    "    # window_type_list = [\"Rec\",\"Kaiser\",\"Hanning\",\"Hamming\"]\n",
    "    # train_blk_set_dic = {\"01234\":[0,1,2,3,4],\"0123\":[0,1,2,3],,\"012\":[0,1,2],,\"01\":[0,1],\"0\":[0]}\n",
    "\n",
    "    classifier_dic = {\"XGB\":XGBClassifier(),\"LDA\":LDA()}\n",
    "    number_of_components_list = [10]\n",
    "    number_of_selected_features_list = [10,2]\n",
    "    overlap_percent_list = [100,25]\n",
    "    channels_to_remove = []\n",
    "    number_of_channels = NUMBER_OF_CHANNELS\n",
    "    # overlapflag = \"Without_Overlap\"\n",
    "    window_time_length_list = [4]\n",
    "    sliding_time_tr = 4\n",
    "    sliding_time_te = 4\n",
    "    vote_window = 3\n",
    "    window_type_list = [\"Rec\",\"Kaiser\"]\n",
    "    train_blk_set_dic = {\"12345\":[0,1,2,3,4],\"1234\":[0,1,2,3],\"123\":[0,1,2],\"12\":[0,1],\"1\":[0]}\n",
    "    test_blk_set_dic = {\"67\":[5,6],\"567\":[4,5,6],\"4567\":[3,4,5,6],\"34567\":[2,3,4,5,6],\"234567\":[1,2,3,4,5,6]}\n",
    "    # train_blk_set_dic = {\"12345\":[0,1,2,3,4]}\n",
    "    # test_blk_set_dic = {\"67\":[5,6]}\n",
    "    class_1_list = ['Hand','Feet','Tongue','Mis']\n",
    "    class_2 = 'Rest'\n",
    "    p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "    PATH = \"/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results-v3/\"\n",
    "\n",
    "    for classifier, classifier_name in zip(classifier_dic.values(),classifier_dic.keys()):\n",
    "        for number_of_components in number_of_components_list:\n",
    "            for number_of_selected_features in number_of_selected_features_list:\n",
    "                    for overlap_percent in overlap_percent_list:                  \n",
    "                        for window_time_length in window_time_length_list:\n",
    "                            sliding_time_tr = sliding_time_te = window_time_length*overlap_percent/100\n",
    "                            if overlap_percent != 100:\n",
    "                                window_type_list = [\"Rec\"]  \n",
    "                            else:\n",
    "                                window_type_list = [\"Rec\",\"Kaiser\"]\n",
    "\n",
    "                            for window_type in window_type_list:\n",
    "                                for train_blk_set,train_blk_name,test_blk_set,test_blk_name in zip(train_blk_set_dic.values(),train_blk_set_dic.keys(),test_blk_set_dic.values(),test_blk_set_dic.keys()):\n",
    "                                    p = 0\n",
    "                                    for p_num in p_num_list:\n",
    "                                        for class_1 in class_1_list:\n",
    "                                            import time\n",
    "                                            start_time = time.time()\n",
    "                                            X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blk_set,'TRAIN',class_1,class_2,True,sliding_time_tr,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "                                            X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blk_set,'TEST',class_1,class_2,True,sliding_time_te,window_time_length,window_type,channels_to_remove,number_of_channels)\n",
    "\n",
    "                                            print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "                                            print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "                                            [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te,number_of_components)\n",
    "                                            # [train_features, test_features] =  apply_pca(X_tr,X_te,10)\n",
    "                                            print(train_features.shape, \"train_features shape\")\n",
    "                                            print(test_features.shape, \"test_features shape\")\n",
    "                                            selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "                                \n",
    "                                            clf = classifier\n",
    "                                            runs = 1\n",
    "                                            train_acc_list = []\n",
    "                                            test_acc_list = []\n",
    "                                            vote_acc_list = []\n",
    "\n",
    "                                            for r in range(runs):\n",
    "                                                clf.fit(train_features[:, selected_features], Y_tr[:,0])\n",
    "                                                \n",
    "                                                y_pr_te = clf.predict(test_features[:, selected_features])\n",
    "                                                y_pr_tr = clf.predict(train_features[:,selected_features])\n",
    "\n",
    "                                                accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "                                                test_acc_list.append(accuracy_te)\n",
    "\n",
    "\n",
    "                                                accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "                                                train_acc_list.append(accuracy_tr)\n",
    "\n",
    "                            \n",
    "                                                y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "                                                Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "                                                vote_acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "                                                vote_acc_list.append(vote_acc)\n",
    "                                                \n",
    "        \n",
    "                                            end_time = time.time()\n",
    "                                            running_time = end_time-start_time\n",
    "                                            participant = p_num\n",
    "                                            class1 = class_1\n",
    "                                            class2 = class_2\n",
    "                                            running_time = running_time\n",
    "                                            test_acc = np.average(test_acc_list)\n",
    "                                            train_acc = np.average(train_acc_list)\n",
    "                                            vote_acc = np.average(vote_acc_list)\n",
    "                                            test_size = X_te.shape\n",
    "                                            train_size = X_tr.shape\n",
    "                                            train_block = train_blk_name\n",
    "                                            test_block = test_blk_name\n",
    "                                            new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,vote_acc]\n",
    "\n",
    "                                            # if overlapflag == \"With_Overlap\":\n",
    "                                            path = os.path.join(\n",
    "                                                PATH,\n",
    "                                                classifier_name,\n",
    "                                                f\"{number_of_components}_CSP_Components\",\n",
    "                                                f\"{number_of_selected_features}-Selected_Features\",\n",
    "                                                f\"{overlap_percent}%_Overlap\",\n",
    "                                                f\"{window_time_length}_window_time_length\",\n",
    "                                                f\"{window_type}_Window\",\n",
    "                                                f\"{train_blk_name}_Train/\"\n",
    "                                            )\n",
    "                                            # else:\n",
    "                                            #     path = os.path.join(\n",
    "                                            #         PATH,\n",
    "                                            #         classifier_name,\n",
    "                                            #         f\"{number_of_components}_CSP_Components\",\n",
    "                                            #         f\"{number_of_selected_features}-Selected_Features\",\n",
    "                                            #         overlapflag,\n",
    "                                            #         f\"{window_time_length}_window_time_length\",\n",
    "                                            #         f\"{window_type}_Window\",\n",
    "                                            #         f\"{train_blk_name}_Train/\"\n",
    "                                            #     )\n",
    "\n",
    "                                            save_csv(new_row, path + f\"P{p_num}.csv\")\n",
    "                                            print(\n",
    "                                            f\"classifier_name = {classifier_name}\\n\"\n",
    "                                            f\"number_of_components = {number_of_components}\\n\"\n",
    "                                            f\"number_of_selected_features = {number_of_selected_features}\\n\"\n",
    "                                            f\"overlap_percent = {overlap_percent}\\n\"\n",
    "                                            f\"window_time_length = {window_time_length}\\n\"\n",
    "                                            f\"window_type = {window_type}\\n\"\n",
    "                                            f\"train_blk_name = {train_blk_name}\\n\"\n",
    "                                            f\"Participant = {p_num}\"\n",
    "                                            )\n",
    "\n",
    "                                            print(train_acc_list,\"train\",class_1)\n",
    "                                            print(test_acc_list,\"test\",class_1)\n",
    "\n",
    "                                            \n",
    "                                        p+=1\n",
    "                                    get_results_average(path,p_num_list,class_1_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "                \n",
    "# p = 0\n",
    "# for p_num in p_num_list:\n",
    "#     for class_1 in class_1_list:\n",
    "#         import time\n",
    "#         start_time = time.time()\n",
    "#         X_tr, Y_tr = trials_set_builder(data_dicts_list[p],train_blocks_set,'TRAIN',class_1,class_2,True,sliding_time_tr,channels_to_remove,number_of_channels)\n",
    "#         X_te, Y_te = trials_set_builder(data_dicts_list[p],test_blocks_set,'TEST',class_1,class_2,True,sliding_time_te,channels_to_remove,number_of_channels)\n",
    "\n",
    "#         print(X_tr.shape,Y_tr.shape,\"train shape\")\n",
    "#         print(X_te.shape,Y_te.shape,\"test shape\")\n",
    "\n",
    "#         [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te)\n",
    "#         # [train_features, test_features] =  apply_pca(X_tr,X_te,10)\n",
    "#         print(train_features.shape, \"train_features shape\")\n",
    "#         print(test_features.shape, \"test_features shape\")\n",
    "#         selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "#         train_acc_list = []\n",
    "#         test_acc_list = []\n",
    "\n",
    "#         clf = XGBClassifier()\n",
    "#         for r in range(1):\n",
    "#             clf.fit(train_features[:, selected_features], Y_tr[:,0])\n",
    "            \n",
    "#             y_pr_te = clf.predict(test_features[:, selected_features])\n",
    "#             y_pr_tr = clf.predict(train_features[:,selected_features])\n",
    "\n",
    "\n",
    "#             accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#             test_acc_list.append(accuracy_te)\n",
    "\n",
    "#             accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#             train_acc_list.append(accuracy_tr)\n",
    "\n",
    "#             # clf.fit(train_features[:, :], Y_tr[:,0])\n",
    "\n",
    "#             # y_pr_te = clf.predict(test_features[:, :])\n",
    "#             # y_pr_tr = clf.predict(train_features[:,:])\n",
    "\n",
    "#             # accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "#             # test_acc_list.append(accuracy_te)\n",
    "\n",
    "#             # accuracy_tr = accuracy_score(Y_tr,y_pr_tr)\n",
    "#             # train_acc_list.append(accuracy_tr)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # for i in range(len(Y_te)):\n",
    "#             #     print(f\"Test_Real: {Y_te[i][0]}   Test_Predication: {y_pr_te[i]}\")\n",
    "            \n",
    "#             # count_0s = np.count_nonzero(Y_tr == 0)\n",
    "#             # count_1s = np.count_nonzero(Y_tr == 1)\n",
    "\n",
    "#             # print(f\"Number of 0s: {count_0s}\")\n",
    "#             # print(f\"Number of 1s: {count_1s}\")\n",
    "\n",
    "#             # print(\"selected_features: \",selected_features)\n",
    "#             # print(Y_te.shape,y_pr_te.shape,\"shape \")\n",
    "#             y_pr_te_Vote = majority_vote_sliding_with_prev_v2(y_pr_te,vote_window)\n",
    "#             Y_te_Vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1),vote_window)\n",
    "\n",
    "#             # for i in range(len(Y_te)):\n",
    "#             #     print(f\"Test_Real_Vote: {Y_te_Vote[i]}   Test_Predication_vote: {y_pr_te_Vote[i]}\")\n",
    "\n",
    "\n",
    "#             acc, num_of_mismatches ,mismatches_list = custom_accuracy(Y_te_Vote,y_pr_te_Vote)\n",
    "#             # print(acc,num_of_mismatches,mismatches_list, \"acc, num_of_mismatches ,mismatches_list\",class_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         running_time = end_time-start_time\n",
    "#         participant = p_num\n",
    "#         class1 = class_1\n",
    "#         class2 = class_2\n",
    "#         running_time = running_time\n",
    "#         test_acc = np.average(test_acc_list)\n",
    "#         train_acc = np.average(train_acc_list)\n",
    "#         test_size = X_te.shape\n",
    "#         train_size = X_tr.shape\n",
    "#         train_block = '01234'\n",
    "#         test_block = '56'\n",
    "#         test_acc_vote = acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         new_row = [participant, class1, class2,running_time,test_acc,train_acc,test_size,train_size,train_block,test_block,test_acc_vote]\n",
    "\n",
    "#         new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "#         rf = pd.read_csv(PATH +'P'+str(p_num)+'.csv')\n",
    "#         cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "#         cf.to_csv(PATH +'P'+str(p_num)+'.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "#         print(train_acc_list,\"train\",class_1)\n",
    "#         print(test_acc_list,\"test\",class_1)\n",
    "        \n",
    "#     p+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n",
      "classifier_name = XGB\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "overlap_percent = 100\n",
      "window_time_length = 4\n",
      "window_type = Kaiser\n",
      "train_blk_name = 12345\n",
      "Participant = 3\n",
      "[1.0] train Hand\n",
      "[0.9464285714285714] test Hand\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n",
      "classifier_name = XGB\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "overlap_percent = 100\n",
      "window_time_length = 4\n",
      "window_type = Kaiser\n",
      "train_blk_name = 12345\n",
      "Participant = 3\n",
      "[1.0] train Feet\n",
      "[0.6428571428571429] test Feet\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n",
      "p.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 16 \t 15 \t 1.000\n",
      "2 \t 13 \t 12 \t 1.000\n",
      "3 \t 15 \t 14 \t 0.811\n",
      "4 \t 41 \t 40 \t 0.811\n",
      "5 \t 10 \t 9 \t 0.811\n",
      "6 \t 22 \t 21 \t 0.811\n",
      "7 \t 32 \t 31 \t 0.811\n",
      "8 \t 23 \t 22 \t 0.689\n",
      "9 \t 14 \t 13 \t 0.689\n",
      "10 \t 24 \t 23 \t 0.689\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 16 \t 15 \t 1.000\n",
      "2 \t 2 \t 1 \t 0.000\n",
      "3 \t 13 \t 12 \t 0.231\n",
      "4 \t 15 \t 14 \t 0.144\n",
      "5 \t 10 \t 9 \t 0.136\n",
      "6 \t 32 \t 31 \t 0.146\n",
      "7 \t 41 \t 40 \t 0.114\n",
      "8 \t 22 \t 21 \t 0.124\n",
      "9 \t 14 \t 13 \t 0.129\n",
      "10 \t 19 \t 18 \t 0.140\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 21 \t 20 \t 0.451\n",
      "2 \t 12 \t 11 \t 0.399\n",
      "3 \t 27 \t 26 \t 0.293\n",
      "4 \t 18 \t 17 \t 0.289\n",
      "5 \t 15 \t 14 \t 0.272\n",
      "6 \t 22 \t 21 \t 0.262\n",
      "7 \t 17 \t 16 \t 0.245\n",
      "8 \t 16 \t 15 \t 0.233\n",
      "9 \t 13 \t 12 \t 0.232\n",
      "10 \t 25 \t 24 \t 0.207\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 21 \t 20 \t 0.451\n",
      "2 \t 16 \t 15 \t 0.131\n",
      "3 \t 18 \t 17 \t 0.141\n",
      "4 \t 13 \t 12 \t 0.125\n",
      "5 \t 12 \t 11 \t 0.120\n",
      "6 \t 17 \t 16 \t 0.082\n",
      "7 \t 27 \t 26 \t 0.094\n",
      "8 \t 15 \t 14 \t 0.081\n",
      "9 \t 19 \t 18 \t 0.071\n",
      "10 \t 38 \t 37 \t 0.079\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 21 \t 20 \t 0.451\n",
      "2 \t 12 \t 11 \t 0.399\n",
      "3 \t 27 \t 26 \t 0.293\n",
      "4 \t 18 \t 17 \t 0.289\n",
      "5 \t 15 \t 14 \t 0.272\n",
      "6 \t 22 \t 21 \t 0.262\n",
      "7 \t 17 \t 16 \t 0.245\n",
      "8 \t 16 \t 15 \t 0.233\n",
      "9 \t 13 \t 12 \t 0.232\n",
      "10 \t 25 \t 24 \t 0.207\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 21 \t 20 \t 0.451\n",
      "2 \t 16 \t 15 \t 0.131\n",
      "3 \t 18 \t 17 \t 0.141\n",
      "4 \t 13 \t 12 \t 0.125\n",
      "5 \t 12 \t 11 \t 0.120\n",
      "6 \t 17 \t 16 \t 0.082\n",
      "7 \t 27 \t 26 \t 0.094\n",
      "8 \t 15 \t 14 \t 0.081\n",
      "9 \t 19 \t 18 \t 0.071\n",
      "10 \t 38 \t 37 \t 0.079\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 2 \t 1 \t 0.265\n",
      "2 \t 28 \t 27 \t 0.231\n",
      "3 \t 13 \t 12 \t 0.221\n",
      "4 \t 87 \t 86 \t 0.209\n",
      "5 \t 30 \t 29 \t 0.204\n",
      "6 \t 77 \t 76 \t 0.194\n",
      "7 \t 5 \t 4 \t 0.193\n",
      "8 \t 20 \t 19 \t 0.186\n",
      "9 \t 68 \t 67 \t 0.180\n",
      "10 \t 22 \t 21 \t 0.180\n",
      "\n",
      "*** mRMR features *** \n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 2 \t 1 \t 0.265\n",
      "2 \t 20 \t 19 \t 0.146\n",
      "3 \t 30 \t 29 \t 0.121\n",
      "4 \t 7 \t 6 \t 0.115\n",
      "5 \t 77 \t 76 \t 0.117\n",
      "6 \t 28 \t 27 \t 0.128\n",
      "7 \t 5 \t 4 \t 0.123\n",
      "8 \t 22 \t 21 \t 0.112\n",
      "9 \t 9 \t 8 \t 0.098\n",
      "10 \t 13 \t 12 \t 0.101\n",
      "\n",
      "\n",
      " *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n",
      "     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n",
      "     the paper \n",
      "     \"Feature selection based on mutual information: criteria of \n",
      "      max-dependency, max-relevance, and min-redundancy,\"\n",
      "      Hanchuan Peng, Fuhui Long, and Chris Ding, \n",
      "      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
      "      Vol. 27, No. 8, pp.1226-1238, 2005.\n",
      "\n",
      "\n",
      "*** MaxRel features ***\n",
      "Order \t Fea \t Name \t Score\n",
      "1 \t 11 \t 10 \t 0.391\n",
      "2 \t 30 \t 29 \t 0.307\n",
      "3 \t 34 \t 33 \t 0.277\n",
      "4 \t 23 \t 22 \t 0.259\n",
      "5 \t 15 \t 14 \t 0.210\n",
      "6 \t 3 \t 2 \t 0.206\n",
      "7 \t 7 \t 6 \t 0.203\n",
      "8 \t 27 \t 26 \t 0.191\n",
      "9 \t 6 \t 5 \t classifier_name = XGB\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "overlap_percent = 100\n",
      "window_time_length = 4\n",
      "window_type = Kaiser\n",
      "train_blk_name = 12345\n",
      "Participant = 3\n",
      "[1.0] train Tongue\n",
      "[0.8214285714285714] test Tongue\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n",
      "classifier_name = XGB\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "overlap_percent = 100\n",
      "window_time_length = 4\n",
      "window_type = Kaiser\n",
      "train_blk_name = 12345\n",
      "Participant = 3\n",
      "[1.0] train Mis\n",
      "[0.9107142857142857] test Mis\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n",
      "classifier_name = XGB\n",
      "number_of_components = 10\n",
      "number_of_selected_features = 10\n",
      "overlap_percent = 100\n",
      "window_time_length = 4\n",
      "window_type = Kaiser\n",
      "train_blk_name = 12345\n",
      "Participant = 4\n",
      "[1.0] train Hand\n",
      "[0.9107142857142857] test Hand\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "After concatenation - final_data shape: (84, 64, 1000) final_labels shape: (84, 1)\n",
      "After concatenation - final_data shape: (112, 64, 1000) final_labels shape: (112, 1)\n",
      "After concatenation - final_data shape: (140, 64, 1000) final_labels shape: (140, 1)\n",
      "After concatenation - final_data shape: (56, 64, 1000) final_labels shape: (56, 1)\n",
      "(140, 64, 1000) (140, 1) train shape\n",
      "(56, 64, 1000) (56, 1) test shape\n",
      "Frequency range:  4\n",
      "Frequency range:  8\n",
      "Frequency range:  12\n",
      "Frequency range:  16\n",
      "Frequency range:  20\n",
      "Frequency range:  24\n",
      "Frequency range:  28\n",
      "Frequency range:  32\n",
      "Frequency range:  36\n",
      "(140, 90) train_features shape\n",
      "(56, 90) test_features shape\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 84\u001b[0m, in \u001b[0;36mclassifier\u001b[0;34m(data_dicts_list)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_features\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_features shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_features\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_features shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_selector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_selected_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m clf \u001b[38;5;241m=\u001b[39m classifier\n\u001b[1;32m     88\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mfeature_selector\u001b[0;34m(train_features, labels, number_of_selected_features)\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y,X], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[43mpymrmr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmRMR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m selected_features\n",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/pymrmr/pymrmr.pyx:28\u001b[0m, in \u001b[0;36mpymrmr.mRMR\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/jupyter2/lib/python3.8/site-packages/pandas/core/indexes/base.py:5304\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5300\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[1;32m   5302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex does not support mutable operations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 5304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5306\u001b[0m \u001b[38;5;124;03m    Override numpy.ndarray's __getitem__ method to work as desired.\u001b[39;00m\n\u001b[1;32m   5307\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5313\u001b[0m \n\u001b[1;32m   5314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   5315\u001b[0m     getitem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier(data_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def capture_output(filename):\n",
    "    original_stdout = sys.stdout\n",
    "    original_stderr = sys.stderr\n",
    "    with open(filename, 'w') as f:\n",
    "        sys.stdout = sys.stderr = f\n",
    "        yield\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (import statements and existing code)\n",
    "\n",
    "def process_configuration(data_dict, classifier_name, number_of_components, number_of_selected_features, overlap_percent,\n",
    "                          window_time_length, window_type, train_blk_set, test_blk_set, p_num, class_1_list, class_2):\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call your existing functions to build training and testing sets\n",
    "    X_tr, Y_tr = trials_set_builder(data_dict, train_blk_set, 'TRAIN', class_1_list[0], class_2, True, sliding_time_tr,\n",
    "                                     window_time_length, window_type, channels_to_remove, number_of_channels)\n",
    "    X_te, Y_te = trials_set_builder(data_dict, test_blk_set, 'TEST', class_1_list[0], class_2, True, sliding_time_te,\n",
    "                                     window_time_length, window_type, channels_to_remove, number_of_channels)\n",
    "\n",
    "    # ... (remaining existing code)\n",
    "\n",
    "    [train_features, test_features] = feature_extractor(X_tr, Y_tr, number_of_bands, X_te, number_of_components)\n",
    "    selected_features = feature_selector(train_features, Y_tr, number_of_selected_features)\n",
    "\n",
    "    clf = classifier_dic[classifier_name]\n",
    "    runs = 1\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    vote_acc_list = []\n",
    "\n",
    "    for r in range(runs):\n",
    "        clf.fit(train_features[:, selected_features], Y_tr[:, 0])\n",
    "\n",
    "        y_pr_te = clf.predict(test_features[:, selected_features])\n",
    "        y_pr_tr = clf.predict(train_features[:, selected_features])\n",
    "\n",
    "        accuracy_te = accuracy_score(Y_te, y_pr_te)\n",
    "        test_acc_list.append(accuracy_te)\n",
    "\n",
    "        accuracy_tr = accuracy_score(Y_tr, y_pr_tr)\n",
    "        train_acc_list.append(accuracy_tr)\n",
    "\n",
    "        y_pr_te_vote = majority_vote_sliding_with_prev_v2(y_pr_te, vote_window)\n",
    "        Y_te_vote = majority_vote_sliding_with_prev_v2(Y_te.reshape(-1), vote_window)\n",
    "        vote_acc, num_of_mismatches, mismatches_list = custom_accuracy(Y_te_vote, y_pr_te_vote)\n",
    "        vote_acc_list.append(vote_acc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    participant = p_num\n",
    "    class1 = class_1_list[0]\n",
    "    class2 = class_2\n",
    "    test_acc = np.average(test_acc_list)\n",
    "    train_acc = np.average(train_acc_list)\n",
    "    vote_acc = np.average(vote_acc_list)\n",
    "    test_size = X_te.shape\n",
    "    train_size = X_tr.shape\n",
    "    train_block = train_blk_set\n",
    "    test_block = test_blk_set\n",
    "    new_row = [participant, class1, class2, running_time, test_acc, train_acc, test_size, train_size, train_block,\n",
    "               test_block, vote_acc]\n",
    "\n",
    "    path = os.path.join(\n",
    "        PATH,\n",
    "        classifier_name,\n",
    "        f\"{number_of_components}_CSP_Components\",\n",
    "        f\"{number_of_selected_features}-Selected_Features\",\n",
    "        f\"{overlap_percent}%_Overlap\",\n",
    "        f\"{window_time_length}_window_time_length\",\n",
    "        f\"{window_type}_Window\",\n",
    "        f\"{train_blk_set}_Train/\"\n",
    "    )\n",
    "\n",
    "    save_csv(new_row, os.path.join(path, f\"P{p_num}.csv\"))\n",
    "    print(\n",
    "        f\"classifier_name = {classifier_name}\\n\"\n",
    "        f\"number_of_components = {number_of_components}\\n\"\n",
    "        f\"number_of_selected_features = {number_of_selected_features}\\n\"\n",
    "        f\"overlap_percent = {overlap_percent}\\n\"\n",
    "        f\"window_time_length = {window_time_length}\\n\"\n",
    "        f\"window_type = {window_type}\\n\"\n",
    "        f\"train_blk_set = {train_blk_set}\\n\"\n",
    "        f\"test_blk_set = {test_blk_set}\\n\"\n",
    "        f\"Participant = {p_num}\"\n",
    "    )\n",
    "\n",
    "    print(train_acc_list, \"train\", class_1_list[0])\n",
    "    print(test_acc_list, \"test\", class_1_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-49:\n",
      "Process ForkPoolWorker-60:\n",
      "Process ForkPoolWorker-61:\n",
      "Process ForkPoolWorker-55:\n",
      "Process ForkPoolWorker-53:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-51:\n",
      "Process ForkPoolWorker-62:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m     pool\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mclassifier_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 42\u001b[0m, in \u001b[0;36mclassifier_multiprocess\u001b[0;34m(data_dicts_list)\u001b[0m\n\u001b[1;32m     29\u001b[0m configurations \u001b[38;5;241m=\u001b[39m [(classifier_name, number_of_components, number_of_selected_features, overlap_percent,\n\u001b[1;32m     30\u001b[0m                     window_time_length, window_type, train_blk_set, test_blk_set, p_num)\n\u001b[1;32m     31\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m classifier_name \u001b[38;5;129;01min\u001b[39;00m classifier_dic\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m test_blk_set, test_blk_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_blk_set_dic\u001b[38;5;241m.\u001b[39mvalues(), test_blk_set_dic\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     39\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m p_num \u001b[38;5;129;01min\u001b[39;00m p_num_list]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Use multiprocessing to process the configurations\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigurations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     45\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-63:\n",
      "Process ForkPoolWorker-64:\n",
      "Process ForkPoolWorker-50:\n",
      "Process ForkPoolWorker-52:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-67:\n",
      "Process ForkPoolWorker-66:\n",
      "Process ForkPoolWorker-58:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-41:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-43:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-56:\n",
      "Process ForkPoolWorker-47:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 358, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"pandas/_libs/internals.pyx\", line 575, in pandas._libs.internals._unpickle_block\n",
      "  File \"/home/mahdi146/jupyter2/lib/python3.8/site-packages/pandas/core/internals/blocks.py\", line 2172, in new_block\n",
      "    def new_block(values, placement, *, ndim: int) -> Block:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-69:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "def classifier_multiprocess(data_dicts_list):\n",
    "    classifier_dic = {\"XGB\": XGBClassifier(n_jobs = 32), \"LDA\": LDA()}\n",
    "    number_of_components_list = [10]\n",
    "    number_of_selected_features_list = [10, 2]\n",
    "    overlap_percent_list = [100, 25]\n",
    "    channels_to_remove = []\n",
    "    number_of_channels = NUMBER_OF_CHANNELS\n",
    "    window_time_length_list = [4]\n",
    "    sliding_time_tr = 4\n",
    "    sliding_time_te = 4\n",
    "    vote_window = 3\n",
    "    window_type_list = [\"Rec\", \"Kaiser\"]\n",
    "    train_blk_set_dic = {\"12345\": [0, 1, 2, 3, 4], \"1234\": [0, 1, 2, 3], \"123\": [0, 1, 2], \"12\": [0, 1], \"1\": [0]}\n",
    "    test_blk_set_dic = {\"67\": [5, 6], \"567\": [4, 5, 6], \"4567\": [3, 4, 5, 6], \"34567\": [2, 3, 4, 5, 6],\n",
    "                        \"234567\": [1, 2, 3, 4, 5, 6]}\n",
    "    class_1_list = ['Hand', 'Feet', 'Tongue', 'Mis']\n",
    "    class_2 = 'Rest'\n",
    "    p_num_list = [3, 4, 5, 6, 7, 9, 10, 11, 13, 14]\n",
    "    PATH = \"/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results-v3/\"\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "\n",
    "    # Create a partial function with fixed arguments\n",
    "    partial_process = partial(process_configuration, data_dicts_list=data_dicts_list, class_1_list=class_1_list,\n",
    "                               class_2=class_2)\n",
    "\n",
    "    # Generate all configurations to process\n",
    "    configurations = [(classifier_name, number_of_components, number_of_selected_features, overlap_percent,\n",
    "                        window_time_length, window_type, train_blk_set, test_blk_set, p_num)\n",
    "                       for classifier_name in classifier_dic.keys()\n",
    "                       for number_of_components in number_of_components_list\n",
    "                       for number_of_selected_features in number_of_selected_features_list\n",
    "                       for overlap_percent in overlap_percent_list\n",
    "                       for window_time_length in window_time_length_list\n",
    "                       for window_type in window_type_list\n",
    "                       for train_blk_set, train_blk_name in zip(train_blk_set_dic.values(), train_blk_set_dic.keys())\n",
    "                       for test_blk_set, test_blk_name in zip(test_blk_set_dic.values(), test_blk_set_dic.keys())\n",
    "                       for p_num in p_num_list]\n",
    "\n",
    "    # Use multiprocessing to process the configurations\n",
    "    pool.starmap(partial_process, configurations)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    classifier_multiprocess(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Frame Maker\n",
    "PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results-v2/XGB/10_CSP_Components/10-Selected_Features/With_Overlap/Rec_Window/50%_Overlap/12345_Train/'\n",
    "df = pd.read_csv(PATH+'frame.csv')\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "for p_num in p_num_list:\n",
    "    df.to_csv(PATH+'P'+str(p_num)+'.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\n",
    "    PATH,\n",
    "    classifier_name,\n",
    "    f\"{number_of_components}_CSP_Components\",\n",
    "    f\"{number_of_selected_features}-Selected_Features\",\n",
    "    overlapflag,\n",
    "    f\"{window_time_length}_window_time_length\",\n",
    "    f\"{window_type}_Window\",\n",
    "    f\"{train_blk_name}_Train/\",\n",
    ")\n",
    "print(path + f\"P{p_num}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_csv(new_row, classifier, number_of_components, number_of_selected_features, overlapflag,\n",
    "             window_time_length, window_type, train_blk_name, p_num):\n",
    "    # Convert to relative path\n",
    "    relative_path = os.path.join(\n",
    "        classifier,\n",
    "        f\"{number_of_components}_CSP_Components\",\n",
    "        f\"{number_of_selected_features}-Selected_Features\",\n",
    "        overlapflag,\n",
    "        f\"{window_time_length}_window_time_length\",\n",
    "        window_type,\n",
    "        f\"{train_blk_name}_Train\",\n",
    "        f\"P{p_num}.csv\"\n",
    "    )\n",
    "\n",
    "    # Convert relative path to absolute path\n",
    "    absolute_path = os.path.abspath(relative_path)\n",
    "\n",
    "    # Read existing CSV or create an empty DataFrame\n",
    "    try:\n",
    "        rf = pd.read_csv(absolute_path)\n",
    "    except FileNotFoundError:\n",
    "        rf = pd.DataFrame(columns=column_names_v2)\n",
    "\n",
    "    # Create a new row DataFrame\n",
    "    new_row_df = pd.DataFrame([new_row], columns=column_names_v2)\n",
    "\n",
    "    # Concatenate existing DataFrame and new row DataFrame\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "\n",
    "    # Write the concatenated DataFrame to CSV\n",
    "    cf.to_csv(absolute_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "classifier = \"XGBClassifier\"\n",
    "number_of_components = 10\n",
    "number_of_selected_features = 10\n",
    "overlapflag = \"With_Overlap\"\n",
    "window_time_length = \"4_window_time_length\"\n",
    "window_type = \"50% Overlap\"\n",
    "train_blk_name = \"12345\"\n",
    "p_num = 9\n",
    "\n",
    "new_row = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10]\n",
    "save_csv(new_row, classifier, number_of_components, number_of_selected_features, overlapflag,\n",
    "         window_time_length, window_type, train_blk_name, p_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Specific path and file name\n",
    "# file_path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/frame.csv'  # Replace with your specific path\n",
    "# df = pd.DataFrame(columns=column_names)\n",
    "# # Write the DataFrame to a CSV file at the specified path\n",
    "# df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list = [\"12345\", \"1234\", \"123\", \"12\", \"1\"]\n",
    "\n",
    "second_list = []\n",
    "\n",
    "for item in first_list:\n",
    "    result = \"\"\n",
    "    for i in range(len(item) - 1):\n",
    "        result += str(int(item[i]) + int(item[i + 1]))\n",
    "    second_list.append(result)\n",
    "\n",
    "print(second_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blk_set_dic = {\"01234\":[0,1,2,3,4],\"0123\":[0,1,2,3],\"012\":[0,1,2],\"01\":[0,1],\"0\":[0]}\n",
    "for train_blk_set,train_blk_name in zip(train_blk_set_dic.values(),train_blk_set_dic.keys()):\n",
    "    print(len(train_blk_set))\n",
    "    print(train_blk_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # Specify the threshold for outliers (you can adjust this based on your data)\n",
    "    threshold = 10 ** 5\n",
    "\n",
    "    # Calculate median and MAD for each row\n",
    "    median = df.iloc[:, :-1].median(axis=1)\n",
    "    mad = np.median(np.abs(df.iloc[:, :-1].sub(median, axis=0)), axis=1)\n",
    "    threshold_array = median + threshold * mad\n",
    "\n",
    "    # Identify rows with values exceeding the threshold\n",
    "    outliers = df.iloc[:, :-1].gt(threshold_array[:, None], axis=0).any(axis=1)\n",
    "\n",
    "    # Remove rows identified as outliers\n",
    "    clean_df = df[~outliers]\n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pikle_converter\n",
    "def data_reader(path,p_num,block_list):\n",
    "    data_dict = {}\n",
    "    for b_num in block_list:\n",
    "        print(b_num)\n",
    "        mat = loadmat(path+'P'+ str(p_num) + '/'+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "        df = pd.DataFrame(mat['Data'])\n",
    "        df.to_pickle(f\"{path}Pickels_Participants/P{p_num}/P{p_num}B{b_num}.pkl\")\n",
    "    #     data_dict[b_num] = df\n",
    "    # return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "block_list = [0,1,2,3,4,5,6]\n",
    "p_num_list = [9]\n",
    "path = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/Participants/'\n",
    "for p_num in p_num_list:\n",
    "    print(\"reading\",p_num)\n",
    "    data_reader(path,p_num,block_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3878301127.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(len()data_dict_list)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(len()data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers_across_channels(df, threshold):\n",
    "    # print(df.shape,\"shape0\")\n",
    "\n",
    "    data_columns = df.columns[:-1]  # Excluding the label column by default\n",
    "\n",
    "    # Separate the label column\n",
    "    labels = df.iloc[:, -1]  # Assuming the label is in the last column\n",
    "    data_without_label = df.iloc[:, :-1]  # DataFrame without the label column\n",
    "\n",
    "    # Calculate Z-scores for each row\n",
    "    # print(data_without_label.shape,\"shape1\")\n",
    "    # print(data_without_label[data_columns].shape,\"shape2\")\n",
    "    # print(data_without_label.head(10))\n",
    "    data_without_label[data_columns] = data_without_label[data_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    z_scores = stats.zscore(data_without_label[data_columns], axis=1)\n",
    "    abs_z_scores = abs(z_scores)\n",
    "\n",
    "    filtered_entries = (abs_z_scores < threshold).all(axis=1)\n",
    "    filtered_data = data_without_label[filtered_entries]\n",
    "\n",
    "    # Remove corresponding labels for removed rows\n",
    "    filtered_labels = labels[filtered_entries]\n",
    "\n",
    "    # If you want to reset index after filtering\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    filtered_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Combine the filtered data and labels\n",
    "    filtered_df = pd.concat([filtered_data, filtered_labels], axis=1)\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = [1, 1, 1, 1, 0, 0, 1, 1, 0, 0]  # Replace with your actual prediction list\n",
    "\n",
    "result = majority_vote_sliding_with_next(prediction_list)\n",
    "result2 = majority_vote_sliding_with_prev_v2(prediction_list)\n",
    "print(\"Majority Votes:\", result)\n",
    "print(\"Majority Votes Previous:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/mahdi146/projects/def-b09sdp/mahdi146/Cedar/Classification/EEG/Results/XGBoost/Win4Shift2/'\n",
    "p_num_list = [3,4,5,6,7,9,10,11,13,14]\n",
    "vf = pd.DataFrame(columns=column_names_v2) \n",
    "for p_num in p_num_list:\n",
    "    print(p_num)\n",
    "    rf = pd.read_csv(PATH + \"P\" + str(p_num) + \".csv\")\n",
    "    vf = pd.concat([vf, rf], ignore_index=True)\n",
    "vf.to_csv(PATH+ 'ResultsOfAll.csv', index=False)\n",
    "# vf.tail()\n",
    "    \n",
    "columnNames = ['class','test_acc','vote_acc']\n",
    "kf = pd.DataFrame(columns=columnNames)\n",
    "kf.to_csv(PATH+'AverageAcc.csv',index=False)\n",
    "\n",
    "vf = pd.read_csv(PATH +\"ResultsOfAll.csv\")\n",
    "df = vf\n",
    "\n",
    "class_list=['Hand','Feet','Tongue','Mis']\n",
    "blk_list = [1234]\n",
    "for class_ in class_list:\n",
    "    avg_list = []\n",
    "    avg_vote_list = []\n",
    "    for blk in blk_list:\n",
    "        gf = df[(df['train_block'] == blk) & (df['class1'] == class_)]\n",
    "        avg = gf['test_acc'].mean()\n",
    "        avg_vote = gf['test_acc_vote'].mean()\n",
    "        avg_list.append(avg)\n",
    "        avg_vote_list.append(avg_vote)\n",
    "    print(avg_list)    \n",
    "    new_row = [class_, avg_list[0],avg_vote_list[0]] \n",
    "    new_row_df = pd.DataFrame([new_row], columns=columnNames)\n",
    "    rf = pd.read_csv(PATH + 'AverageAccExcP8.csv')\n",
    "    cf = pd.concat([rf, new_row_df], ignore_index=True)\n",
    "    cf.to_csv(PATH +'AverageAccExcP8.csv',index=False)  \n",
    "kf = pd.read_csv(PATH +'AverageAccExcP8.csv') \n",
    "kf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with the last column named 'label'\n",
    "data = {'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter'],\n",
    "        'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "print(\"his\")\n",
    "# Define a custom sorting order based on the desired grouping\n",
    "sorting_order = {'a': 0, 'b': 1}\n",
    "\n",
    "# Create a new column with the sorting order\n",
    "df['sorting_order'] = df.iloc[:, 2].map(sorting_order)\n",
    "\n",
    "# Sort the DataFrame based on the new column and the original order within each group\n",
    "df.sort_values(by=['sorting_order', df.columns[2]], inplace=True)\n",
    "\n",
    "# Drop the temporary sorting column\n",
    "df.drop('sorting_order', axis=1, inplace=True)\n",
    "\n",
    "# Optional: Reset the index if needed\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','b'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "x=0\n",
    "i=0\n",
    "class_1 = 'a'\n",
    "class_2 = 'b'\n",
    "sampleList = []\n",
    "while i<len(df):\n",
    "    if (df.iloc[i,2]==class_1):\n",
    "        x+=1\n",
    "    else:\n",
    "        i-=1\n",
    "        sampleList.append(x)\n",
    "        x=0\n",
    "        class_1,class_2 = class_2,class_1\n",
    "    i+=1\n",
    "sampleList.append(x)\n",
    "print(sampleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'it', 'does', 'not matter'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'b', 'b', 'b']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(get_group_start_indices(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13],\n",
    "    'col2': ['some', 'random', 'data', 'for', 'example', 'purposes', 'in', 'this', 'case', 'it', 'does', 'not matter','c'],\n",
    "    'label': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'b', 'b','b']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify consecutive groups of 'a's by creating a new group ID each time 'label' changes from 'b' to 'a'\n",
    "df['group'] = (df['label'] != df['label'].shift(1)).cumsum()\n",
    "\n",
    "# Count occurrences of 'a' within each group\n",
    "group_counts = df[df['label'] == 'a'].groupby('group').size()\n",
    "\n",
    "group_counts_b = df[df['label'] == 'b'].groupby('group').size()\n",
    "print(group_counts_b)\n",
    "print(group_counts_b.index[0])\n",
    "print(group_counts_b.iloc[0])\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_num = 8\n",
    "b_num = 7\n",
    "path = f'../../Participants/P{p_num}v5/'\n",
    "mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "df = pd.DataFrame(mat['Data'])\n",
    "\n",
    "path = f'../../Participants/P{p_num}v0/'\n",
    "mat = loadmat(path+'P'+str(p_num)+'B'+str(b_num)+'.mat', chars_as_strings=True, mat_dtype=True, squeeze_me=True, struct_as_record=False, verify_compressed_data_integrity=False, variable_names=None)\n",
    "df_1 = pd.DataFrame(mat['Data'])\n",
    "\n",
    "print(df.shape)\n",
    "print(df_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import median\n",
    "\n",
    "def apply_median_filter(df, window_size=10):\n",
    "    channel_data = df.iloc[:, :-1].values\n",
    "    labels = df.iloc[:, -1].values \n",
    "    filtered_channel_data = np.zeros_like(channel_data) \n",
    "\n",
    "    for i in range(channel_data.shape[1]):\n",
    "        print(i,\"channel\")\n",
    "        filtered_channel_data[:, i] = median(channel_data[:, i], selem=np.ones(window_size))\n",
    "\n",
    "    df_filtered = pd.DataFrame(filtered_channel_data, columns=df.columns[:-1])\n",
    "    df_filtered['Label'] = labels\n",
    "    print(df_filtered.shape, \"shape after median filter\")\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(train_data, test_data=None, n_components=50):\n",
    "    # Reshape train data\n",
    "    train_data_reshaped = np.reshape(train_data, (train_data.shape[0], -1))\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train_data = scaler.fit_transform(train_data_reshaped)\n",
    "    # scaled_train_data = train_data_reshaped\n",
    "    \n",
    "    # Apply PCA on train data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(scaled_train_data)\n",
    "    transformed_train_data = pca.transform(scaled_train_data)\n",
    "\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Explained Variance Ratio:\")\n",
    "    print(explained_variance_ratio)\n",
    "    print(\"\\nCumulative Explained Variance:\")\n",
    "    print(cumulative_explained_variance)\n",
    "\n",
    "\n",
    "    test_data_reshaped = np.reshape(test_data, (test_data.shape[0], -1))\n",
    "    scaled_test_data = scaler.transform(test_data_reshaped)\n",
    "    transformed_test_data = pca.transform(scaled_test_data)\n",
    "\n",
    "    return transformed_train_data, transformed_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_samples_block_counter(df_1,trial_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(7):\n",
    "    extra_samples_block_counter(data_dicts_list[-1][b],trial_order[b],b)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHUhd3A17lB+9o7HFw3Jl4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel2",
   "language": "python",
   "name": "kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
